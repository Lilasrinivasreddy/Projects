Run Profile Date	LOB	Domain Name	DB Name	Table Name	Feature Name	Dimensions	DQ Category	Weekday	Value	Avg Value	Percentage Change	Variance	Std Deviation	Sigma Value	Consistency Score	DQ Score	Status
0	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	ACSS_BATCH_REQUEST_TYPE_V	Executive Reports	None	Consistency	2	38.00	38.00	0.00	0.00	0.00	1	100.00	100.00	PASS
1	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	AREA_V	Executive Reports	None	Consistency	2	5.00	5.00	0.00	0.00	0.00	1	100.00	100.00	PASS
2	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	CELLULAR_CARRIER_V	Executive Reports	None	Consistency	2	917.00	917.00	0.00	0.00	0.00	1	100.00	100.00	PASS
3	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	INDIRECT_DEVICE_TRADE_CANCEL_V	Executive Reports	None	Consistency	2	44,392.00	45,113.00	-1.62	1,559,523.00	1,248.79	1	98.40	100.00	PASS
4	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	PAYMENT_SOURCE_V	Executive Reports	None	Consistency	2	324.00	324.00	0.00	0.00	0.00	1	100.00	100.00	PASS
5	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	PRICE_PLAN_BRAND_NAME_V	Executive Reports	None	Consistency	2	44.00	44.00	0.00	0.00	0.00	1	100.00	100.00	PASS
6	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	PRICE_PLAN_CLASS_V	Executive Reports	None	Consistency	2	3.00	3.00	0.00	0.00	0.00	1	100.00	100.00	PASS
7	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	PRICE_PLAN_GEOGRAPHY_V	Executive Reports	None	Consistency	2	6.00	6.00	0.00	0.00	0.00	1	100.00	100.00	PASS
8	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	PRICE_PLAN_HANDSET_REQ_V	Executive Reports	None	Consistency	2	6.00	6.00	0.00	0.00	0.00	1	100.00	100.00	PASS
9	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	PRICE_PLAN_HOME_RT_CVRG_V	Executive Reports	None	Consistency	2	3.00	3.00	0.00	0.00	0.00	1	100.00	100.00	PASS
10	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	PRICE_PLAN_PRODUCT_LINE_V	Executive Reports	None	Consistency	2	126.00	126.00	0.00	0.00	0.00	1	100.00	100.00	PASS
11	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	PRICE_PLAN_PROMO_V	Executive Reports	None	Consistency	2	94.00	94.00	0.00	0.00	0.00	1	100.00	100.00	PASS
12	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	REGION_V	Executive Reports	None	Consistency	2	23.00	23.00	0.00	0.00	0.00	1	100.00	100.00	PASS
13	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	SUB_REGION_V	Executive Reports	None	Consistency	2	141.00	141.00	0.00	0.00	0.00	1	100.00	100.00	PASS
14	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	NETACE_MARKETING_XREF_V	Executive Reports	None	Consistency	2	5,030.00	5,030.00	0.00	0.00	0.00	1	100.00	100.00	PASS
15	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	EQUIP_SUM_FACT_V	Executive Reports	None	Consistency	1	10,490,005.00	3,187,768.08	69.59	13,555,773,008,428.51	3,681,816.54	2	329.06	0.00	HIGH
16	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	CUST_ACCT_LINE_EHA_V	Executive Reports	None	Consistency	1	7,242.00	6,256.25	13.59	4,498,461.19	2,120.96	1	115.76	100.00	PASS
17	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	PAYMENT_ACH_V	Executive Reports	None	Consistency	4	15,782.00	0.00	0.00	0.00	0.00	outlier	0.00	0.00	LOW
18	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	CREDIT_ACTIVATION_V	Executive Reports	None	Consistency	1	219,348.00	202,143.00	7.83	864,295,397.08	29,398.90	1	108.51	100.00	PASS
19	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	CREDIT_DECISION_V	Executive Reports	None	Consistency	1	1,287,673.00	1,312,468.13	-1.91	15,788,373,677.20	125,651.80	1	98.09	100.00	PASS
20	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	CUST_ACCT_DLY_HIST_V	Executive Reports	None	Consistency	1	751,337.00	4,167,955.00	-454.74	152,971,088,269,974.59	12,368,148.13	1	18.03	0.00	LOW
21	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	CUST_ACCT_LINE_CBR_V	Executive Reports	None	Consistency	1	58,217.00	69,161.46	-18.80	366,857,439.62	19,153.52	1	84.18	100.00	PASS
22	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	CUST_DIM_DLY_HIST_V	Executive Reports	None	Consistency	1	31,289.00	28,804.84	7.94	33,309,333.67	5,771.42	1	108.62	100.00	PASS
23	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	DLY_LINE_ACTIVITY_PPLAN_PEND_V	Executive Reports	None	Consistency	1	57,881.00	143,471.69	-147.87	70,571,972,924.52	265,653.84	1	40.34	0.00	LOW
24	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	POS_DEVICE_RECYCLE_ORDER_EXT_V	Executive Reports	None	Consistency	1	49,006.00	34,078.15	30.46	90,023,869.67	9,488.09	2	143.80	0.00	HIGH
25	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	SID_V	Executive Reports	None	Consistency	1	516.00	516.00	0.00	0.00	0.00	1	100.00	100.00	PASS
26	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	cust_acct_line_pplan_hist_V	Executive Reports	None	Consistency	1	365,482.00	473,706.00	-29.60	55,501,945,179.08	235,588.51	1	77.15	0.00	LOW
27	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	FIXED_5GPM_ORDER_TRACKING_V	Executive Reports	None	Consistency	1	0.00	0.00	0.00	0.00	0.00	1	0.00	0.00	LOW
28	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	OMP_OFFER_V	Executive Reports	None	Consistency	2	113,550.00	112,980.50	0.50	39,668.75	199.15	3	100.50	100.00	LOW
29	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	EPS_FEATURE_DETAILS_HIST_V	Executive Reports	None	Consistency	1	0.00	0.00	0.00	0.00	0.00	1	0.00	0.00	LOW
30	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	EPS_DEVICE_DETAILS_HIST_V	Executive Reports	None	Consistency	2	44,392.00	45,113.00	-1.62	1,559,523.00	1,248.79	1	98.40	100.00	PASS
31	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	dpv_feat_pack_map_V	Executive Reports	None	Consistency	2	35,391.00	35,329.50	0.17	726.75	26.96	3	100.17	100.00	LOW
32	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	POS_OFFER_V	Executive Reports	None	Consistency	1	0.00	0.00	0.00	0.00	0.00	1	0.00	0.00	LOW
33	2025-03-31	VCG	One_Ex_Executive_report	NTL_PRD_ALLVM	RPT_ATOMIC_PROD_HIER_INTEGRITY_V	Executive Reports	None	Consistency	2	7,909.00	7,909.00	0.00	0.00	0.00	1	100.00	100.00	PASS

=======================================


import re
import pandas as pd
import numpy as np
import os
import logging
import sys
from datetime import datetime, timedelta
from functools import reduce
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)


## Importing User Defined Modules
import config_params as config
from send_email import SendEmail
from common_handlers import CommonUtils, set_logger, get_args_parser

from dqaas_opsgenie import Alert
from dqaas_jira import Jira_ticket
import math



class CustomeMetrics:
    def __init__(self, data_src = None) -> None:
        self.__set_default_values()
        self.logger: logging = set_logger(
            logger_path=config.LOGS_DIR,
            log_filename="CustomMetricsLogs",
            process_name='CRM',
            # no_date_yn="Y",
        )
        
        self.utils = CommonUtils(config, self.logger)
        self.df_email_distro= pd.DataFrame()
        
    ## Default Values
    def __set_default_values(self):
        self.rule_col_rename_list = {
            'profile_date': 'data_dt',
            'pageName': 'value',
            'pageflow': 'value' ,
            'value': 'count_curr',
            'insert_dt': 'rule_run_ts',
        }
        
        self.final_col_rename_list = {
            'profile_date': 'data_dt',
            'value': 'count_curr',
            'dimension': 'grouped_columns',
            'rule_run_ts': 'prfl_run_ts',
        }
        
        self.custom_profile_report_columns = {
            "NUMERIC": [
                'avg_count_prev', 'variance_value', 'std_dev_value',
                'pct_change', 'count_curr','sigma_2_value','dq_score',
                'consistency_score',
            ],
            "INT64": ['prfl_id', 'weekday', 'rpt_seq_num', ],
            "STRING": [
                'feature_name', 'grouped_columns', 'sigma_value', 'rpt_ref_key', 'data_dt',
                'prfl_type', 'src_tbl', 'dq_pillar', 'meas_name', 'dq_ind'
            ],
            "TIMESTAMP": ['prfl_run_ts'],
        }
        
        self.email_rename_cols = {
            'prfl_run_ts': 'Run Profile Date',
            'data_lob': 'LOB',
            'data_bus_elem': 'Domain Name',
            'db_name': 'DB Name',
            'src_tbl': 'Table Name',
            'feature_name': 'Feature Name',
            'grouped_columns': 'Dimensions',
            'dq_pillar': 'DQ Category',
            'weekday': 'Weekday',
            'count_curr': 'Value',
            'avg_count_prev': 'Avg Value',
            'pct_change': 'Percentage Change',
            'variance_value': 'Variance',
            'std_dev_value': "Std Deviation",
            'sigma_value': 'Sigma Value',
            'consistency_score': 'Consistency Score',
            'dq_score': 'DQ Score',
            'dq_ind': 'Status',
        }
        
        ## Same order used in the Email
        self.summary_cols = [
            'prfl_run_ts', 'data_lob', 'data_bus_elem', 'db_name', 'src_tbl',
            'feature_name', 'grouped_columns', 'dq_pillar',
            'weekday', 'count_curr', 'avg_count_prev', 'pct_change',
            'variance_value', 'std_dev_value', 'sigma_value',
            'consistency_score', 'dq_score', 'dq_ind'
        ]
        
        ## Columns used to round off the float values in Email, 
        ## Applicable Columns :- Should be available in self.summary_cols
        self.email_float_cols = [
           'count_curr', 'avg_count_prev', 'pct_change',
            'variance_value', 'std_dev_value',
            'consistency_score', 'dq_score',
        ]
    
    ## Historical Queries 
    @staticmethod
    def historical_queries(comparison_type: str='WEEKDAYS'):
        query = {
            
            "WEEKDAYS": r"""
            with top_records as 
            (
            select *,row_number() over (partition by prfl_id,feature_name,grouped_columns order by data_dt desc) as row_num 
            from $rpt_table join 
            unnest(GENERATE_DATE_ARRAY(date_sub($RUN_DT,INTERVAL 3 MONTH),$RUN_DT, INTERVAL 1 day)) as interval_Date
            on data_dt = interval_Date
            where prfl_id IN ($prfl_id_list)
            and data_dt != $RUN_DT
            order by prfl_id,feature_name, grouped_columns,data_dt desc
            ) 
            select prfl_id,feature_name,grouped_columns,WEEKDAY,
            round(sum(ifnull(count_curr, 0)), 2) as sum_count_prev,
            round(avg(ifnull(count_curr, 0)), 2) as avg_count_prev,
            round(var_pop(ifnull(count_curr, 0)), 2) as variance_value,
            round(stddev_pop(ifnull(count_curr, 0)), 2) as std_dev_value,
            round(stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as sigma_2_value, 
            round(avg(ifnull(count_curr, 0)) - stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as min_thresh_value, 
            round(avg(ifnull(count_curr, 0)) + stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as max_thresh_value
            from top_records 
            group by prfl_id,feature_name,grouped_columns,WEEKDAY
            order by prfl_id,feature_name,grouped_columns,WEEKDAY;
            """,
            
            "DTRAN_MONTHLY": r"""
            with top_records as 
            (select *,row_number() over (partition by prfl_id,feature_name,grouped_columns order by data_dt desc) as row_num 
            from $rpt_table join 
            unnest(GENERATE_DATE_ARRAY(date_trunc(date_sub($RUN_DT,INTERVAL 3 MONTH), MONTH),$RUN_DT, INTERVAL 1 MONTH)) as interval_Date
            on data_dt = interval_Date
            where prfl_id IN ($prfl_id_list)
            and data_dt != date_trunc(date($RUN_DT), MONTH)
            order by prfl_id,feature_name, grouped_columns,data_dt desc) 
            select prfl_id,feature_name,grouped_columns,
            round(sum(ifnull(count_curr, 0)), 2) as sum_count_prev,
            round(avg(ifnull(count_curr, 0)), 2) as avg_count_prev,
            round(var_pop(ifnull(count_curr, 0)), 2) as variance_value,
            round(stddev_pop(ifnull(count_curr, 0)), 2) as std_dev_value,
            round(stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as sigma_2_value, 
            round(avg(ifnull(count_curr, 0)) - stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as min_thresh_value, 
            round(avg(ifnull(count_curr, 0)) + stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as max_thresh_value
            from top_records 
            group by prfl_id,feature_name,grouped_columns
            order by prfl_id,feature_name,grouped_columns;
            """,
        }
        if comparison_type is None:
            return query['WEEKDAYS']
        if comparison_type.upper() in query:
            return query[comparison_type.upper()]
        return query['WEEKDAYS']
              
    ## Historical Results 
    def get_historical_details(self, prfl_id_list: list, run_date: str, comparison_type: str='WEEKDAYS'):
        hist_record_query = self.historical_queries(comparison_type)
        
        placeholders = {
            "$rpt_table": config.dqaas_profile_rpt,
            "$prfl_id_list": prfl_id_list,
            "$RUN_DT": run_date,
        }
        # hist_record_query = hist_record_query.format(placeholders)
        hist_record_query = reduce(lambda sql, replace_str: sql.replace(*replace_str), [hist_record_query, *list(placeholders.items())])
        df_tbl_hist_rec = self.utils.run_bq_sql(
            bq_auth=config.dq_gcp_auth_payload,
            select_query=hist_record_query
        )
        df_tbl_hist_rec = df_tbl_hist_rec.rename(columns={col: str(col).lower() for col in df_tbl_hist_rec.columns.tolist()})
        self.logger.info(f'Length of the historical groupby list : {len(df_tbl_hist_rec)}')
        self.logger.info(f'\n{df_tbl_hist_rec.columns}\n{df_tbl_hist_rec.head()}')

        if len(df_tbl_hist_rec) == 0:
            self.logger.warning("Historical Data Not found for Identifying the variance")
        
        return df_tbl_hist_rec
        
    ## Comparing Hist and Current Records
    def compare_historical_latest_dimensions(self, df_tbl_hist_rec, df_tbl_latest_rec: pd.DataFrame, comparison_type="WEEKDAYS", isHourly="N"):
        
        ## Historical Records
        self.logger.info('------------------------------------------------------------------')
        df_tbl_hist_rec = df_tbl_hist_rec.rename(columns={col: str(col).lower() for col in df_tbl_hist_rec.columns.tolist()})
        df_tbl_hist_rec['grouped_columns'] = df_tbl_hist_rec['grouped_columns'].fillna(np.nan).astype(str).replace('<NA>',np.nan).replace('nan',np.nan).replace('None',np.nan)
        self.logger.info(f'Length of the historical Records : {len(df_tbl_hist_rec)}')
        self.logger.info(f'\n{df_tbl_hist_rec.columns}\n{df_tbl_hist_rec.head()}')
        
        ## Latest Records
        self.logger.info('------------------------------------------------------------------')
        df_tbl_latest_rec = df_tbl_latest_rec.rename(columns={col: str(col).lower() for col in df_tbl_latest_rec.columns.tolist()})
        df_tbl_latest_rec['grouped_columns'] = df_tbl_latest_rec['grouped_columns'].fillna(np.nan).astype(str).replace('<NA>',np.nan).replace('nan',np.nan).replace('None',np.nan)
        self.logger.info(f'Length of the latest Records : {len(df_tbl_latest_rec)}')
        self.logger.info(f'\n{df_tbl_latest_rec.columns}\n{df_tbl_latest_rec.head()}')

        ## Merging the Historical and Latest Records
        self.logger.info('------------------------------------------------------------------')
        
        join_list = ['prfl_id','feature_name','grouped_columns','weekday']
        
        if comparison_type == 'DTRAN_MONTHLY':
            join_list.remove('weekday')
        
        self.logger.info(f'Historical Dataframe :: {df_tbl_hist_rec}')
        df_tbl_hist_rec.to_csv('/apps/opt/application/smartdq/smartdq_gcp_migration/logs/hist_rec.csv', encoding='utf-8', index=False)
        self.logger.info(f'Latest Dataframe :: {df_tbl_latest_rec}')
        df_tbl_latest_rec.to_csv('/apps/opt/application/smartdq/smartdq_gcp_migration/logs/latest_rec.csv', encoding='utf-8', index=False)
                             
        df_merge_rec= pd.merge(
            df_tbl_hist_rec,
            df_tbl_latest_rec,
            on=join_list,
            how='right'
        )
        
        self.logger.info(f'Length of the Merged Records : {len(df_merge_rec)}')
        self.logger.info(f'\n{df_merge_rec.columns}\n{df_merge_rec.head()}')
        self.logger.info('------------------------------------------------------------------')
        return df_merge_rec
    
    #Create an opsgenie function
    def send_opsgenie_jira_outlier_alert(self, reference_key: str):
        try:
            report_query = f"""                
                SELECT rpt.*, 
                    mtd.data_lob, mtd.data_dmn,mtd.data_sub_dmn, mtd.db_name, mtd.src_tbl,mtd.meas_name,
                    mtd.data_src, mtd.dq_pillar, mtd.opsgenie_api_key, mtd.is_opsgenie_flg,mtd.jira_assignee
                FROM {config.dqaas_profile_mtd} mtd 
                INNER JOIN 
                (SELECT * FROM {config.dqaas_profile_rpt} 
                WHERE rpt_ref_key = '{reference_key}') rpt
                ON mtd.prfl_id = rpt.prfl_id
                WHERE upper(mtd.prfl_type) = 'CUSTOM_RULES'
            """
            self.logger.info(f"Opsgenie Info Query: {report_query}")

            report_df = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=report_query
            )
            
            if len(report_df) == 0:
                self.logger.warning("No records found for Opsgenie alert")
                return
            
            self.logger.info(f"Report DataFrame for Opsgenie Alerts: {report_df}")

            # self.logger.info(f"Report DataFrame head: {report_df.head()}")
            # self.logger.info(f"Report DataFrame head: {report_df.columns}")
            # self.logger.info(f"Report DataFrame head: {report_df.dtypes}")
            #report_df.reset_index(drop=True)

            for idx in report_df.index:
                alert_type = None
                priority = None

                # print(f"idx: {idx}")
                # print(report_df.loc[idx,])
                # print(report_df.loc[idx, 'dq_ind'])

                # Fetch records where dq_status = 'LOW' OR sigma_value = 'outlier'
                if report_df.loc[idx, 'dq_ind'].upper() == "LOW" and report_df.loc[idx, 'sigma_value'].lower() == "outlier":
                    alert_type = 'custom_profile_both_dq_status and sigma_value failed'
                    priority = "P3"  

                elif report_df.loc[idx, 'sigma_value'].lower() == "outlier":
                    alert_type = 'custom_profile_outlier'
                    priority = "P3" 
                elif report_df.loc[idx, 'dq_ind'].upper() == "LOW":
                    alert_type = 'custom_profile_dq_status failed'
                    priority = "P3"
                else:
                    continue
                
                env = config.get_config_values('environment', 'env')
                data_src = {report_df.loc[idx,'data_src']}
                data_sub_dmn = {report_df.loc[idx,'data_sub_dmn']}
                dq_pillar = {report_df.loc[idx,'dq_pillar']}
                db_name = {report_df.loc[idx,'db_name']}
                src_tbl = report_df.loc[idx,'src_tbl']

                if report_df.loc[idx, 'is_opsgenie_flg'].upper() == "Y":
                    profile_type = "custom"
                    api_key = report_df.loc[idx, 'opsgenie_api_key']           
                    # Use default API key
                    if api_key in config.EMPTY_STR_LIST or (isinstance(api_key, float) and math.isnan(api_key)):
                        api_key = config.get_config_values('opsgenie', 'api_key')

                    renamed_report_df = report_df.rename(columns={col: str(col).upper() for col in report_df.columns.tolist()})
                    gcp_http_proxy_url = config.GCP_HTTP_PROXY_URL
                
                    opsgenie_client = Alert(api_key=api_key, proxy=gcp_http_proxy_url)
                    
                    # Send Opsgenie alert
                    response, request_id, message = opsgenie_client.create_opsgenie_alert(
                        renamed_report_df, 0, alert_type, priority, env, profile_type
                    )

                    self.logger.info(f"Opsgenie response code: {response}")
                    self.logger.info(f"Opsgenie alert sent successfully for {alert_type}")

                elif report_df.loc[idx, 'jira_assignee'] is not None: 
                    try:
                        JIRA_ASSIGNEE = report_df.loc[idx, 'jira_assignee']
                        label = "DQaaS"
                        summary = f"LensX | {env} | {data_src} | {data_sub_dmn} | {dq_pillar} | {db_name} | Table: {src_tbl}" | {alert_type}
                        description = f"DQ Issue detected for Table {src_tbl} on Run Date {report_df.loc[idx,'prfl_run_ts']}."
                        jira_client = Jira_ticket()
                        ticket_id = jira_client.create_jira_ticket(JIRA_ASSIGNEE, summary, description, label)
                        self.logger.info(f"Jira ticket created: {ticket_id}")
                    except Exception as err:
                        self.logger.error(f"Error while creating JIRA ticket: {err}")
        
        except Exception as err:
            self.logger.error(f"Error occurred while sending Opsgenie alert. Error: {err}")

    ## Email distros
    def email_distro(self, sub_domain:str): 
        try:
            self.df_email_distro: pd.DataFrame = self.utils.get_email_distros_from_table(data_sub_dmn_list=[sub_domain])
            self.logger.info(f"Email Dataframe: \n{self.df_email_distro}")
            
            receipents_mail_group: list = self.utils.get_mail_distro(
                df_val=self.df_email_distro,
                persona='PERSONA_2',
                sub_dmn=sub_domain
            )
            self.logger.info(f"Email Group: {receipents_mail_group}")
            # receipents_mail_group.append(config.dqaas_default_email_distro)
            receipents_mail_group += config.dqaas_default_email_distro
            
            self.logger.info(f"Final Email Receipents: {receipents_mail_group}")  
            return receipents_mail_group
        except Exception as err:
            self.logger.error(f"Error in retrieving Email Distro. Assigning default email group. Error: {err}")
        return config.dqaas_default_email_distro
           
    ## Summary Report Generation and Sending part
    def send_email_report(self, reference_key: str, sub_domain: str):
        try:
            
            report_query = f"""
                select rpt.*, 
                mtd.data_lob, mtd.data_bus_elem ,mtd.db_name ,mtd.src_tbl ,mtd.dq_pillar 
                from {config.dqaas_profile_mtd} mtd 
                inner join 
                (select * from {config.dqaas_profile_rpt} 
                where rpt_ref_key = '{reference_key}' ) rpt
                on mtd.prfl_id = rpt.prfl_id
                and upper(mtd.prfl_type) = 'CUSTOM_RULES'
            """
            self.logger.info(f"Mail Query : {report_query}")

            report_df = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=report_query
            )
            
            if len(report_df) == 0:
                raise Exception("No Records found for Email Summary")

            
            receipents_mail_group = self.email_distro(sub_domain=sub_domain)
            
            # report_df = report_df[['prfl_run_ts', 'feature_name',
            #     'grouped_columns', 'weekday', 'count_curr', 'avg_count_prev', 'pct_change',
            #     'variance_value', 'std_dev_value', 'sigma_value']]

            # col = [
            #     'count_curr', 'avg_count_prev', 'pct_change',
            #     'variance_value', 'std_dev_value'
            # ]
            
            report_df = report_df[self.summary_cols]
            col = self.email_float_cols

            for c in col:
                report_df[c] = report_df[c].fillna(np.nan).astype('float64').map(self.utils.round_off)
            
            report_df['prfl_run_ts'] = pd.to_datetime(report_df['prfl_run_ts']).dt.date
            report_df = report_df.rename(columns=self.email_rename_cols).reset_index(drop=True)
            
            style_format_dict = {
                'Weekday': '{:,.0f}',
                'Value': '{:,.2f}',
                'Avg Value': '{:,.2f}',
                'Percentage Change': '{:,.2f}',
                'Variance': '{:,.2f}',
                'Std Deviation': '{:,.2f}'
            }

            style_details_dict = {
                'outlier': '{background-color:#BF3131; color:#FFF; font-weight:bold;}'
            }
            validate_column = 'Sigma Value'
            

            email = SendEmail(
                smtp=config.SMTP_SERVER_NAME,
                mail_from=config.SENDER_EMAIL_ID,
                loggerObj=self.logger
            )
            current_date = datetime.strftime(datetime.now() - timedelta(days=1), '%Y-%m-%d')
            email.send_summary_with_highlights(
                email_template_filepath=os.path.join(config.TEMPLATE_DIR, "dq_summary_report_template_with_highlights.html"),
                mail_subject=f"{config.EMAIL_ENV} Custom profile summary for {sub_domain} - {current_date}",
                message='',
                df_val=report_df,
                receipents_email_id=receipents_mail_group,
                style_format=style_format_dict,
                style_details_dict=style_details_dict,
                validate_column=validate_column
            )
        except Exception as err:
            self.logger.error(f"Error in Report Summary Email Block. Error:{err}")
    
    ## Loading Results to Report Table
    def load_to_report_results(self, df_report_result: pd.DataFrame, reference_key: str):
        try:
            ## BigQuery Client Connection
            dbclient, db_creds = self.utils.bigquery_client(
                auth=config.dq_gcp_auth_payload
            )
        
            # Remove duplicate columns
            df_report_result = df_report_result.loc[:, ~df_report_result.columns.duplicated()]

            # Reference Key
            df_report_result["rpt_ref_key"] = reference_key

            # consistecy_score' exists (Fixed the error)
            if 'consistecy_score' not in df_report_result.columns:
                self.logger.warning("Column 'consistecy_score' missing, adding it with default value 0.")
                df_report_result['consistecy_score'] = 0.0

            # Converting numeric columns to correct dtypes
            numeric_columns = [
                "avg_count_prev", "variance_value", "std_dev_value", "sigma_2_value",
                "pct_change", "count_curr", "dq_score", "min_thresh_value", "max_thresh_value", "consistecy_score"
            ]
            for col in numeric_columns:
                if col in df_report_result.columns:
                    df_report_result[col] = pd.to_numeric(df_report_result[col], errors='coerce').fillna(0)

            # Convert int columns (Fixed string issue)
            int_columns = ["weekday", "prfl_id", "rpt_seq_num", "dq_score"]
            for col in int_columns:
                if col in df_report_result.columns:
                    try:
                        df_report_result[col] = pd.to_numeric(df_report_result[col], errors='coerce').fillna(0).astype(int)
                    except Exception as e:
                        self.logger.error(f"Error converting column '{col}' to int: {e}")
                        self.logger.info(f"Problematic column '{col}' Unique Values: {df_report_result[col].unique()}")

            # Convert Date/Datetime columns (Fixed 'NaT' issues)
            datetime_columns = ["data_dt", "insert_date", "prfl_run_ts"]
            for col in datetime_columns:
                if col in df_report_result.columns:
                    df_report_result[col] = pd.to_datetime(df_report_result[col], errors='coerce')

            # Remove future dates ( Unwanted date or future dates causing issues in BigQuery while load)
            if "data_dt" in df_report_result.columns:
                df_report_result = df_report_result[
                    (df_report_result["data_dt"] >= "2000-01-01") & (df_report_result["data_dt"] <= "2100-12-31")
                ]

            # Checking for NaN values in numeric columns
            nan_counts = df_report_result[numeric_columns].isna().sum().sum()
            if nan_counts > 0:
                self.logger.warning(f"Warning: Some numeric values contain NaN Count: {nan_counts}")

            
            self.logger.info(f"Columns before BigQuery Load: {df_report_result.columns.tolist()}")
            self.logger.info(f"Data types before load:\n{df_report_result.dtypes}")

            # Load Data into BigQuery
            self.utils.load_result_to_bq_report_table(
                dq_bq_client=dbclient,
                dq_credentials=db_creds,
                dq_report_table_name=config.dqaas_profile_rpt,
                df_load_data=df_report_result,
                seq_name='rpt_seq_num',
                column_details=self.custom_profile_report_columns
            )

        except Exception as err:
            self.logger.error(f"Error in load_to_report_results: {err}")

    
    ## Consistency Score, DQ Score, Variation Percentage
    @staticmethod
    def calculate_percentage_change(df):
        print("calculate_percentage_change")
        df['count_curr'] = df['count_curr'].fillna(0).astype(float)
        df['avg_count_prev'] = df['avg_count_prev'].astype(float)
        df['min_threshold'] = df['min_threshold'].fillna(config.CUST_MIN_THRESHOLD).astype(float)
        df['max_threshold'] = df['max_threshold'].fillna(config.CUST_MAX_THRESHOLD).astype(float)
        # df['pct_change'] = (((df['count_curr'] - df['avg_count_prev']) / df['count_curr']) * 100).round(2)
        df['pct_change'] = np.where( 
            df['count_curr'].fillna(0) == 0, 0,
            (((df['count_curr'] - df['avg_count_prev']) / df['count_curr']) * 100).round(2)
        )
        # df['consistency_score'] = ((df['count_curr'] / df['avg_count_prev']) * 100).round(2)
        df['consistency_score'] = np.where( 
            df['avg_count_prev'].fillna(0).astype(float) == 0, 0,
            ((df['count_curr'] / df['avg_count_prev']) * 100).round(2)
        )
        df['dq_score'] = np.where(
            (df['min_threshold'] <= df['consistency_score']) & (df['consistency_score'] <= df['max_threshold']),
            100 , 0
        )

    ## 3 Sigma Values
    @staticmethod
    def calculate_sigma_value( df):
        print("calculate_sigma_value")
        df['std_dev_value'] = df['std_dev_value'].astype(float)
        df['count_curr'] = df['count_curr'].fillna(0).astype(float)
        df['avg_count_prev'] = df['avg_count_prev'].astype(float)    
        conditions_null_dim = [
            (df['count_curr'] >= df['avg_count_prev'].fillna(0).astype(float) - 1 * df['std_dev_value'].fillna(0).astype(float)) & (df['count_curr'] <= df['avg_count_prev'].fillna(0).astype(float) + 1 * df['std_dev_value'].fillna(0).astype(float)),
            (df['count_curr'] >= df['avg_count_prev'].fillna(0).astype(float) - 2 * df['std_dev_value'].fillna(0).astype(float)) & (df['count_curr'] <= df['avg_count_prev'].fillna(0).astype(float) + 2 * df['std_dev_value'].fillna(0).astype(float)),
            (df['count_curr'] >= df['avg_count_prev'].fillna(0).astype(float) - 3 * df['std_dev_value'].fillna(0).astype(float)) & (df['count_curr'] <= df['avg_count_prev'].fillna(0).astype(float) + 3 * df['std_dev_value'].fillna(0).astype(float)),
            pd.isna(df['grouped_columns'])
        ]
        choices = [1, 2, 3, 'outlier']
        df['sigma_value'] = np.select(conditions_null_dim, choices, default='outlier')
       
    ## DQ Status  
    @staticmethod
    def calculate_dq_status(df):
        print("calculate_dq_status")
        df['std_dev_value'] = df['std_dev_value'].astype(float)
        df['count_curr'] = df['count_curr'].fillna(0).astype(float)
        df['avg_count_prev'] = df['avg_count_prev'].astype(float)
        df['consistency_score'] = df['consistency_score'].fillna(0).astype(float)
        df['min_thresh_value'] = df['min_thresh_value'].fillna(0).astype(float)
        df['max_thresh_value'] = df['max_thresh_value'].fillna(0).astype(float)
        df['min_threshold'] = df['min_threshold'].fillna(config.CUST_MIN_THRESHOLD).astype(float)
        df['max_threshold'] = df['max_threshold'].fillna(config.CUST_MAX_THRESHOLD).astype(float)

        main_condition = (
            (df['count_curr'].fillna(0) != 0) & 
            (df['max_thresh_value'] >= df['count_curr']) &
            (df['min_thresh_value'] <= df['count_curr']) | 
            (
                (df['count_curr'] == df['avg_count_prev'].fillna(0).astype(float)) &
                (df['std_dev_value'].fillna(0).astype(float) == df['avg_count_prev'].fillna(0).astype(float))
            )
        )
        
        dq_status_condition = [
            main_condition & (( df['min_threshold'] <= df['consistency_score'] ) & ( df['consistency_score'] <= df['max_threshold'])), 
            
            ( main_condition &  (df['consistency_score'] < df['min_threshold'] ) |
            ( (df['count_curr'] != 0 & (df['count_curr'] < df['min_thresh_value'])) &
            (df['consistency_score'] < df['min_threshold'] ) | 
            (( df['min_threshold']  <= df['consistency_score'] ) & ( df['consistency_score'] <= df['max_threshold'])) ) ),
            
            (  main_condition & (df['consistency_score'] > df['max_threshold']) |
            ( (df['count_curr'] != 0 & (df['count_curr'] > df['max_thresh_value'])) &
            (df['consistency_score'] > df['max_threshold']) | (( df['min_threshold']  <= df['consistency_score'] ) & ( df['consistency_score'] <= df['max_threshold'])) ) )
        ]
        choices = ["PASS", "LOW", "HIGH"]
        df['dq_ind'] = np.select(dq_status_condition, choices, default='NA')

    ## Not in Use
    def delete_current_null_records(self, prfl_id_list, business_date):
        try:
            delete_dup_record_query=f'''
                delete from {config.dqaas_profile_rpt}
                where date(rule_profile_dt)={business_date} 
                and prfl_id in ({prfl_id_list}) 
                and weekday = extract(dayofweek from {business_date})  
                and avg_count_prev is null
                and pct_change is null 
                and variance_avg_count_prev is null 
                and std_dev_value is null 
                and sigma_value is null;
            '''
            
            num_recs_deleted = self.utils.run_bq_dml_sql(
                bq_auth=config.dq_gcp_auth_payload,
                dml_query=delete_dup_record_query
            )
            
            self.logger.info(f'Number of latest records deleted :  {num_recs_deleted}')
        except Exception as err:
            self.logger.error(f"Error in Deleting historical records. Error:{err}")
        
    ## Profile Engine 
    def profile_engine(self, df_rules: pd.DataFrame, param_to_replace: dict = None, comparison_type="WEEKDAYS"):
        
        ## Profiling Rules
        # df_result = pd.DataFrame()
        df_result = []
        for i in df_rules.index:
            self.logger.info('------------------------------------------------------------------')

            try:
                result = pd.DataFrame()
                
                prfl_id = df_rules.loc[i, "prfl_id"]
                rule_sql = df_rules.loc[i, "meas_rule_sql"]
                data_src = df_rules.loc[i, "data_src"]
                
                # self.logger.info(f"Index: {i} : Rule ID:: {prfl_id}, \nquery: {query}\n")
                self.logger.info(f"Index:: {i}, Rule ID:: {prfl_id}")
                
                rule_sql = reduce(lambda sql, replace_str: sql.replace(*replace_str), [rule_sql, *list(param_to_replace.items())])
            
                result = self.utils.get_query_data(
                    data_src=data_src,
                    dbname=df_rules.loc[i, "db_name"],
                    select_query=rule_sql
                )
                
                result = result \
                .rename(columns={col: str(col).lower() for col in result.columns.tolist()}) \
                .rename(columns=self.rule_col_rename_list)
                
                
                if len(result) == 0:
                    self.logger.info('No Records Found in Rules')
                    # date_val = self.utils.run_bq_sql(
                    #     bq_auth=config.dq_gcp_auth_payload,
                    #     select_query=f'''select date({param_to_replace["RUN_DT"]}) as run_dt, extract(dayofweek from date({param_to_replace["RUN_DT"]})) as weekday;'''
                    # )
                    self.logger.info(f'Date Results :: \n{self.date_val}')
                    result = pd.DataFrame.from_records([{
                        "data_dt" : self.date_val.loc[0, "run_dt"],
                        "feature_name" : df_rules.loc[i, "feature_name"],
                        "dimension" : np.nan,
                        "count_curr" : 0,
                        "rule_run_ts" : datetime.now(),
                        "weekday" : self.date_val.loc[0, "weekday"],
                    }])

                    if comparison_type == 'DTRAN_MONTHLY':
                        result = result.drop(columns=["weekday"], errors='ignore', axis=1)
                    
                    # result["data_dt"] = date_val.loc[0, "run_dt"]
                    # result["feature_name"] = df_rules.loc[i, "feature_name"]
                    # result["dimension"] = np.nan
                    # result["count_curr"] = 0
                    # result["rule_run_ts"] = datetime.now()
                    # result["weekday"] = date_val.loc[0, "weekday"]
    
                result["prfl_id"] = prfl_id
                
                self.logger.info(f'\n{result}')
                # df_result = df_result.append(result)
                res_val = []
                res_val = result.to_dict("records")
                df_result.extend(res_val)
            except Exception as err:
                self.logger.error(f"Error While Executing Rules: Error{err}")
            
            self.logger.info('------------------------------------------------------------------')
            
        dfEndRes = pd.DataFrame(df_result)
        self.logger.info(f'Length of the Rules Result : {len(dfEndRes)}')
        self.logger.info(f'\n{dfEndRes.columns}\n{dfEndRes.head()}')
           
        if len(dfEndRes) == 0:
            raise ValueError("No Records Found in Custom Rule Profiling.")
        
        # dfEndRes = dfEndRes.rename(columns=self.final_col_rename_list).reset_index(drop=True)
        return dfEndRes.rename(columns=self.final_col_rename_list).reset_index(drop=True)
    
    @staticmethod
    def int_df_to_list(df):
        int_list: list = [i for i in df['prfl_id']]
        return list(set(int_list))
    
    ## Level 2 - Execution Layer : Run Rules, Get Hist Records, Compare the Results, and Find metrics
    def run_metrics_engine(self, df_mtd: pd.DataFrame, start_date, end_date, val_replace_params:dict = None):
        try:

            df_mtd = df_mtd.reset_index(drop=True)
            df_mtd["comparison_type"] = df_mtd["comparison_type"].fillna("WEEKDAYS")
            df_mtd["is_hourly_flg"] = df_mtd["is_hourly_flg"].fillna("N")
            comparisonType = df_mtd.loc[0,"comparison_type"]
            isHourly = df_mtd.loc[0, 'is_hourly_flg']
            self.logger.info(f"Comparison:: {comparisonType}, Hourly:: {isHourly}")
            ## Creating Profile ID list for historical Load
            # rule_id_list: list = []
            # for i in df_mtd['prfl_id']:
            #     rule_id_list.append(i)
            
            print(df_mtd['prfl_id'])
            prfl_id_list = [int(i) for i in df_mtd['prfl_id']]
            prfl_id_list = list(set(prfl_id_list))
            prfl_id_list = f"{prfl_id_list}".replace('[', '').replace(']' ,'')
            self.logger.info(f'Profile ID List : {prfl_id_list}')
        
            ## For null results, weekday and date will be included
            self.date_val = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=f'''
                select date({val_replace_params["RUN_DT"]}) as run_dt,
                extract(dayofweek from date({val_replace_params["RUN_DT"]})) as weekday;
                '''
            )
            
            df_latest = self.profile_engine(
                df_rules=df_mtd,
                param_to_replace=val_replace_params,
            )
            # df_latest = pd.DataFrame(df_latest)
            # df_latest = df_latest.rename(columns=self.final_col_rename_list).reset_index(drop=True)
            df_latest["prfl_run_ts"] = datetime.now().strftime(config.DTM_FMT)
            if isHourly == 'Y':
                df_latest["hour"] = df_latest["grouped_columns"]
                
            # df_latest.to_csv("./cm_res_1.txt")
            
            df_historical = self.get_historical_details(
                run_date=start_date,
                prfl_id_list=prfl_id_list,
                comparison_type=comparisonType
            )
            

            df_merged_dimensions = self.compare_historical_latest_dimensions(
                df_tbl_hist_rec=df_historical,
                df_tbl_latest_rec=df_latest,
                comparison_type=comparisonType,
                isHourly=isHourly,
            )
            
            # df_merged_dimensions.to_csv(os.path.join(os.path.dirname(__file__), "merged_results.txt"))
            mtdCols = ['prfl_id', 'prfl_type', 'src_tbl', 'dq_pillar', 'meas_name', 'min_threshold', 'max_threshold', 'comparison_type', 'is_hourly_flg']
            df_final_result = pd.merge(
                df_merged_dimensions,
                df_mtd[mtdCols],
                on=['prfl_id'],
                how='inner'
            )
            
            
            self.calculate_percentage_change(df_final_result)
            self.calculate_sigma_value(df_final_result)
            self.calculate_dq_status(df_final_result)
            
            # df_merged_dimensions.to_csv(os.path.join(os.path.dirname(__file__), "merged_results_final.txt"))

            self.logger.info('------------------------------------------------------------------')
            self.logger.info(f'Length of the End Results : {len(df_final_result)}')
            self.logger.info(f'\n{df_final_result.columns}\n{df_final_result.head()}')
            self.logger.info('------------------------------------------------------------------')
            # df_final_result.to_csv(os.path.join(os.path.dirname(__file__), "cm_res_3.txt"))
           
            # self.delete_historical_records(hist_date=hist_date, prfl_id_list=prfl_id_list)
            return df_final_result
                
        except Exception as err:
            self.logger.error(f"Error in Run Metrics Main Block. Error {err}")
    
    ## Level 1 - Metrics Execution initiation - entry point for Time based and Engine Initiation - Main Block
    def main_metrics_execution(self, df_mtd: pd.DataFrame, sub_domain: str, start_date: str, end_date: str):
        
        df_mtd = df_mtd.rename(columns={col: str(col).lower() for col in df_mtd.columns.tolist()})
        
        ## Reference Key -> For every new request one ID will be created
        reference_key = datetime.now().strftime('%Y%m%d%H%M%S%f')
        self.logger.info(f"\nRequest Reference:: {reference_key}\nSub Domain:: {sub_domain}")
  
        ## Placeholders for replacing the values in query during execution
        replace_params_for_rules = {"RUN_DT": start_date, "$START_DATE": start_date, "$END_DATE":end_date,}
        
        ## Metrics Initiation
        df_end_result = self.run_metrics_engine(
            df_mtd=df_mtd,
            start_date=start_date,
            end_date=end_date,
            val_replace_params=replace_params_for_rules
        )
        
        if len(df_end_result) == 0 :
            self.logger.error("No Records Found for Loading Custom Metrics")
            return 
        
        ## Loading Results to Report
        self.load_to_report_results(
            df_report_result=df_end_result,
            reference_key=reference_key
        )
        
        ## Send Email
        self.send_email_report(
            reference_key=reference_key,
            sub_domain=sub_domain
        )

        ## Send opsgenie alert
        self.send_opsgenie_jira_outlier_alert(
            reference_key=reference_key
        )

    ## Metadata Retrival    
    def get_metadata(self, add_condition: str = None) -> pd.DataFrame:
            if add_condition in config.EMPTY_STR_LIST:
                add_condition = ""
            
            metadata_query = f"""
                SELECT * FROM {config.dqaas_profile_mtd}
                WHERE prfl_type = 'CUSTOM_RULES'    
                and IS_ACTIVE_FLG = 'Y'    
                {add_condition}                     
                ORDER BY PRFL_ID;
            """
            
            df_val = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=metadata_query
            )
            
            self.logger.info(f"Metadata Length: {len(df_val)}")
            
            if len(df_val) == 0:
                raise ValueError("No Records Found for Custom Metrics")
            
            df_val = df_val.rename(columns={col: str(col).lower() for col in df_val.columns.tolist()})
            return df_val


    def laod_historical_report(self):

        source_data = ''

        try:
            self.logger.info(f"Inside laod_historical_report")

            fetch_sql = f""" select distinct mtd.prfl_id, mtd.prfl_type, mtd.dq_pillar, mtd.src_tbl, mtd.meas_name, mtd.feature_name, meas_rule_sql from {config.dqaas_profile_mtd} mtd
            left join {config.dqaas_profile_rpt} rpt
            on mtd.prfl_id = rpt.prfl_id
            where rpt.prfl_id is null
            and upper(mtd.prfl_type) = 'CUSTOM_RULES'"""

            self.logger.info(f"Fetch SQL executing: {fetch_sql}")

            source_data = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=fetch_sql)

            self.logger.info(f"Fetched new onboarded tables")

        except Exception as err:
                self.logger.error(f"Error While Executing fetch_sql: Error{err}")

        try:
            for index, row in source_data.iterrows():
                prfl_id = row[0]
                prfl_type = row[1]
                dq_pillar = row[2]
                src_tbl = row[3]
                meas_name = row[4].replace(" ","_")
                meas_rule_sql = row[6]

                self.logger.info(f"Orginal meas_rule_sql : {meas_rule_sql}")

                modified_sql = meas_rule_sql.replace("= RUN_DT","between current_date() -1 and current_date() - 92")
                modified_sql = re.sub(r"(?i)null.* as.* dimension","'null' AS dimension", modified_sql)
                modified_sql = re.sub(r"(?i)^SELECT\s",f"""Select 999999 as rpt_seq_num, {prfl_id} as prfl_id, '{prfl_type}' as prfl_type, '{dq_pillar}' as dq_pillar, '{src_tbl}' as src_tbl, '{meas_name}' as meas_name, """, modified_sql)
                modified_sql = re.sub(r"(?i)date\s*\(\s*current_timestamp\s*\)","current_timestamp", modified_sql)
                modified_sql = re.sub(r"(?i)extract.*\(.*date.*current_timestamp\s*\(\s*\)\s*\)","current_timestamp()", modified_sql)
                modified_sql = re.sub(r"(?i)group\s* by\s*.*","group by 1,2,3,4,5,6,7,8,9,11,12;", modified_sql)

                insert_sql = f"""insert into {config.dqaas_profile_rpt} (rpt_seq_num, prfl_id, prfl_type, dq_pillar, src_tbl, meas_name, data_dt,feature_name,grouped_columns,count_curr,prfl_run_ts,weekday) """ + modified_sql

                self.logger.info(f"History SQL for {src_tbl} : {insert_sql}")

                #job = self.utils.client.query(insert_sql)

                insert_status = self.utils.run_bq_dml_sql(
                    bq_auth=config.dq_gcp_auth_payload,
                    dml_query=insert_sql
                )

                self.logger.info('insert_status')
                self.logger.info(insert_status)

        except Exception as err:
                self.logger.error(f"Error While Executing insert_sql: Error{err}")


    def main(self):
        try:
            #self.laod_historical_report()
            df_mtd = self.get_metadata()
            
            sub_domain_list = df_mtd['data_sub_dmn'].unique().tolist()
            start_date = "current_date-1"
            end_date = "current_date-1"
            
            for dmn in sub_domain_list:
                self.main_metrics_execution(
                    df_mtd=df_mtd,
                    sub_domain=dmn,
                    start_date=start_date,
                    end_date=end_date
                )
            
        except ValueError as err:
            self.logger.error(err)
        except Exception as err:
            self.logger.error(f"Error in Main Block. Error {err}")
            
            


# CustomeMetrics().main()
