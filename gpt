from datetime import datetime, timedelta
import pandas as pd
import sys
import os
import logging
import concurrent.futures
import pandas_gbq
import numpy as np
from zoneinfo import ZoneInfo

## Importing User Defined Modules
sys.path.insert(1,os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))
from scripts.common_handlers import CommonUtils, set_logger, get_args_parser
from utils.send_email import SendEmail
import scripts.config_params as config

class SourceCheckAvailability():

    def __init__(self, data_src: str, df_val: pd.DataFrame) -> None:
        self.df = df_val
        self.MAX_THREADS = config.SOURCE_CHECK_THREADS
        self.logger = self.set_src_chk_logger(
            process_name="Src-Chk-Avail",
            data_src=data_src
        )
        self.utils = CommonUtils(logObj=self.logger)
        self.email = SendEmail(
            loggerObj=self.logger,
            mail_from=config.SENDER_EMAIL_ID,
            smtp=config.SMTP_SERVER_NAME
        )
        self.dbclient, self.db_creds = self.utils.bigquery_client(auth=config.dq_gcp_auth_payload)

    # @staticmethod
    def set_src_chk_logger(self, process_name:str, data_src: str):
        timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
        filename = f'{data_src}_src_chk_avail_{timestamp}'
        process_id = os.getpid()
        log: logging = set_logger(
            logger_path=config.SRC_CHK_AVAIL_LOGS,
            log_filename=filename,
            process_name=f'{process_name}-{process_id}',
            no_date_yn="Y",
        )
        return log
    
    def src_chk_run_query(self, input_row: dict):
        result: dict = {}
        result['table_id'] = input_row.get('PROFILE_ID','')
        result['project_name'] = input_row.get('PROJECT_NAME','')
        result['database_name'] = input_row.get('DATABASE_NAME','')
        result['table_name'] = input_row.get('TABLE_NAME','')
        result['data_sub_dmn'] = input_row.get('DATA_SUB_DMN','')
        try:
            if self.data_src == 'GCP':
                if input_row.get('INCR_DATE_COL','') in config.EMPTY_STR_LIST:
                    count_query = f"select count(*) as counts from {result['project_name']}.{result['database_name']}.{result['table_name']}"
                else:
                    count_query = f"select count(*) as counts from {result['project_name']}.{result['database_name']}.{result['table_name']} where {input_row.get('INCR_DATE_COL','')} >= date(current_timestamp(),'US/Eastern') - {input_row.get('INCR_DATE_COND') if input_row.get('INCR_DATE_COND') not in config.EMPTY_STR_LIST else '1'}"
            elif self.data_src == 'TD':
                if input_row.get('INCR_DATE_COL','') in config.EMPTY_STR_LIST:
                    count_query = f"select count(*) as counts from {result['database_name']}.{result['table_name']}"
                else:
                    count_query = f"select count(*) as counts from {result['database_name']}.{result['table_name']} where {input_row.get('INCR_DATE_COL','')} >= date(current_timestamp(),'US/Eastern') - {input_row.get('INCR_DATE_COND') if input_row.get('INCR_DATE_COND') not in config.EMPTY_STR_LIST else '1'}"
            self.logger.info(f"Count Query :: {count_query}")
            result['count_query'] = count_query
            if self.data_src == 'GCP':
                result['count'] = self.dbclient.query(count_query).to_dataframe()['counts'][0] 
            else:
                td_result_df = self.utils.get_query_data(data_src=self.data_src, dbname=result['database_name'], select_query=count_query)
                self.logger.info(f"td df:{td_result_df}")
                result['count'] = td_result_df['counts'][0]
            result['data_availability_indicator'] = 'Y' if result['count'] > 0 else 'N'
            result['message'] = "Count Fetched Successfully"
        except Exception as e:
            #traceback.print_exc()
            self.logger.error(f"Error in src_chk_run_query() Method. Error: {e}")
            result['data_availability_indicator'] = 'E'
            result['count'] = 0
            result['message'] = "Error Occurred"
        self.logger.info(result)
        return result
    
    #Summary and Other Mails
    def send_email_alert(
        self, vsad_name: str = '', message: str = None, subject: str = None,
        df_val=pd.DataFrame(), receipents_email_group: list = None):
        self.logger.info('-------------------------------------------------------------------------')
        self.logger.info('Email Initiated')
        self.logger.info('-------------------------------------------------------------------------')
        try:
            df_val = df_val.drop(['update_made_ts','insert_made_ts'], axis=1)
            subject = f'DEV - DQaaS 2.0 - Source Check Availability' if subject in config.EMPTY_STR_LIST else subject
            message = f'Please find the below Source Check Summary for current run' if message in config.EMPTY_STR_LIST else message

            receipents_email_addr_list: list = None
            receipents_email_addr_list: list = receipents_email_group if receipents_email_group not in config.EMPTY_STR_LIST else  []

            if len(receipents_email_addr_list) == 0:
                self.logger.info(f"Email ID not Found in the Mail Distro Table. Assigning Default mail distro({config.SLA_WATCHER_DEFAULT_MAIL_GROUP})")

            receipents_email_addr_list = receipents_email_addr_list + config.SLA_WATCHER_DEFAULT_MAIL_GROUP

            self.logger.info(f"Receipents e-Mail Group:{receipents_email_addr_list}")

            self.email.send_common_message(
                email_template_filepath=config.common_email_template,
                mail_subject=subject,
                message=message,
                df_val=df_val,
                receipents_email_id=receipents_email_addr_list
            )

            self.logger.info('Email Send Successfully')
        except Exception as e:
            self.logger.error(f"Error Occured in email trigger :: {e}")

    def main(self,data_src, profile_type):
        try:
            count_results = []
            #self.logger.info(self.df.to_string())
            self.data_src = data_src
            self.df_unique = self.df.drop_duplicates(subset=["PROJECT_NAME","DATABASE_NAME", "TABLE_NAME"])
            #self.df_unique = self.df_unique.iloc[:5]
            self.input_rows = self.df_unique.to_dict('records')
            self.logger.info(f"Input Rows :: {len(self.input_rows)}")
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.MAX_THREADS) as executor:
                result_futures = executor.map(self.src_chk_run_query, self.input_rows)
                for future in result_futures:
                    count_results.append(future)

            if len(count_results) > 0:
                count_results_df = pd.DataFrame(count_results)
                count_results_df['retry_count'] = 0
                count_results_df['profile_type'] = profile_type
                count_results_df['server_name'] = np.nan
                count_results_df['run_status'] = 'Ready'
                count_results_df['run_dt'] = np.datetime64(datetime.now(ZoneInfo("US/Eastern")).date())
                count_results_df['update_made_ts'] = np.datetime64(datetime.now(ZoneInfo("US/Eastern")))
                count_results_df['insert_made_ts'] = np.datetime64(datetime.now(ZoneInfo("US/Eastern")))
                self.logger.info(f"Count Results Length {len(count_results_df)}")
                self.logger.info(f"Count Results {count_results_df.head}")
                #count_results_df['count'] = count_results_df['count'].fillna(np.nan).astype('float64')
                #self.logger.info(count_results_df.info())

                self.logger.info(count_results_df.head())
                
                #Split Existing and New tables in control table
                fetch_full_ctrl_tbl = f"""select * from {config.dqaas_src_chk_avail} where cast(run_dt AS DATE) = date(current_timestamp(),'US/Eastern') and profile_type='{profile_type}'"""
                full_ctrl_tbl = self.utils.run_bq_sql(
                    bq_auth=config.dq_gcp_auth_payload,
                    select_query=fetch_full_ctrl_tbl
                )
                ctrl_tbl_n = full_ctrl_tbl[full_ctrl_tbl['data_availability_indicator']=='N']
                if len(full_ctrl_tbl) > 0:
                    index_cols = ['project_name','database_name','table_name'] if data_src=='GCP' else ['database_name','table_name']
                    #Splitting New and Old Records
                    count_results_df.set_index(index_cols, inplace=True)
                    ctrl_tbl_n.set_index(index_cols, inplace=True)
                    full_ctrl_tbl.set_index(index_cols, inplace=True)
                    new_records = count_results_df[~count_results_df.index.isin(full_ctrl_tbl.index)].reset_index()
                    old_records = count_results_df[count_results_df.index.isin(ctrl_tbl_n.index)].reset_index()

                    #Splitting data availability indicator Y/N
                    data_avail_ind_y = old_records[old_records['data_availability_indicator']=='Y']
                    data_avail_ind_n = old_records[old_records['data_availability_indicator']=='N']
                    self.logger.info(f"Data Availability of Existing Data :: Yes Length :: {len(data_avail_ind_y)}, No Length :: {len(data_avail_ind_n)}")
                    #Updating retry_count and data availability indicator for old records
                    if len(data_avail_ind_y) > 0:
                        upd_ctrl_tbl_src_chk_y = f"""update {config.dqaas_src_chk_avail} set data_availability_indicator='Y',retry_count=retry_count+1,update_made_ts=current_timestamp() where cast(run_dt AS DATE) = date(current_timestamp(),'US/Eastern') and profile_type='{profile_type}' and concat(project_name,'.',database_name,'.',table_name) in ({','.join([f"'{p}.{d}.{t}'" for p,d,t in zip(data_avail_ind_y['project_name'],data_avail_ind_y['database_name'],data_avail_ind_y['table_name'])])})"""
                        upd_ctrl_tbl_y = self.utils.run_bq_sql(
                            bq_auth=config.dq_gcp_auth_payload,
                            select_query=upd_ctrl_tbl_src_chk_y
                        )
                        self.logger.info(f"{upd_ctrl_tbl_y.to_string()}")
                    if len(data_avail_ind_n) > 0:
                        upd_ctrl_tbl_src_chk_n = f"""update {config.dqaas_src_chk_avail} set retry_count=retry_count+1,update_made_ts=current_timestamp() where cast(run_dt AS DATE) = date(current_timestamp(),'US/Eastern') and profile_type='{profile_type}' and concat(project_name,'.',database_name,'.',table_name) in ({','.join([f"'{p}.{d}.{t}'" for p,d,t in zip(data_avail_ind_n['project_name'],data_avail_ind_n['database_name'],data_avail_ind_n['table_name'])])})"""
                        upd_ctrl_tbl_n = self.utils.run_bq_sql(
                            bq_auth=config.dq_gcp_auth_payload,
                            select_query=upd_ctrl_tbl_src_chk_n
                        )
                        self.logger.info(f"{upd_ctrl_tbl_n.to_string()}")
                else: new_records = count_results_df

                new_records = new_records.drop(['count'], axis=1)
                #Loading New Records into Control Table
                pandas_gbq.to_gbq(
                    dataframe=new_records,
                    destination_table=config.dqaas_src_chk_avail,
                    if_exists='append',
                    credentials=self.db_creds,
                    project_id=config.DQ_GCP_CONN_PROJECT_ID,
                )
                self.logger.info("Source Check Availability Control table loaded successfully")
                #Summary Email of Source Check Availability
                fetch_ctrl_tbl = f"""select * from {config.dqaas_src_chk_avail} where cast(run_dt AS DATE) = date(current_timestamp(),'US/Eastern') and profile_type='{profile_type}'"""
                full_ctrl_tbl = self.utils.run_bq_sql(
                    bq_auth=config.dq_gcp_auth_payload,
                    select_query=fetch_ctrl_tbl
                )
                full_ctrl_tbl.set_index(index_cols, inplace=True)
                email_df = full_ctrl_tbl[full_ctrl_tbl.index.isin(count_results_df.index)].reset_index()
                self.send_email_alert(df_val=email_df, receipents_email_group=config.SLA_WATCHER_DEFAULT_MAIL_GROUP)
                
        except Exception as e:
            self.logger.error(f"Error in main method of Source Check Availability Process. Error: {e}")
            raise f"Error in main method of Source Check Availability Process. Error: {e}"
