import re
import pandas as pd
import numpy as np
import os
import logging
import sys
from datetime import datetime, timedelta
from functools import reduce
import warnings
import math
warnings.filterwarnings("ignore", category=FutureWarning)

## Importing User Defined Modules
sys.path.insert(1,os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))
import config_params as config
from customer_metrics_and_micro_segment.send_email import SendEmail
from common_handlers import CommonUtils, set_logger, get_args_parser
from scripts.dqaas_opsgenie import Alert
from scripts.dqaas_jira import Jira_ticket




class CustomeMetrics:
    def __init__(self, data_src = None) -> None:
        self.__set_default_values()
        self.logger: logging = set_logger(
            logger_path=config.LOGS_DIR,
            log_filename="CustomMetricsLogs",
            process_name='CRM',
            # no_date_yn="Y",
        )
        
        self.utils = CommonUtils( self.logger)
        self.df_email_distro= pd.DataFrame()
        
    ## Default Values
    def __set_default_values(self):
        self.rule_col_rename_list = {
            'profile_date': 'data_dt',
            'pageName': 'value',
            'pageflow': 'value' ,
            'value': 'count_curr',
            'insert_dt': 'rule_run_ts',
        }
        
        self.final_col_rename_list = {
            'profile_date': 'data_dt',
            'value': 'count_curr',
            'dimension': 'grouped_columns',
            'rule_run_ts': 'prfl_run_ts',
        }
        
        self.custom_profile_report_columns = {
            "NUMERIC": [
                'avg_count_prev', 'variance_value', 'std_dev_value',
                'pct_change', 'count_curr','sigma_2_value','dq_score',
                'consistency_score',
            ],
            "INT64": ['prfl_id', 'weekday', 'rpt_seq_num', ],
            "STRING": [
                'feature_name', 'grouped_columns', 'sigma_value', 'rpt_ref_key', 'data_dt',
                'prfl_type', 'src_tbl', 'dq_pillar', 'meas_name', 'dq_ind'
            ],
            "TIMESTAMP": ['prfl_run_ts'],
        }
        
        self.email_rename_cols = {
            'prfl_run_ts': 'Run Profile Date',
            'data_lob': 'LOB',
            'data_bus_elem': 'Domain Name',
            'db_name': 'DB Name',
            'src_tbl': 'Table Name',
            'feature_name': 'Feature Name',
            'grouped_columns': 'Dimensions',
            'dq_pillar': 'DQ Category',
            'weekday': 'Weekday',
            'count_curr': 'Value',
            'avg_count_prev': 'Avg Value',
            'pct_change': 'Percentage Change',
            'variance_value': 'Variance',
            'std_dev_value': "Std Deviation",
            'sigma_value': 'Sigma Value',
            'consistency_score': 'Consistency Score',
            'dq_score': 'DQ Score',
            'dq_ind': 'Status',
        }
        
        ## Same order used in the Email
        self.summary_cols = [
            'prfl_run_ts', 'data_lob', 'data_bus_elem', 'db_name', 'src_tbl',
            'feature_name', 'grouped_columns', 'dq_pillar',
            'weekday', 'count_curr', 'avg_count_prev', 'pct_change',
            'variance_value', 'std_dev_value', 'sigma_value',
            'consistency_score', 'dq_score', 'dq_ind'
        ]
        
        ## Columns used to round off the float values in Email, 
        ## Applicable Columns :- Should be available in self.summary_cols
        self.email_float_cols = [
           'count_curr', 'avg_count_prev', 'pct_change',
            'variance_value', 'std_dev_value',
            'consistency_score', 'dq_score',
        ]
    
    ## Historical Queries 
    @staticmethod
    def historical_queries(comparison_type: str='WEEKDAYS'):
        query = {
            
            "WEEKDAYS": r"""
            with top_records as 
            (
            select *,row_number() over (partition by prfl_id,feature_name,grouped_columns order by data_dt desc) as row_num 
            from $rpt_table join 
            unnest(GENERATE_DATE_ARRAY(date_sub($RUN_DT,INTERVAL 3 MONTH),$RUN_DT, INTERVAL 1 day)) as interval_Date
            on data_dt = interval_Date
            where prfl_id IN ($prfl_id_list)
            and data_dt != $RUN_DT
            order by prfl_id,feature_name, grouped_columns,data_dt desc
            ) 
            select prfl_id,feature_name,grouped_columns,WEEKDAY,
            round(sum(ifnull(count_curr, 0)), 2) as sum_count_prev,
            round(avg(ifnull(count_curr, 0)), 2) as avg_count_prev,
            round(var_pop(ifnull(count_curr, 0)), 2) as variance_value,
            round(stddev_pop(ifnull(count_curr, 0)), 2) as std_dev_value,
            round(stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as sigma_2_value, 
            round(avg(ifnull(count_curr, 0)) - stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as min_thresh_value, 
            round(avg(ifnull(count_curr, 0)) + stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as max_thresh_value
            from top_records 
            group by prfl_id,feature_name,grouped_columns,WEEKDAY
            order by prfl_id,feature_name,grouped_columns,WEEKDAY;
            """,
            
            "DTRAN_MONTHLY": r"""
            with top_records as 
            (select *,row_number() over (partition by prfl_id,feature_name,grouped_columns order by data_dt desc) as row_num 
            from $rpt_table join 
            unnest(GENERATE_DATE_ARRAY(date_trunc(date_sub($RUN_DT,INTERVAL 3 MONTH), MONTH),$RUN_DT, INTERVAL 1 MONTH)) as interval_Date
            on data_dt = interval_Date
            where prfl_id IN ($prfl_id_list)
            and data_dt != date_trunc(date($RUN_DT), MONTH)
            order by prfl_id,feature_name, grouped_columns,data_dt desc) 
            select prfl_id,feature_name,grouped_columns,
            round(sum(ifnull(count_curr, 0)), 2) as sum_count_prev,
            round(avg(ifnull(count_curr, 0)), 2) as avg_count_prev,
            round(var_pop(ifnull(count_curr, 0)), 2) as variance_value,
            round(stddev_pop(ifnull(count_curr, 0)), 2) as std_dev_value,
            round(stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as sigma_2_value, 
            round(avg(ifnull(count_curr, 0)) - stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as min_thresh_value, 
            round(avg(ifnull(count_curr, 0)) + stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as max_thresh_value
            from top_records 
            group by prfl_id,feature_name,grouped_columns
            order by prfl_id,feature_name,grouped_columns;
            """,
        }
        if comparison_type is None:
            return query['WEEKDAYS']
        if comparison_type.upper() in query:
            return query[comparison_type.upper()]
        return query['WEEKDAYS']
              
    ## Historical Results 
    def get_historical_details(self, prfl_id_list: list, run_date: str, comparison_type: str='WEEKDAYS'):
        hist_record_query = self.historical_queries(comparison_type)
        
        placeholders = {
            "$rpt_table": config.dqaas_profile_rpt,
            "$prfl_id_list": prfl_id_list,
            "$RUN_DT": run_date,
        }
        # hist_record_query = hist_record_query.format(placeholders)
        hist_record_query = reduce(lambda sql, replace_str: sql.replace(*replace_str), [hist_record_query, *list(placeholders.items())])
        df_tbl_hist_rec = self.utils.run_bq_sql(
            bq_auth=config.dq_gcp_auth_payload,
            select_query=hist_record_query
        )
        df_tbl_hist_rec = df_tbl_hist_rec.rename(columns={col: str(col).lower() for col in df_tbl_hist_rec.columns.tolist()})
        self.logger.info(f'Length of the historical groupby list : {len(df_tbl_hist_rec)}')
        self.logger.info(f'\n{df_tbl_hist_rec.columns}\n{df_tbl_hist_rec.head()}')

        if len(df_tbl_hist_rec) == 0:
            self.logger.warning("Historical Data Not found for Identifying the variance")
        
        return df_tbl_hist_rec
        
    ## Comparing Hist and Current Records
    def compare_historical_latest_dimensions(self, df_tbl_hist_rec, df_tbl_latest_rec: pd.DataFrame, comparison_type="WEEKDAYS", isHourly="N"):
        
        ## Historical Records
        self.logger.info('------------------------------------------------------------------')
        df_tbl_hist_rec = df_tbl_hist_rec.rename(columns={col: str(col).lower() for col in df_tbl_hist_rec.columns.tolist()})
        df_tbl_hist_rec['grouped_columns'] = df_tbl_hist_rec['grouped_columns'].fillna(np.nan).astype(str).replace('<NA>',np.nan).replace('nan',np.nan).replace('None',np.nan)
        self.logger.info(f'Length of the historical Records : {len(df_tbl_hist_rec)}')
        self.logger.info(f'\n{df_tbl_hist_rec.columns}\n{df_tbl_hist_rec.head()}')
        
        ## Latest Records
        self.logger.info('------------------------------------------------------------------')
        df_tbl_latest_rec = df_tbl_latest_rec.rename(columns={col: str(col).lower() for col in df_tbl_latest_rec.columns.tolist()})
        df_tbl_latest_rec['grouped_columns'] = df_tbl_latest_rec['grouped_columns'].fillna(np.nan).astype(str).replace('<NA>',np.nan).replace('nan',np.nan).replace('None',np.nan)
        self.logger.info(f'Length of the latest Records : {len(df_tbl_latest_rec)}')
        self.logger.info(f'\n{df_tbl_latest_rec.columns}\n{df_tbl_latest_rec.head()}')

        ## Merging the Historical and Latest Records
        self.logger.info('------------------------------------------------------------------')
        
        join_list = ['prfl_id','feature_name','grouped_columns','weekday']
        
        if comparison_type == 'DTRAN_MONTHLY':
            join_list.remove('weekday')
                             
        df_merge_rec= pd.merge(
            df_tbl_hist_rec,
            df_tbl_latest_rec,
            on=join_list,
            how='right'
        )
        
        self.logger.info(f'Length of the Merged Records : {len(df_merge_rec)}')
        self.logger.info(f'\n{df_merge_rec.columns}\n{df_merge_rec.head()}')
        self.logger.info('------------------------------------------------------------------')
        return df_merge_rec
    
    #Create an opsgenie function
    def send_opsgenie_jira_outlier_alert(self, reference_key: str):
        try:
            report_query = f"""                
                SELECT rpt.*, 
                    mtd.data_lob, mtd.data_sub_dmn, mtd.database_name, mtd.table_name, 
                    mtd.data_src, mtd.dq_pillar, mtd.opsgenie_api, mtd.opsgenie_flag
                FROM {config.dqaas_mtd} mtd 
                INNER JOIN 
                (SELECT * FROM {config.dqaas_profile_rpt} 
                WHERE rpt_ref_key = '{reference_key}' 
                AND (dq_status = 'LOW' OR sigma_value = 'outlier') ) rpt
                ON mtd.profile_id = rpt.profile_id
                WHERE upper(mtd.profile_type) = 'RULE_CUSTOM'
            """
            self.logger.info(f"Opsgenie Info Query: {report_query}")

            report_df = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=report_query
            )
            
            if len(report_df) == 0:
                self.logger.warning("No records found for Opsgenie alert")
                return
            
            self.logger.info(f"Report DataFrame for Opsgenie Alerts: {report_df}")

            for idx in report_df.index:
                alert_type = None
                priority = None

                # Fetch records where dq_status = 'LOW' OR sigma_value = 'outlier'
                if report_df.loc[idx, 'dq_status'].upper() == "LOW":
                    alert_type = 'custom_profile_failed'
                    priority = "P3"  

                if report_df.loc[idx, 'sigma_value'].lower() == "outlier":
                    alert_type = 'custom_profile_outlier'
                    priority = "P3"  

                if alert_type and report_df.loc[idx, 'opsgenie_flag'].upper() == "Y":
                    profile_type = "custom"
                    env = config.get_config_values('environment', 'env')
                    api_key = report_df.loc[idx, 'opsgenie_api']

                    # Use default API key
                    if api_key in config.EMPTY_STR_LIST or (isinstance(api_key, float) and math.isnan(api_key)):
                        api_key = config.get_config_values('opsgenie', 'api_key')

                    report_df = report_df.rename(columns={col: str(col).upper() for col in report_df.columns.tolist()})
                    gcp_http_proxy_url = config.GCP_HTTP_PROXY_URL
                
                    opsgenie_client = Alert(api_key=api_key, proxy=gcp_http_proxy_url)
                    
                    # Send Opsgenie alert
                    response, request_id, message = opsgenie_client.create_opsgenie_alert(
                        report_df, 0, alert_type, priority, env, profile_type
                    )

                    self.logger.info(f"Opsgenie response code: {response}")
                    self.logger.info(f"Opsgenie alert sent successfully for {alert_type}")

                elif report_df.loc[idx, 'JIRA_ASSIGNEE'] is not None: 
                    try:
                        JIRA_ASSIGNEE = report_df.loc[idx, 'JIRA_ASSIGNEE']
                        label = "DQaaS"
                        summary = f"LensX | DQ Issue | Table: {report_df.loc[idx,'TABLE_NAME']}"
                        description = f"DQ Issue detected for Table {report_df.loc[idx,'TABLE_NAME']} on Run Date {report_df.loc[idx,'PRFL_RUN_DT']}."
                        jira_client = Jira_ticket()
                        ticket_id = jira_client.create_jira_ticket(JIRA_ASSIGNEE, summary, description, label)
                        self.logger.info(f"Jira ticket created: {ticket_id}")
                    except Exception as err:
                        self.logger.error(f"Error while creating JIRA ticket: {err}")
        
        except Exception as err:
            self.logger.error(f"Error occurred while sending Opsgenie alert. Error: {err}")
    ## Email distros
    def email_distro(self, sub_domain:str): 
        try:
            self.df_email_distro: pd.DataFrame = self.utils.get_email_distros_from_table(data_sub_dmn_list=[sub_domain])
            self.logger.info(f"Email Dataframe: \n{self.df_email_distro}")
            
            receipents_mail_group: list = self.utils.get_mail_distro(
                df_val=self.df_email_distro,
                persona='PERSONA_2',
                sub_dmn=sub_domain
            )
            self.logger.info(f"Email Group: {receipents_mail_group}")
            # receipents_mail_group.append(config.dqaas_default_email_distro)
            receipents_mail_group += config.dqaas_default_email_distro
            
            self.logger.info(f"Final Email Receipents: {receipents_mail_group}")  
            return receipents_mail_group
        except Exception as err:
            self.logger.error(f"Error in retrieving Email Distro. Assigning default email group. Error: {err}")
        return config.dqaas_default_email_distro
           
    ## Summary Report Generation and Sending part
    def send_email_report(self, reference_key: str, sub_domain: str):
        try:
            
            report_query = f"""
                select rpt.*, 
                mtd.data_lob, mtd.data_bus_elem ,mtd.db_name ,mtd.src_tbl ,mtd.dq_pillar 
                from {config.dqaas_profile_mtd} mtd 
                inner join 
                (select * from {config.dqaas_profile_rpt} 
                where rpt_ref_key = '{reference_key}' ) rpt
                on mtd.prfl_id = rpt.prfl_id
                and upper(mtd.prfl_type) = 'CUSTOM_RULES'
            """
            self.logger.info(f"Mail Query : {report_query}")

            report_df = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=report_query
            )
            
            if len(report_df) == 0:
                raise Exception("No Records found for Email Summary")

            
            receipents_mail_group = self.email_distro(sub_domain=sub_domain)
            
            # report_df = report_df[['prfl_run_ts', 'feature_name',
            #     'grouped_columns', 'weekday', 'count_curr', 'avg_count_prev', 'pct_change',
            #     'variance_value', 'std_dev_value', 'sigma_value']]

            # col = [
            #     'count_curr', 'avg_count_prev', 'pct_change',
            #     'variance_value', 'std_dev_value'
            # ]
            
            report_df = report_df[self.summary_cols]
            col = self.email_float_cols

            for c in col:
                report_df[c] = report_df[c].fillna(np.nan).astype('float64').map(self.utils.round_off)
            
            report_df['prfl_run_ts'] = pd.to_datetime(report_df['prfl_run_ts']).dt.date
            report_df = report_df.rename(columns=self.email_rename_cols).reset_index(drop=True)
            
            style_format_dict = {
                'Weekday': '{:,.0f}',
                'Value': '{:,.2f}',
                'Avg Value': '{:,.2f}',
                'Percentage Change': '{:,.2f}',
                'Variance': '{:,.2f}',
                'Std Deviation': '{:,.2f}'
            }

            style_details_dict = {
                'outlier': '{background-color:#BF3131; color:#FFF; font-weight:bold;}'
            }
            validate_column = 'Sigma Value'
            

            email = SendEmail(
                smtp=config.SMTP_SERVER_NAME,
                mail_from=config.SENDER_EMAIL_ID,
                loggerObj=self.logger
            )
            current_date = datetime.strftime(datetime.now() - timedelta(days=1), '%Y-%m-%d')
            email.send_summary_with_highlights(
                email_template_filepath=os.path.join(config.TEMPLATE_DIR, "dq_summary_report_template_with_highlights.html"),
                mail_subject=f"{config.EMAIL_ENV} Custom profile summary for {sub_domain} - {current_date}",
                message='',
                df_val=report_df,
                receipents_email_id=receipents_mail_group,
                style_format=style_format_dict,
                style_details_dict=style_details_dict,
                validate_column=validate_column
            )
        except Exception as err:
            self.logger.error(f"Error in Report Summary Email Block. Error:{err}")
    
    ## Loading Results to Report Table
    def load_to_report_results(self, df_report_result: pd.DataFrame, reference_key: str):
        try:
            ## BigQuery Client Connection
            dbclient, db_creds = self.utils.bigquery_client(
                auth=config.dq_gcp_auth_payload
            )
        
            df_report_result["rpt_ref_key"] = reference_key
            
            df_report_result = df_report_result.rename(columns={col: str(col).lower() for col in df_report_result.columns.tolist()})
            
            ## Loading Table Level Report
            self.utils.load_result_to_bq_report_table(
                dq_bq_client=dbclient,
                dq_credentials=db_creds,
                dq_report_table_name=config.dqaas_profile_rpt,
                df_load_data=df_report_result,
                seq_name='rpt_seq_num',
                column_details=self.custom_profile_report_columns
            )
            
        except Exception as err:
            self.logger.error(f" {err}")
    
    ## Consistency Score, DQ Score, Variation Percentage
    @staticmethod
    def calculate_percentage_change(df):
        print("calculate_percentage_change")
        df['count_curr'] = df['count_curr'].fillna(0).astype(float)
        df['avg_count_prev'] = df['avg_count_prev'].astype(float)
        df['min_threshold'] = df['min_threshold'].fillna(config.CUST_MIN_THRESHOLD).astype(float)
        df['max_threshold'] = df['max_threshold'].fillna(config.CUST_MAX_THRESHOLD).astype(float)
        # df['pct_change'] = (((df['count_curr'] - df['avg_count_prev']) / df['count_curr']) * 100).round(2)
        df['pct_change'] = np.where( 
            df['count_curr'].fillna(0) == 0, 0,
            (((df['count_curr'] - df['avg_count_prev']) / df['count_curr']) * 100).round(2)
        )
        # df['consistency_score'] = ((df['count_curr'] / df['avg_count_prev']) * 100).round(2)
        df['consistency_score'] = np.where( 
            df['avg_count_prev'].fillna(0).astype(float) == 0, 0,
            ((df['count_curr'] / df['avg_count_prev']) * 100).round(2)
        )
        df['dq_score'] = np.where(
            (df['min_threshold'] <= df['consistency_score']) & (df['consistency_score'] <= df['max_threshold']),
            100 , 0
        )

    ## 3 Sigma Values
    @staticmethod
    def calculate_sigma_value( df):
        print("calculate_sigma_value")
        df['std_dev_value'] = df['std_dev_value'].astype(float)
        df['count_curr'] = df['count_curr'].fillna(0).astype(float)
        df['avg_count_prev'] = df['avg_count_prev'].astype(float)    
        conditions_null_dim = [
            (df['count_curr'] >= df['avg_count_prev'].fillna(0).astype(float) - 1 * df['std_dev_value'].fillna(0).astype(float)) & (df['count_curr'] <= df['avg_count_prev'].fillna(0).astype(float) + 1 * df['std_dev_value'].fillna(0).astype(float)),
            (df['count_curr'] >= df['avg_count_prev'].fillna(0).astype(float) - 2 * df['std_dev_value'].fillna(0).astype(float)) & (df['count_curr'] <= df['avg_count_prev'].fillna(0).astype(float) + 2 * df['std_dev_value'].fillna(0).astype(float)),
            (df['count_curr'] >= df['avg_count_prev'].fillna(0).astype(float) - 3 * df['std_dev_value'].fillna(0).astype(float)) & (df['count_curr'] <= df['avg_count_prev'].fillna(0).astype(float) + 3 * df['std_dev_value'].fillna(0).astype(float)),
            pd.isna(df['grouped_columns'])
        ]
        choices = [1, 2, 3, 'outlier']
        df['sigma_value'] = np.select(conditions_null_dim, choices, default='outlier')
       
    ## DQ Status  
    @staticmethod
    def calculate_dq_status(df):
        print("calculate_dq_status")
        df['std_dev_value'] = df['std_dev_value'].astype(float)
        df['count_curr'] = df['count_curr'].fillna(0).astype(float)
        df['avg_count_prev'] = df['avg_count_prev'].astype(float)
        df['consistency_score'] = df['consistency_score'].fillna(0).astype(float)
        df['min_thresh_value'] = df['min_thresh_value'].fillna(0).astype(float)
        df['max_thresh_value'] = df['max_thresh_value'].fillna(0).astype(float)
        df['min_threshold'] = df['min_threshold'].fillna(config.CUST_MIN_THRESHOLD).astype(float)
        df['max_threshold'] = df['max_threshold'].fillna(config.CUST_MAX_THRESHOLD).astype(float)

        main_condition = (
            (df['count_curr'].fillna(0) != 0) & 
            (df['max_thresh_value'] >= df['count_curr']) &
            (df['min_thresh_value'] <= df['count_curr']) | 
            (
                (df['count_curr'] == df['avg_count_prev'].fillna(0).astype(float)) &
                (df['std_dev_value'].fillna(0).astype(float) == df['avg_count_prev'].fillna(0).astype(float))
            )
        )
        
        dq_status_condition = [
            main_condition & (( df['min_threshold'] <= df['consistency_score'] ) & ( df['consistency_score'] <= df['max_threshold'])), 
            
            ( main_condition &  (df['consistency_score'] < df['min_threshold'] ) |
            ( (df['count_curr'] != 0 & (df['count_curr'] < df['min_thresh_value'])) &
            (df['consistency_score'] < df['min_threshold'] ) | 
            (( df['min_threshold']  <= df['consistency_score'] ) & ( df['consistency_score'] <= df['max_threshold'])) ) ),
            
            (  main_condition & (df['consistency_score'] > df['max_threshold']) |
            ( (df['count_curr'] != 0 & (df['count_curr'] > df['max_thresh_value'])) &
            (df['consistency_score'] > df['max_threshold']) | (( df['min_threshold']  <= df['consistency_score'] ) & ( df['consistency_score'] <= df['max_threshold'])) ) )
        ]
        choices = ["PASS", "LOW", "HIGH"]
        df['dq_ind'] = np.select(dq_status_condition, choices, default='NA')

    ## Not in Use
    def delete_current_null_records(self, prfl_id_list, business_date):
        try:
            delete_dup_record_query=f'''
                delete from {config.dqaas_profile_rpt}
                where date(rule_profile_dt)={business_date} 
                and prfl_id in ({prfl_id_list}) 
                and weekday = extract(dayofweek from {business_date})  
                and avg_count_prev is null
                and pct_change is null 
                and variance_avg_count_prev is null 
                and std_dev_value is null 
                and sigma_value is null;
            '''
            
            num_recs_deleted = self.utils.run_bq_dml_sql(
                bq_auth=config.dq_gcp_auth_payload,
                dml_query=delete_dup_record_query
            )
            
            self.logger.info(f'Number of latest records deleted :  {num_recs_deleted}')
        except Exception as err:
            self.logger.error(f"Error in Deleting historical records. Error:{err}")
        
    ## Profile Engine 
    def profile_engine(self, df_rules: pd.DataFrame, param_to_replace: dict = None, comparison_type="WEEKDAYS"):
        
        ## Profiling Rules
        # df_result = pd.DataFrame()
        df_result = []
        for i in df_rules.index:
            self.logger.info('------------------------------------------------------------------')

            try:
                result = pd.DataFrame()
                
                prfl_id = df_rules.loc[i, "prfl_id"]
                rule_sql = df_rules.loc[i, "meas_rule_sql"]
                data_src = df_rules.loc[i, "data_src"]
                
                # self.logger.info(f"Index: {i} : Rule ID:: {prfl_id}, \nquery: {query}\n")
                self.logger.info(f"Index:: {i}, Rule ID:: {prfl_id}")
                
                rule_sql = reduce(lambda sql, replace_str: sql.replace(*replace_str), [rule_sql, *list(param_to_replace.items())])
            
                result = self.utils.get_query_data(
                    data_src=data_src,
                    dbname=df_rules.loc[i, "db_name"],
                    select_query=rule_sql
                )
                
                result = result \
                .rename(columns={col: str(col).lower() for col in result.columns.tolist()}) \
                .rename(columns=self.rule_col_rename_list)
                
                
                if len(result) == 0:
                    self.logger.info('No Records Found in Rules')
                    # date_val = self.utils.run_bq_sql(
                    #     bq_auth=config.dq_gcp_auth_payload,
                    #     select_query=f'''select date({param_to_replace["RUN_DT"]}) as run_dt, extract(dayofweek from date({param_to_replace["RUN_DT"]})) as weekday;'''
                    # )
                    self.logger.info(f'Date Results :: \n{self.date_val}')
                    result = pd.DataFrame.from_records([{
                        "data_dt" : self.date_val.loc[0, "run_dt"],
                        "feature_name" : df_rules.loc[i, "feature_name"],
                        "dimension" : np.nan,
                        "count_curr" : 0,
                        "rule_run_ts" : datetime.now(),
                        "weekday" : self.date_val.loc[0, "weekday"],
                    }])

                    if comparison_type == 'DTRAN_MONTHLY':
                        result = result.drop(columns=["weekday"], errors='ignore', axis=1)
                    
                    # result["data_dt"] = date_val.loc[0, "run_dt"]
                    # result["feature_name"] = df_rules.loc[i, "feature_name"]
                    # result["dimension"] = np.nan
                    # result["count_curr"] = 0
                    # result["rule_run_ts"] = datetime.now()
                    # result["weekday"] = date_val.loc[0, "weekday"]
    
                result["prfl_id"] = prfl_id
                
                self.logger.info(f'\n{result}')
                # df_result = df_result.append(result)
                res_val = []
                res_val = result.to_dict("records")
                df_result.extend(res_val)
            except Exception as err:
                self.logger.error(f"Error While Executing Rules: Error{err}")
            
            self.logger.info('------------------------------------------------------------------')
            
        dfEndRes = pd.DataFrame(df_result)
        self.logger.info(f'Length of the Rules Result : {len(dfEndRes)}')
        self.logger.info(f'\n{dfEndRes.columns}\n{dfEndRes.head()}')
           
        if len(dfEndRes) == 0:
            raise ValueError("No Records Found in Custom Rule Profiling.")
        
        # dfEndRes = dfEndRes.rename(columns=self.final_col_rename_list).reset_index(drop=True)
        return dfEndRes.rename(columns=self.final_col_rename_list).reset_index(drop=True)
    
    @staticmethod
    def int_df_to_list(df):
        int_list: list = [i for i in df['prfl_id']]
        return list(set(int_list))
    
    ## Level 2 - Execution Layer : Run Rules, Get Hist Records, Compare the Results, and Find metrics
    def run_metrics_engine(self, df_mtd: pd.DataFrame, start_date, end_date, val_replace_params:dict = None):
        try:

            df_mtd = df_mtd.reset_index(drop=True)
            df_mtd["comparison_type"] = df_mtd["comparison_type"].fillna("WEEKDAYS")
            df_mtd["is_hourly_flg"] = df_mtd["is_hourly_flg"].fillna("N")
            comparisonType = df_mtd.loc[0,"comparison_type"]
            isHourly = df_mtd.loc[0, 'is_hourly_flg']
            self.logger.info(f"Comparison:: {comparisonType}, Hourly:: {isHourly}")
            ## Creating Profile ID list for historical Load
            # rule_id_list: list = []
            # for i in df_mtd['prfl_id']:
            #     rule_id_list.append(i)
            
            print(df_mtd['prfl_id'])
            prfl_id_list = [int(i) for i in df_mtd['prfl_id']]
            prfl_id_list = list(set(prfl_id_list))
            prfl_id_list = f"{prfl_id_list}".replace('[', '').replace(']' ,'')
            self.logger.info(f'Profile ID List : {prfl_id_list}')
        
            ## For null results, weekday and date will be included
            self.date_val = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=f'''
                select date({val_replace_params["RUN_DT"]}) as run_dt,
                extract(dayofweek from date({val_replace_params["RUN_DT"]})) as weekday;
                '''
            )
            
            df_latest = self.profile_engine(
                df_rules=df_mtd,
                param_to_replace=val_replace_params,
            )
            # df_latest = pd.DataFrame(df_latest)
            # df_latest = df_latest.rename(columns=self.final_col_rename_list).reset_index(drop=True)
            df_latest["prfl_run_ts"] = datetime.now().strftime(config.DTM_FMT)
            if isHourly == 'Y':
                df_latest["hour"] = df_latest["grouped_columns"]
                
            # df_latest.to_csv("./cm_res_1.txt")
            
            df_historical = self.get_historical_details(
                run_date=start_date,
                prfl_id_list=prfl_id_list,
                comparison_type=comparisonType
            )
            

            df_merged_dimensions = self.compare_historical_latest_dimensions(
                df_tbl_hist_rec=df_historical,
                df_tbl_latest_rec=df_latest,
                comparison_type=comparisonType,
                isHourly=isHourly,
            )
            
            # df_merged_dimensions.to_csv(os.path.join(os.path.dirname(__file__), "merged_results.txt"))
            mtdCols = ['prfl_id', 'prfl_type', 'src_tbl', 'dq_pillar', 'meas_name', 'min_threshold', 'max_threshold', 'comparison_type', 'is_hourly_flg']
            df_final_result = pd.merge(
                df_merged_dimensions,
                df_mtd[mtdCols],
                on=['prfl_id'],
                how='inner'
            )
            
            
            self.calculate_percentage_change(df_final_result)
            self.calculate_sigma_value(df_final_result)
            self.calculate_dq_status(df_final_result)
            
            # df_merged_dimensions.to_csv(os.path.join(os.path.dirname(__file__), "merged_results_final.txt"))

            self.logger.info('------------------------------------------------------------------')
            self.logger.info(f'Length of the End Results : {len(df_final_result)}')
            self.logger.info(f'\n{df_final_result.columns}\n{df_final_result.head()}')
            self.logger.info('------------------------------------------------------------------')
            # df_final_result.to_csv(os.path.join(os.path.dirname(__file__), "cm_res_3.txt"))
           
            # self.delete_historical_records(hist_date=hist_date, prfl_id_list=prfl_id_list)
            return df_final_result
                
        except Exception as err:
            self.logger.error(f"Error in Run Metrics Main Block. Error {err}")
    
    ## Level 1 - Metrics Execution initiation - entry point for Time based and Engine Initiation - Main Block
    def main_metrics_execution(self, df_mtd: pd.DataFrame, sub_domain: str, start_date: str, end_date: str):
        
        df_mtd = df_mtd.rename(columns={col: str(col).lower() for col in df_mtd.columns.tolist()})
        
        ## Reference Key -> For every new request one ID will be created
        reference_key = datetime.now().strftime('%Y%m%d%H%M%S%f')
        self.logger.info(f"\nRequest Reference:: {reference_key}\nSub Domain:: {sub_domain}")
  
        ## Placeholders for replacing the values in query during execution
        replace_params_for_rules = {"RUN_DT": start_date, "$START_DATE": start_date, "$END_DATE":end_date,}
        
        ## Metrics Initiation
        df_end_result = self.run_metrics_engine(
            df_mtd=df_mtd,
            start_date=start_date,
            end_date=end_date,
            val_replace_params=replace_params_for_rules
        )
        
        if len(df_end_result) == 0 :
            self.logger.error("No Records Found for Loading Custom Metrics")
            return 
        
        ## Loading Results to Report
        self.load_to_report_results(
            df_report_result=df_end_result,
            reference_key=reference_key
        )
        
        ## Send Email
        self.send_email_report(
            reference_key=reference_key,
            sub_domain=sub_domain
        )

        ## Send opsgenie alert
        self.send_opsgenie_jira_outlier_alert(
            reference_key=reference_key
        )   

    ## Metadata Retrival    
    def get_metadata(self, add_condition: str = None) -> pd.DataFrame:
            if add_condition in config.EMPTY_STR_LIST:
                add_condition = ""
            
            metadata_query = f"""
                SELECT * FROM {config.dqaas_profile_mtd}
                WHERE prfl_type = 'CUSTOM_RULES'    
                and IS_ACTIVE_FLG = 'Y'    
                {add_condition}                     
                ORDER BY PRFL_ID;
            """
            
            df_val = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=metadata_query
            )
            
            self.logger.info(f"Metadata Length: {len(df_val)}")
            
            if len(df_val) == 0:
                raise ValueError("No Records Found for Custom Metrics")
            
            df_val = df_val.rename(columns={col: str(col).lower() for col in df_val.columns.tolist()})
            return df_val


    def laod_historical_report(self):

        source_data = ''

        try:
            self.logger.info(f"Inside laod_historical_report")

            fetch_sql = f""" select distinct mtd.prfl_id, mtd.prfl_type, mtd.dq_pillar, mtd.src_tbl, mtd.meas_name, mtd.feature_name, meas_rule_sql from {config.dqaas_profile_mtd} mtd
            left join {config.dqaas_profile_rpt} rpt
            on mtd.prfl_id = rpt.prfl_id
            where rpt.prfl_id is null
            and upper(mtd.prfl_type) = 'CUSTOM_RULES'"""

            self.logger.info(f"Fetch SQL executing: {fetch_sql}")

            source_data = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=fetch_sql)

            self.logger.info(f"Fetched new onboarded tables")

        except Exception as err:
                self.logger.error(f"Error While Executing fetch_sql: Error{err}")

        try:
            for index, row in source_data.iterrows():
                prfl_id = row[0]
                prfl_type = row[1]
                dq_pillar = row[2]
                src_tbl = row[3]
                meas_name = row[4].replace(" ","_")
                meas_rule_sql = row[6]

                self.logger.info(f"Orginal meas_rule_sql : {meas_rule_sql}")

                modified_sql = meas_rule_sql.replace("= RUN_DT","between current_date() -1 and current_date() - 92")
                modified_sql = re.sub(r"(?i)null.* as.* dimension","'null' AS dimension", modified_sql)
                modified_sql = re.sub(r"(?i)^SELECT\s",f"""Select 999999 as rpt_seq_num, {prfl_id} as prfl_id, '{prfl_type}' as prfl_type, '{dq_pillar}' as dq_pillar, '{src_tbl}' as src_tbl, '{meas_name}' as meas_name, """, modified_sql)
                modified_sql = re.sub(r"(?i)date\s*\(\s*current_timestamp\s*\)","current_timestamp", modified_sql)
                modified_sql = re.sub(r"(?i)extract.*\(.*date.*current_timestamp\s*\(\s*\)\s*\)","current_timestamp()", modified_sql)
                modified_sql = re.sub(r"(?i)group\s* by\s*.*","group by 1,2,3,4,5,6,7,8,9,11,12;", modified_sql)

                insert_sql = f"""insert into {config.dqaas_profile_rpt} (rpt_seq_num, prfl_id, prfl_type, dq_pillar, src_tbl, meas_name, data_dt,feature_name,grouped_columns,count_curr,prfl_run_ts,weekday) """ + modified_sql

                self.logger.info(f"History SQL for {src_tbl} : {insert_sql}")

                #job = self.utils.client.query(insert_sql)

                insert_status = self.utils.run_bq_dml_sql(
                    bq_auth=config.dq_gcp_auth_payload,
                    dml_query=insert_sql
                )

                self.logger.info('insert_status')
                self.logger.info(insert_status)

        except Exception as err:
                self.logger.error(f"Error While Executing insert_sql: Error{err}")


    def main(self):
        try:
            self.laod_historical_report()
            df_mtd = self.get_metadata()
            
            sub_domain_list = df_mtd['data_sub_dmn'].unique().tolist()
            start_date = "current_date-1"
            end_date = "current_date-1"
            
            for dmn in sub_domain_list:
                self.main_metrics_execution(
                    df_mtd=df_mtd,
                    sub_domain=dmn,
                    start_date=start_date,
                    end_date=end_date
                )
            
        except ValueError as err:
            self.logger.error(err)
        except Exception as err:
            self.logger.error(f"Error in Main Block. Error {err}")
            
            
CustomeMetrics().main()
