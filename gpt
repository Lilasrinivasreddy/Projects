 #------------------PROCESS_VALIDATION_COLUMNS---------------------
        if not process_meta.empty:
            # Converting empty string to null values
            process_meta= process_meta.infer_objects(copy=False).replace(r'^\s*$',np.nan,regex=True)
            
            #Remove whitespaces
            process_meta=process_meta.map(lambda x:x.strip() if isinstance(x,str) else x)

            # Updating the Variable
            dictionary = dict(zip(variable_list, dev_list))
            if "dev" in env:
                process_meta.infer_objects(copy=False).replace(dictionary,inplace=True,regex=True)
            elif "test" in env:
                process_meta.infer_objects(copy=False).replace(variable_list, test_list,  inplace=True,regex=True)
            elif "ple" in env:
                process_meta.infer_objects(copy=False).replace(variable_list, ple_list, inplace=True,regex=True)
            else:
                process_meta.infer_objects(copy=False).replace(variable_list, prod_list, inplace=True,regex=True)

            process_meta = process_meta.astype("string")
             #Updating the datatype for Integer Column
            for col in process_integer_column:
                process_meta[col]=process_meta[col].replace('',np.nan).fillna('0').astype(int)

             #Updating the datatype for Boolean Column
            for col in process_boolean_column:
                process_meta[col]=process_meta[col].replace('',np.nan).map({'Y': True,'N':False}).fillna(False)

             #Checking for mandatory columns
            process_col_check=[]
            for mandatory_col in process_mandatory_column_list:
                if(process_meta[mandatory_col].isna().any() or (process_meta[mandatory_col].astype(str).str.strip()=="").any()):
                    print("checking for mandatory columns")
                    process_col_check.append(mandatory_col)

            process_meta['schedule_interval']=process_meta['schedule_interval'].fillna('').astype(str)
            process_meta['frequency']=process_meta['frequency'].fillna('').astype(str)

            # -----------------------------------Cron Validation-------------------------------------------------------------

            for index,row in process_meta.iterrows():
                dly_cron_vld,wkly_cron_vld,mntly_cron_vld,hrly_cron_vld,chrly_cron_vld,uknwn_cron_vld=0,0,0,0,0,0
                schedule=row['schedule_interval']
                if row['frequency']== 'daily':
                # Valid daily schedule should match: '0 HH  ' or similar (e.g., "30 14  ")
                    daily_pattern = r'^\d{1,2} \d{1,2} \* \* \*$'

                    if not re.match(daily_pattern, schedule):
                        dly_cron_vld+=1
                        raise ValueError(schedule + " : Invalid cron for daily frequency. Expected format: 'MINUTE HOUR  *'.",{schedule})   
                
                elif row['frequency'] == 'weekly':

                # Valid weekly schedule should match: '0 HH  DAY' (e.g., "0 0  MON")
                    weekly_pattern = weekly_pattern = r'^\d{1,2} \d{1,2} \* \* (([0-6](,[0-6])*)|((SUN|MON|TUE|WED|THU|FRI|SAT)(,(SUN|MON|TUE|WED|THU|FRI|SAT))*))$'
                    if not re.match(weekly_pattern, schedule):
                        wkly_cron_vld+=1
                        raise ValueError(schedule + " :Invalid cron for weekly frequency. Expected format: 'MINUTE HOUR  DAY'.")

            
                elif row['frequency'] == 'hourly':

                # Valid hourly schedule should match: '0  ' or 'MINUTE  '
                    hourly_pattern = r'^\d{1,2} \* \* \* \*$'
                    if not re.match(hourly_pattern, schedule):
                        hrly_cron_vld+=1
                        raise ValueError(schedule + " :Invalid cron for hourly frequency. Expected format: 'MINUTE  '.")
                
                elif row['frequency'] == 'monthly':

                # Valid monthly schedule should match: '0 HH DD ' (e.g., "0 0 1 " for the 1st of the month)
                    monthly_pattern = r'^\d{1,2} \d{1,2} ((\d{1,2})(,\d{1,2})*) \* \*$'
                    if not re.match(monthly_pattern, schedule):
                        mntly_cron_vld+=1
                        raise ValueError(schedule + " :Invalid cron for monthly frequency. Expected format: 'MINUTE HOUR DAY '.")

                elif row['frequency'] == 'hourly_custom':

                # Valid custom hourly schedule should match: '*/N  ' or 'MINUTE HOUR1,HOUR2,...  *'
                    custom_hourly_pattern = r'^\d{1,2} (?:\*/\d{1,2}|\d{1,2}(?:,\d{1,2})*) \* \* \*$'
                    if not re.match(custom_hourly_pattern, schedule):
                        chrly_cron_vld+=1
                        raise ValueError(schedule + " :Invalid cron for custom hourly frequency. Expected format: 'MINUTE HOUR1,HOUR2,...  ' or '/N  '.")
                else:
                    uknwn_cron_vld+=1
                    raise ValueError( f"Unknown frequency: {row['frequency' ]}")
                
            
            process_stg = 1
            if len(process_col_check)>0:
                raise ValueError("PROCESS META VALIDATION:\n\n Following are the mandatory column in PROCESS tab and can't be left empty :" + str(process_col_check))
            else:
                load_table(process_meta,process_table_id)
                print("Data loaded to Process staging Table")
        else:
            print("Process Tab is Empty")
            process_stg = 0

===========================================================================================================
  if not process_df.empty:
                process_df.dropna(axis=0, how="all", inplace=True)
                total_process_count = process_df.shape[0]
                obs_poc_cnt = process_df['obs_poc'].isna().sum()
                program_name_validation_cnt = process_df['program_name'].isna().sum()
                application_name_validation_cnt = process_df['application_name'].isna().sum()+process_df.loc[process_df['application_name'].str.strip().str.contains(r'\s',case=False), :].shape[0]
                process_name_validation_cnt = process_df['process_name'].isna().sum() +process_df.loc[process_df['process_name'].str.strip().str.contains(r'\s',case=False), :].shape[0]
                subprocess_name_validation_cnt = process_df['subprocess_name'].isna().sum() +process_df.loc[process_df['subprocess_name'].str.strip().str.contains(r'\s',case=False), :].shape[0]
                process_df["frequency"] = process_df["frequency"].str.lower()
                frequency_validation_cnt = process_df['frequency'].isna().sum() + (total_process_count - process_df['frequency'].str.strip().str.lower().isin(frequency_list).sum())
                process_df['is_critical']= process_df['is_critical'].replace('',"N")
                is_critical_validation_cnt = total_process_count-process_df['is_critical'].fillna('n').str.lower().isin(flags).sum()
                platform_name_validtion_cnt = total_process_count - process_df['platform_name'].fillna('na').str.lower().isin(platform_list).sum()
                env_name_validtion_cnt = total_process_count - process_df['env_name'].fillna('na').str.lower().isin(env_list).sum()
                process_poc_validation_cnt =  total_process_count -  process_df.loc[process_df['process_poc'].fillna('na').str.contains(r'[^@]+@[^@]+\.[^@]+',case=False), :].shape[0]
                scheduler_name_validation_cnt =  total_process_count - process_df['scheduler_name'].fillna('na').str.lower().isin(scheduler_list).sum()
                step_id_validation_cnt =  total_process_count - process_df.loc[process_df['step_id'].fillna('1').str.contains(r'[0-9]',case=False), :].shape[0]
                #process_df['logs_enabled' ]= process_df['logs_enabled'].replace('',"N")
                logs_enabled_validation_cnt =  total_process_count - process_df['logs_enabled'].str.lower().isin(flags).sum()
                process_df['is_mandatory'] = process_df['is_mandatory'].replace('',"N")
                is_mandatory_validation_cnt =  total_process_count - process_df['is_mandatory'].fillna('n').str.lower().isin(flags).sum()
                businessunit_validation_cnt =  total_process_count - process_df['businessunit'].fillna('n').str.lower().isin(businessunit_list).sum()
                vsad_validation_cnt = process_df['vsad'].isna().sum()
                #process_df["timezone"] = process_df["timezone"].fillna("UTC")
                process_df["timezone"] = process_df["timezone"].replace('',"UTC")
                process_df["timezone"] = process_df["timezone"].str.upper()
                timezone_validation_cnt = total_process_count - process_df['timezone'].isin(time_zone_list).sum()
                #process_df_source_filtered = process_df[process_df['source'].notna()]
                process_df_source_filtered = process_df[process_df['source'].str.len() != 0]
                source_type_validation_cnt = process_df_source_filtered.shape[0] - process_df_source_filtered['source_type'].str.lower().isin(source_type_list).sum()
                #process_df_target_filtered = process_df[process_df['target'].notna()]
                process_df_target_filtered = process_df[process_df['target'].str.len() != 0]
                target_type_validation_cnt = process_df_target_filtered.shape[0] - process_df_target_filtered['target_type'].str.lower().isin(target_type_list).sum()
                process_df["logs_trace_id_enabled"] = process_df["logs_trace_id_enabled"].fillna('N')
                logs_trace_id_enabled_validation_cnt =  total_process_count - process_df['logs_trace_id_enabled'].fillna('n').str.lower().isin(flags).sum()
                is_active_validation_cnt =  total_process_count - process_df['is_active'].fillna('n').str.lower().isin(flags).sum()
                process_df_is_active_filtered = process_df[process_df['is_active'].isin(['n','N'])]
                reason_is_active_change_validation_cnt = process_df_is_active_filtered.shape[0] - process_df_is_active_filtered[process_df_is_active_filtered['reason_is_active_change']!='nan'].shape[0]
                process_df['run_date_buffer_interval'] = process_df['run_date_buffer_interval'].replace('',"0")
                run_date_buffer_interval_validation_cnt =  total_process_count -process_df.loc[process_df['run_date_buffer_interval'].fillna('0').str.contains(r'[0-9]',case=False), :].shape[0]              
                process_df['collect_volume']=process_df['collect_volume'].replace('',"false")
                collect_volume_validation_cnt =  total_process_count - process_df['collect_volume'].fillna('false').str.lower().isin(bool_flag).sum()
                process_df_volume_query_filtered = process_df[process_df['collect_volume'].isin(['true','TRUE','True'])] 
                volume_query_validation_cnt = process_df_volume_query_filtered.shape[0] - process_df_volume_query_filtered.loc[~process_df_volume_query_filtered['volume_query'].isna(), 'volume_query'].shape[0]
                #process_df['duration_threshold_low_value'] = process_df['duration_threshold_low_value'].fillna('20')
                #process_df['duration_threshold_high_value'] = process_df['duration_threshold_high_value'].fillna('20')
                process_df['duration_threshold_low_value'] = process_df['duration_threshold_low_value'].replace('',"20")
                process_df['duration_threshold_high_value'] = process_df['duration_threshold_high_value'].replace('',"20")
                duration_threshold_low_value_validation_cnt =  total_process_count - process_df.loc[process_df['duration_threshold_low_value'].fillna('20').str.contains(r'[0-9]',case=False), :].shape[0]
                duration_threshold_high_value_validation_cnt =  total_process_count - process_df.loc[process_df['duration_threshold_high_value'].fillna('20').str.contains(r'[0-9]',case=False), :].shape[0]
                
                process_df['additional_info'] = process_df['additional_info'].replace(['NaN', 'nan', ''], np.nan)
                process_df_additional_info_filtered = process_df.dropna(subset = ['additional_info'])
                
                interval_validation_cnt = 0


                def is_valid_json(json_str):
                    try:
                        json.loads(json_str)
                        return True
                    except json.JSONDecodeError:
                        return False
                    except (ValueError, TypeError):
                        return False

                additional_info_validation_cnt = process_df_additional_info_filtered.shape[0] - process_df_additional_info_filtered.apply(is_valid_json).sum()


                process_df_daily = process_df.loc[(process_df['frequency'].str.lower() =='daily')]
                process_df_weekly = process_df.loc[(process_df['frequency'].str.lower() =='weekly')]
                process_df_monthly = process_df.loc[(process_df['frequency'].str.lower() =='monthly')]
                process_df_hourly = process_df.loc[(process_df['frequency'].str.lower() =='hourly')]
                process_df_custom_hourly = process_df.loc[(process_df['frequency'].str.lower() =='hourly_custom')]
                process_df_fixed = process_df.loc[(process_df['frequency'].str.lower() =='hourly_custom')]

                #interval validation
                hourly_frequencies = ['hourly','fixed','hourly_custom']
                hourly_jobs = process_df[process_df['frequency'].isin(hourly_frequencies)]
                interval_validation_cnt = hourly_jobs['interval'].isna().sum()

                #run_day validation
                daily_run_day = process_df_daily[(process_df_daily['run_day'].str.len() != 7) | (process_df_daily['run_day'].str.contains('0'))]
                #daily_run_day = process_df_daily[(process_df_daily['run_day'].str.len() != 7)]
                fixed_run_day = process_df_fixed[(process_df_fixed['run_day'].str.len() != 7) | (process_df_fixed['run_day'].str.contains('0'))]
                hourly_run_day = process_df_hourly[(process_df_hourly['run_day'].str.len() != 7) | (process_df_hourly['run_day'].str.contains('0'))]
                hourly_custom_run_day = process_df_custom_hourly[(process_df_custom_hourly['run_day'].str.len() != 7) | (process_df_custom_hourly['run_day'].str.contains('0'))]
                weekly_run_day = process_df_weekly[(process_df_weekly['run_day'].str.len() != 7) | (process_df_weekly['run_day'].str.count('1') > 3)]
                monthly_run_day = process_df_monthly[process_df_monthly['run_day'].str.len() > 32] | process_df_monthly[process_df_monthly['run_day'].str.len() < 8]
                
                daily_run_day_cnt = daily_run_day.shape[0]
                fixed_run_day_cnt = fixed_run_day.shape[0]
                hourly_run_day_cnt = hourly_run_day.shape[0]
                hourly_custom_run_day_cnt = hourly_custom_run_day.shape[0]
                weekly_run_day_cnt = weekly_run_day.shape[0]
                monthly_run_day_cnt = monthly_run_day.shape[0]


                daily_count = process_df_daily.shape[0]
                weekly_count = process_df_weekly.shape[0]
                monthly_count = process_df_monthly.shape[0]
                hourly_count = process_df_hourly.shape[0]
                custom_hourly_count = process_df_custom_hourly.shape[0]
                
                sla_cnt = 0
                rh_cnt = 0
                # -----------------------------Process Daily validations-----------------------------------------------------------------------          
                
                d24sla_count=process_df_daily['sla'].str.strip().str.count('24:00:00').sum()
                d24sla_count = d24sla_count +(daily_count -  process_df_daily.loc[process_df_daily['sla'].str.strip().str.contains(r'^\d{2}:\d{2}:\d{2}$',case=False), :].shape[0])
                d24run_hour_count = pd.to_datetime(process_df_daily['run_hour'].str.strip(),format= '%H:%M:%S',  errors='coerce').dt.time.isnull().sum()
                # -----------------------------Process Weekly validations-----------------------------------------------------------------------
                
                w24sla_count=process_df_weekly['sla'].str.strip().str.count('24:00:00').sum()
                w24sla_count = w24sla_count +(weekly_count -  process_df_weekly.loc[process_df_weekly['sla'].str.strip().str.contains(r'^\d{2}:\d{2}:\d{2}$',case=False), :].shape[0])
                w24run_hour_count = pd.to_datetime(process_df_weekly['run_hour'].str.strip(),format= '%H:%M:%S',  errors='coerce').dt.time.isnull().sum()
                # -----------------------------Process Monthly validations-----------------------------------------------------------------------
                
                m24sla_count=process_df_monthly['sla'].str.strip().str.count('24:00:00').sum()
                m24sla_count = m24sla_count +(monthly_count -  process_df_monthly.loc[process_df_monthly['sla'].str.strip().str.contains(r'^[0-9]{2,4}:\d{2}:\d{2}$',case=False), :].shape[0])
                m24run_hour_count = pd.to_datetime(process_df_monthly['run_hour'].str.strip(),format= '%H:%M:%S',  errors='coerce').dt.time.isnull().sum()
  
                # -----------------------------Process Hourly validations-----------------------------------------------------------------------
                h24sla_count=process_df_hourly['sla'].str.strip().str.contains('24:00:00').sum()
                h24run_hour_count=process_df_hourly['run_hour'].str.strip().str.contains('24:00:00').sum()

                # -----------------------------Process custom hourly validations-----------------------------------------------------------------------
                ch24sla_count=process_df_custom_hourly['sla'].str.strip().str.contains('24:00:00').sum()
                ch24run_hour_count=process_df_custom_hourly['run_hour'].str.strip().str.contains('24:00:00').sum()

                process_alert_notification_df.dropna(axis=0, how="all", inplace=True)
                all_columns = list(process_alert_notification_df) # Creates list of all column headers
                process_alert_notification_df[all_columns] = process_alert_notification_df[all_columns].astype(str)
                total_alert_notification_df_count = process_alert_notification_df.shape[0]
                application_name_validation_count_a= process_alert_notification_df['application_name'].isna().sum()+process_alert_notification_df.loc[process_alert_notification_df['application_name'].str.strip().str.contains(r'\s',case=False), :].shape[0]
                program_name_validation_count_a	 = process_alert_notification_df['program_name'].isna().sum()
                process_name_validation_count_a = process_alert_notification_df['process_name'].isna().sum() +process_alert_notification_df.loc[process_alert_notification_df['process_name'].str.strip().str.contains(r'\s',case=False), :].shape[0]
                platform_name_validation_count_a = total_alert_notification_df_count - process_alert_notification_df['platform_name'].fillna('na').str.lower().isin(platform_list).sum()	
                env_name_validation_count_a = total_alert_notification_df_count - process_alert_notification_df['env_name'].fillna('na').str.lower().isin(env_list).sum()
                duplicate_rows = process_alert_notification_df.duplicated(subset=['application_name', 'program_name','process_name','env_name'])
                process_alert_notification_duplicate_count_a = duplicate_rows.sum()
                alert_failure_validation_count_a =  total_alert_notification_df_count - process_alert_notification_df['alert_failure'].fillna('N').str.lower().isin(flags).sum()
                process_alert_notification_df["process_failed_alert_priority"] = process_alert_notification_df["process_failed_alert_priority"].fillna('P3').replace('',"P3")
                process_failed_alert_priority_validation_count_a = total_alert_notification_df_count  - process_alert_notification_df["process_failed_alert_priority"].isin(priority_list).sum()
                alert_sla_misses_validation_count_a =  total_alert_notification_df_count - process_alert_notification_df['alert_sla_misses'].fillna('N').str.lower().isin(flags).sum()
                process_alert_notification_df["process_sla_miss_alert_priority"] = process_alert_notification_df["process_sla_miss_alert_priority"].fillna("P3").replace('',"P3")
                process_sla_miss_alert_priority_validation_count_a =  total_alert_notification_df_count  - process_alert_notification_df["process_sla_miss_alert_priority"].isin(priority_list).sum()
                alert_pending_validation_count_a =  total_alert_notification_df_count - process_alert_notification_df['alert_pending'].fillna('N').str.lower().isin(flags).sum() 
                process_alert_notification_df["process_pending_alert_priority"] = process_alert_notification_df["process_pending_alert_priority"].fillna("P3").replace('',"P3")
                process_pending_alert_priority_validation_count_a = total_alert_notification_df_count  - process_alert_notification_df["process_pending_alert_priority"].isin(priority_list).sum()
            
================================================================================================================================================
Attribute Name	process_name	process_description	dag_id	project_id	composer_instance_name	vsad	project_space	job_type	data_stream	load_type	platform_name	job_url	scheduler_name	timezone	schedule_interval	frequency	max_active_runs	sla	interval	tracex_tags	run_date_buffer_interval	sla_alert_buffer	duration_threshold_high_value	alert_channel	alert_channal_target_name	alert_additional_info	alert_failure_by	alert_priority	alert_failure	alert_slamiss	alert_longrunning	alert_startoverdue	is_critical	logs_trace_id_enabled	sla_type	code_gcs_location	code_git_link	prod_request_number	production_date	notify_email_success	notify_email_failure	notify_email_slamiss	notify_email_longrunning	notify_email_startoverdue	notify_slack_success	notify_slack_failure	notify_slack_slamiss	notify_slack_longrunning	notify_slack_startoverdue	dev_poc	dev_poc_manager	devgroup_email	mod_poc	legacy_esp_application_name	legacy_esp_process_name	runbook_link	logs_enabled	upstream	additional_details_json	downstream
Mandatory	Y	Y	Y	Y	Y	Y	Y	Y	Y	Y	Y	Y	Y	Y	Y	Y	N	Y	N	N	N	N	N	Y	Y	N	Y	Y	Y	Y	N	N	Y	Y	Y	Y	Y	Y	Y	N	N	N	N	N	N	N	N	N	N	Y	Y	Y	Y	N	N	Y	Y	N		N
Description	"Process Name is a meaning name of the job purpose . Process Name should be unique across project spaces.
Note: Need to check on the validaiton of duplicate dag while creating the dag/.py file."	Job Description	dg_${vsad}_${project_space}_${process_name}_(rt for realtime jobs)		Update the production composer Instance of the job where we schedule the job.	Need to the VSAD name from the drop down.	Project space/Server name where the jobs belongs to	If job performs both Injestion and Curation both. What we gonna with the value ?	Need to update Batch/Realtime		"Platform where the application belongs to
GCP,Teradata,Unix,Hadoop"	Dag/ESPX job url.	Job Scheduler Name (ESPX/Airflow/Cron)	"Please mention the Timezone you ar selecting in the DPF metadata or dag. (Default will be UTC).
Please choose the EST for all the ESPX application jobs."	Keep the scheler in the cron based pattern.  (minute hour day month day of week (0-6). Sample value  - 30 02 * * *. (Run daily at 02:30 AM). Ref: https://crontab.guru/		Number of parallel instanced allowed to run	"Keep the sla in minutes (how much time the job takes). SLA for 30 02 * * * gonna be - 04:30 PM

If SLA falls on the next day 1:30 then specify as 25:30:00


"	values in minutes. Required for hourly process to identify how much time should we wait for the job from the scheduled time to map with the instance 	Label name to group the jobs for monitoring in observability tool, can contain multiple values	Number of days to the process date adjusted with the data date	"HH:MM:SS of buffer time to be added with SLA time before sending SLA miss notification/alert
"	"Optional. Required only if need to monitoring job duration alerts. Defult is empty
""threshold high value for duration notifications.
value should be in percentage.(default value is 20%)"""	Which channel the alert /incident should be created	VZ-AID-DataOperations,OPSEXC	Json format when need additional info required on the dashboard	Who sends the alert if the processing framework or Observability Framework	Priority info of the failure		If alert need to be created on SLA breaches	whom do we need to send alert for long runings jobs. If this colume is empty, there will be no duration alert	whom do we need to send alert for start overdue. If this colume is empty, there will be no overdue alert	Is this pipeline critical/not		Update the process sla critically. Choose if the process is sla critical go with critical/high else go with bau from the drop down.	Update the code GCS bucket location where the code will be get deployed.	Update the code git repository link.	Update the pr request/jira/merge request number here for any production deployments.	Update the production_date/cut_over_date here in the format mentioned in the sample (YYYY-MM-DD).	email address  for success notifcations	email address  for failure notifcations	email address  for breach notifcations	email address  for breach notifcations	email address  for breach notifcations	Please mention the Slack Channel name to recive the sla misses over the slack.	Please mention the Slack Channel name to recive the Failure notifications over the slack.	Please mention the Slack Channel name to recive the Success notifications over the slack.	Please mention the Slack Channel name to recive the Long Running Jobs notifications over the slack.	Please mention the Slack Channel name to recive the StartOverDue notifications over the slack.	Need to provide source system poc - Try avoiding individual id's instead mentioned the email destro possibly.	Developer Email who built the job	we need to use only poc_application by keeping only the email destro if needed we need to append the dev user by comma seperated	modernization DEV POC name	Mention the legacy/esp application name Ex: UDM#5STR else with NA	Mention the Legacy/ESPX Long Job Name here UDM_NETA_NA_T160_LD else with NA	Update the runbook link to refer for any process steps and fail over/debug steps.	If custom logs generated and send to observability ? Y for DPF	Up Stream Job Name		Down Sream Job Name
Sample	soi_aggr_accessoriesrec15_bt	This is been built to load the Message Codes	dg_gk1v_cwlspr_soi_aggr_accessoriesrec15_bt	vz-it-gk1v-cwlspr-0	vz-it-pr-wdwg-aidcom-0-cc-aidconsumer-2	gk1v	cwlspr	ing	Batch	incremental	GCP	https://0caf2f7c1c00422c84a0a833c7cebe50-dot-us-east4.composer.googleusercontent.com/dags/gk1v_cwlspr_soi_aggr_accessoriesrec15_bt/grid	Airflow	EST	30 02 * * *	daily		04:30:00	90	5G,Oneex	1	04:30:00	20	Opsgenie	VZ-AID-DataOperations,OPSEXC	{"label":"ACT-38"}	TraceX	P3	N	Y	N	Y	Y	Y	medium		"https://gitlab.verizon.com/vz-data-engineering/consumer_wireless_egress_soi/-/blob/main/users/metadata/pipeline_metadata_soi_aggr_accessoriesrec15_bt.json
https://gitlab.verizon.com/vz-data-engineering/consumer_wireless_egress_soi/-/blob/main/users/sql/soi_aggr_accessoriesrec15_bt.sql"		2024-07-01	user.1@verizon.com,group.2@verizon.com	user.1@verizon.com,group.2@verizon.com	user.1@verizon.com,group.2@verizon.com			dataops_sla_alerts	dataops_failure_alerts	dataops_success_alerts	dataops_longrunning_alerts	dataops_startoverdue_alerts		user.1@verizon.com		For first 2-3 months, need the mod POC name	UDM#5STR	UDM_NETA_NA_T160_LD	https://docs.google.com/document/d/1Rw99UOLQeEA_a-uG8Zm8-E4qE2n-8KqmJTTuP_nnhQo/edit#heading=h.kf44bofefi4u	Y			
	ext_workday_emp_raw_to_curate	This is been built to load the Message Codes	ext_workday_emp_raw_to_curate	vz-it-gk1v-cwlspr-1	vz-it-pr-wdwg-aidcom-0-cc-aidconsumer-3	J0NV	cwlspr	ing	Batch	incremental	GCP	https://0caf2f7c1c00422c84a0a833c7cebe50-dot-us-east4.composer.googleusercontent.com/dags/gk1v_cwlspr_soi_aggr_accessoriesrec15_bt/grid	Airflow	UTC	15 09 * * *	daily		23:15:00	0					Opsgenie	<wireless_failure>		TraceX	P3	N	Y	N	N	Y	Y	medium				2024-07-02	<success_notification_list>	<failure_notification_list>	<sla_miss_notification_list>																		
	gk1v_edwpr_udm_neta_na_t160_ld	This is been built to load the Message Codes	gk1v_edwpr_udm_neta_na_t160_ld	vz-it-gk1v-cwlspr-2	vz-it-pr-wdwg-aidcom-0-cc-aidconsumer-4	gk1v	cwlspr	ing	Batch	incremental	GCP	https://0caf2f7c1c00422c84a0a833c7cebe50-dot-us-east4.composer.googleusercontent.com/dags/gk1v_cwlspr_soi_aggr_accessoriesrec15_bt/grid	Airflow	UTC	00 01 * * *	daily		23:59:59	0					Opsgenie	<wireless_failure>		TraceX	P3	N	Y	N		Y	Y	medium				2024-07-03																					
	"        
gk1v_edwpr_mdr_wkfl_200_bdms_stg_edm_dw"	This is been built to load the Message Codes	gk1v_edwpr_mdr_wkfl_200_bdms_stg_edm_dw	vz-it-gk1v-cwlspr-3	vz-it-pr-wdwg-aidcom-0-cc-aidconsumer-5	gk1v	cwlspr	ing			GCP		Airflow	EST	45 00 * * *	daily		0:45:10	0																																									
	gk1v_edwpr_oars_sabit_trn_fleet_wex	This is been built to load the Message Codes	gk1v_edwpr_oars_sabit_trn_fleet_wex	vz-it-gk1v-cwlspr-4	vz-it-pr-wdwg-aidcom-0-cc-aidconsumer-6	gk1v	edwpr		Batch	incremental	GCP		Airflow	EST	00 01 * * *	daily		23:59:59	0				20	Opsgenie				P3	N	Y	N	N	Y		medium																									
	gk1v_edwpr_udm_neta_na_t160_lw	This is been built to load the Message Codes	gk1v_edwpr_udm_neta_na_t160_lw	vz-it-gk1v-cwlspr-5	vz-it-pr-wdwg-aidcom-0-cc-aidconsumer-7	gk1v	edwpr		Batch	incremental	GCP		Airflow	EST	00 01 * * *	daily		1:00:01	0				20	Opsgenie				P3	N	Y	N	N	Y		medium																									
