import pdfplumber
from datetime import datetime

def pdf_to_markdown(filepath):
    markdown_text = ""
    with pdfplumber.open(filepath) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                markdown_text += text.strip() + "\n\n"
    return markdown_text.strip()

if __name__ == "__main__":
    filepath = "VCG CEO Daily Performance - Contra GA Walk.pdf"
    
    try:
        markdown_output = pdf_to_markdown(filepath)
        print(markdown_output[:1000])
        print("\n--- Extraction Complete ---")
    except Exception as e:
        print(f"Error: {e}")







CREATE TABLE report_table (
    report_name TEXT NOT NULL,
    report_details TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);




@staticmethod
def insert_report(record: dict):
    from src.database_connections.app_database.models import ReportTable  # your ORM model
    session = AppDatabase.get_session()
    try:
        report = ReportTable(**record)
        session.add(report)
        session.commit()
        return {"status": "success", "id": report.id}
    except Exception as e:
        session.rollback()
        raise e
    finally:
        session.close()



from datetime import datetime
from PyPDF2 import PdfReader
from src.database_connections.app_database.service import AppDatabase

def convert_pdf_to_markdown_local(filepath):
    reader = PdfReader(filepath)
    markdown = ""
    for page in reader.pages:
        text = page.extract_text()
        if text:
            markdown += text.strip() + "\n\n"
    return markdown.strip()

if __name__ == "__main__":
    filepath = "VCG CEO Daily Performance - Contra GA Walk.pdf"
    report_name = "VCG CEO Daily Performance - Contra GA Walk"

    try:
        markdown_content = convert_pdf_to_markdown_local(filepath)
        record = {
            "report_name": report_name,
            "report_details": markdown_content,
            "created_at": datetime.utcnow()
        }
        result = AppDatabase.insert_report(record)
        print(f"✅ Report inserted: {result}")
    except Exception as e:
        print(f"❌ Failed to save report: {e}")




1.) Add a endpoint to accept PDFs under contexts router 2.)  Push this into a new table `report_table` columns : report _name , report_details (markdown of pdf) , created_at 3.) converse endpoint under contexts 4.)

###controller.py###
import uuid
from datetime import datetime
from fastapi import APIRouter, File, Form, UploadFile, Query, Body, Response
from pydantic import BaseModel
from fastapi import HTTPException
from src.app.routers.contexts.service import Contexts
from src.utils.error_handlers import handle_internal_exception, handle_value_error
from src.utils.log_wrapper import start_new_session, logger_capture
from src.utils.utils import require_session_id
from typing import List, Optional, Literal, Any

router = APIRouter()


def convert_categorical_definitions_to_dict(categorical_definitions_list):
    """Convert list of key-value objects to dict for database storage"""
    if not categorical_definitions_list:
        return None
    # Handle new format: list of objects like [{k: v}, {k2: v2}]
    result = {}
    for item in categorical_definitions_list:
        if isinstance(item, dict):
            # Handle new format: {k: v} objects
            result.update(item)
    return result

class ColumnUpdatePayload(BaseModel):
    tableid: int
    columnid: int
    name: str = None
    data_type: str = None
    description: str = None
    column_alias: str = None
    categorical_values: list[str] = None
    categorical_definitions: list[dict] = None 


class RuleUpdatePayload(BaseModel):
    tableid: int
    ruleid: int
    rule_definition: str = None


class TableMetadataPayload(BaseModel):
    tableid: int
    table_description: str
    usage_patterns: list[str] = None

class Table(BaseModel):
    name: str 
    alias: str


class Join(BaseModel):
    type: Literal["INNER", "LEFT", "RIGHT", "FULL OUTER"]
    left_table: str
    right_table: str 
    on: str


class SelectItem(BaseModel):
    type: Literal["dimension", "metric"]
    expression: str
    alias: Optional[str] = None


class CaseCondition(BaseModel):
    when: str
    then: str


class CaseLogic(BaseModel):
    type: Literal["CASE_STATEMENT"]
    conditions: List[CaseCondition]
    else_clause: str


class CustomDimension(BaseModel):
    name: str
    logic: CaseLogic


class Filter(BaseModel):
    column: str
    operator: str
    value: Optional[Any] = None
    value_placeholder: Optional[str]


class Rule(BaseModel):
    """New rule structure without rule_id for new rules"""
    description: str
    section: str
    tables: List[Table]
    joins: Optional[List[Join]] = None
    select: List[SelectItem]
    custom_dimensions: Optional[List[CustomDimension]] = None
    group_by: Optional[List[str]] = None
    filters: Optional[List[Filter]] = None

class CatalogRulePayload(BaseModel):
    domain: str
    catalogs: list[str] = []
    catalog_rules: list[Rule]
    
class ColumnCreatePayload(BaseModel):
    tableid: int
    name: str = None
    data_type: str = None
    description: str = None
    column_alias: str = None
    categorical_values: list[str] = None
    categorical_definitions: list[dict] = None 


class RuleCreatePayload(BaseModel):
    tableid: int
    rule_definition: str = None


class ReasoningTracesPayload(BaseModel):
    domain: str
    catalog: str
    question: str
    sql_query: str


class ContextObject(BaseModel):
    table: dict
    columns: list[dict] = []
    rules: list[dict] = []


@router.put("/upload_contexts", status_code=201)
async def upload_contexts(
    files: list[UploadFile] = File(...),
    domain: str = Form(...),
    catalogs: list[str] = Query(...),
    archive: bool = Form(...),
    response: Response = None,
):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "Upload Contexts")
    try:
        session_id = require_session_id()
        result = Contexts.upload_contexts(session_logger, files, domain, catalogs, archive, session_id=session_id)
        if response is not None:
            response.headers["transaction_id"] = transaction_id
        return result
    except ValueError as e:
        if response is not None:
            response.headers["transaction_id"] = transaction_id
        raise handle_value_error(session_logger, transaction_id, e)
    except Exception as e:
        if response is not None:
            response.headers["transaction_id"] = transaction_id
        raise handle_internal_exception(session_logger, transaction_id, e)
    finally:
        session_logger.save_logs()


@router.post("/add_column", status_code=201)
async def add_column(payload: list[ColumnCreatePayload], response: Response = None):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "add column")
    session_id = require_session_id()
    
    # Convert list of payloads to list of dicts with session_id and format conversion
    payloads = []
    for p in payload:
        payload_dict = p.dict()
        payload_dict["session_id"] = session_id
        # Convert categorical_definitions from list to dict for database
        if payload_dict.get("categorical_definitions"):
            payload_dict["categorical_definitions"] = convert_categorical_definitions_to_dict(
                payload_dict["categorical_definitions"]
            )
        payloads.append(payload_dict)
    
    result = Contexts.add_column(session_logger, payloads)
    if response is not None:
        response.headers["transaction_id"] = transaction_id
    return result


@router.put("/update_column", status_code=200)
async def update_column(payload: list[ColumnUpdatePayload], response: Response = None):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "update column")
    session_id = require_session_id()
    
    # Add session_id and convert categorical_definitions format for each payload
    payloads = []
    for p in payload:
        payload_dict = p.dict()
        payload_dict["session_id"] = session_id
        # Convert categorical_definitions from list to dict for database
        if payload_dict.get("categorical_definitions"):
            payload_dict["categorical_definitions"] = convert_categorical_definitions_to_dict(
                payload_dict["categorical_definitions"]
            )
        payloads.append(payload_dict)
    
    result = Contexts.update_columns(session_logger, payloads)
    if response is not None:
        response.headers["transaction_id"] = transaction_id
    return result


@router.delete("/delete_column/{columnid}", status_code=200)
async def delete_column(tableid: int = Query(...), columnid: int = None, response: Response = None):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "delete column")
    session_id = require_session_id()
    result = Contexts.delete_column(session_logger, tableid, columnid, session_id=session_id)
    if response is not None:
        response.headers["transaction_id"] = transaction_id
    return result


@router.post("/add_rule", status_code=201)
async def add_rule(payload: list[RuleCreatePayload], response: Response = None):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "add rule")
    session_id = require_session_id()
    # Convert list of payloads to list of dicts with session_id
    payloads = [dict(p.dict(), session_id=session_id) for p in payload]
    result = Contexts.add_rule(session_logger, payloads)
    if response is not None:
        response.headers["transaction_id"] = transaction_id
    return result


@router.put("/update_rule", status_code=200)
async def update_rule(payload: list[RuleUpdatePayload], response: Response = None):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "update rule")
    session_id = require_session_id()
    payloads = [dict(p.dict(), session_id=session_id) for p in payload]
    result = Contexts.update_rules(session_logger, payloads)
    if response is not None:
        response.headers["transaction_id"] = transaction_id
    return result


@router.delete("/delete_rule/{ruleid}", status_code=200)
async def delete_rule(tableid: int = Query(...), ruleid: int = None, response: Response = None):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "delete rule")
    session_id = require_session_id()
    result = Contexts.delete_rule(session_logger, tableid, ruleid, session_id=session_id)
    if response is not None:
        response.headers["transaction_id"] = transaction_id
    return result


@router.put("/update_table_metadata", status_code=200)
async def update_table_metadata(payload: TableMetadataPayload, response: Response = None):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "update table metadata")
    session_id = require_session_id()
    payload_dict = payload.dict()
    payload_dict["session_id"] = session_id
    result = Contexts.update_table_metadata(session_logger, payload_dict)
    if response is not None:
        response.headers["transaction_id"] = transaction_id
    return result

@router.post("/add_catalog_rule", status_code=200)
async def add_catalog_rule(payload: CatalogRulePayload, response: Response = None):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "add catalog rule")
    session_id = require_session_id()
    payload_dict = payload.dict()
    payload_dict["session_id"] = session_id
    result = Contexts.add_catalog_rule(session_logger, payload_dict)
    if response is not None:
        response.headers["transaction_id"] = transaction_id
    return result


@router.put("/update_catalog_rule/{catalog_ruleid}", status_code=200)
async def update_catalog_rule(catalog_ruleid: int, payload: CatalogRulePayload, response: Response = None):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "update catalog rule")
    session_id = require_session_id()
    payload_dict = payload.dict()
    payload_dict["session_id"] = session_id
    result = Contexts.update_catalog_rule(session_logger, catalog_ruleid, payload_dict)
    if response is not None:
        response.headers["transaction_id"] = transaction_id
    return result


@router.delete("/delete_catalog_rule/{catalog_ruleid}", status_code=200)
async def delete_catalog_rule(catalog_ruleid: int, response: Response = None):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "delete catalog rule")
    session_id = require_session_id()
    result = Contexts.delete_catalog_rule(session_logger, catalog_ruleid)
    if response is not None:
        response.headers["transaction_id"] = transaction_id
    return result


@router.get("/tables", status_code=200)
async def get_all_tables(domain: str = Query(None), catalog: str = Query(None), response: Response = None):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "get_all_tables")
    session_id = require_session_id()
    
    try:
        # Use async database operations for better performance
        from src.database_connections.app_database.service import AppDatabase, get_async_db_session
        
        async for db_session in get_async_db_session():
            result = await AppDatabase.get_tables_by_domain_async(
                domainname=domain,
                catalogname=catalog,
                session=db_session
            )
            
            if response is not None:
                response.headers["transaction_id"] = transaction_id
            return result
            
    except Exception as e:
        # Fallback to sync operation if async fails
        logger_capture.log_error(session_logger, f"Async operation failed, falling back to sync: {e}")
        result = Contexts.get_all_tables(session_logger, domain, catalog)
        if response is not None:
            response.headers["transaction_id"] = transaction_id
        return result


@router.get("/tables/{tableid}", status_code=200)
async def get_table_details(tableid: int, response: Response = None):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "get_table_details")
    session_id = require_session_id()
    result = Contexts.get_table_details(session_logger, tableid)
    if response is not None:
        response.headers["transaction_id"] = transaction_id
    if not result:
        raise HTTPException(status_code=404, detail="Table not found")
    return result


@router.put("/archive_column/{columnid}", status_code=200)
async def archive_column(tableid: int = Query(...), columnid: int = None, archive: bool = Query(...), response: Response = None):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "archive column")
    session_id = require_session_id()
    result = Contexts.archive_column(session_logger, tableid, columnid, archive, session_id=session_id)
    if response is not None:
        response.headers["transaction_id"] = transaction_id
    return result


@router.put("/archive_rule/{ruleid}", status_code=200)
async def archive_rule(tableid: int = Query(...), ruleid: int = None, archive: bool = Query(...), response: Response = None):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "archive rule")
    session_id = require_session_id()
    result = Contexts.archive_rule(session_logger, tableid, ruleid, archive, session_id=session_id)
    if response is not None:
        response.headers["transaction_id"] = transaction_id
    return result


@router.post("/generate_reasoning_traces", status_code=200)
async def generate_reasoning_traces(payload: ReasoningTracesPayload, response: Response = None):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "generate_reasoning_traces")
    payload_dict = payload.dict()
    result = Contexts.generate_reasoning_traces(session_logger, payload_dict)
    if response is not None:
        response.headers["transaction_id"] = transaction_id
    return result


@router.delete("/delete_table/{tableid}", status_code=200)
async def delete_table(tableid: int, response: Response = None):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "delete table")
    session_id = require_session_id()
    result = Contexts.delete_table(session_logger, tableid, session_id=session_id)
    if response is not None:
        response.headers["transaction_id"] = transaction_id
    return result


@router.post("/create_contexts", status_code=201)
async def create_contexts(
    payload: list[ContextObject],
    domain: str = Body(...),
    catalogs: list[str] = Body(...),
    response: Response = None,
):
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "create contexts")
    try:
        session_id = require_session_id()
        result = Contexts.create_contexts(session_logger, [p.dict() for p in payload], domain, catalogs, session_id=session_id)
        if response is not None:
            response.headers["transaction_id"] = transaction_id
        return result
    except ValueError as e:
        if response is not None:
            response.headers["transaction_id"] = transaction_id
        raise handle_value_error(session_logger, transaction_id, e)
    except Exception as e:
        if response is not None:
            response.headers["transaction_id"] = transaction_id
        raise handle_internal_exception(session_logger, transaction_id, e)
    finally:
        session_logger.save_logs()


@router.get("/catalog_rules", status_code=200)
async def get_catalog_rules(
    domain: str = Query(..., description="Domain name to filter catalog rules"),
    response: Response = None
):
    """
    Fetch all catalog rules for a specific domain, optionally filtered by catalog.
    
    Args:
        domain: Domain name (required)
        catalog: Catalog name (optional)
        active_only: Whether to return only active rules (default: True)
    
    Returns:
        JSON response containing catalog rules data
    """
    transaction_id = f"{uuid.uuid4()}_{datetime.now().timestamp()}"
    session_logger = start_new_session(transaction_id, "get catalog rules")
    session_id = require_session_id()
    
    try:
        result = Contexts.get_catalog_rules(session_logger, domain)
        if response is not None:
            response.headers["transaction_id"] = transaction_id
        return result
    except Exception as e:
        if response is not None:
            response.headers["transaction_id"] = transaction_id
        raise handle_internal_exception(session_logger, transaction_id, e)
    finally:
        session_logger.save_logs()
=============================================
======================================================================================
================================================================================================================================
####service###
import io
import pandas as pd
from src.database_connections.app_database.service import AppDatabase
from src.database_connections.app_database import models
from src.llm.services import LLM
from src.utils.log_wrapper import logger_capture
from src.utils.utils import Utils
import json
from src.models.model_parameters import Parameters

def convert_categorical_definitions_to_list(categorical_definitions_dict):
    """Convert dict from database to list of objects for API response"""
    if not categorical_definitions_dict:
        return []
    return [{k:v} for k, v in categorical_definitions_dict.items()]

class Contexts:
    @staticmethod
    def upload_contexts(session_logger, files, domain, catalogs, archive_value, session_id=None):
        # Validate catalogs
        catalogs_df = AppDatabase.get_allcatalogs(domain)
        valid_catalogs = dict(zip(catalogs_df["catalogname"], catalogs_df["catalogid"]))
        catalog_domain_map = dict(zip(catalogs_df["catalogid"], catalogs_df["domainid"]))
        invalid_catalogs = [name for name in catalogs if name not in valid_catalogs]
        if invalid_catalogs:
            raise ValueError(f"Invalid catalog(s): {', '.join(invalid_catalogs)}")
        catalog_ids = [valid_catalogs[name] for name in catalogs]
        tables_data, columns_data = [], []
        for file in files:
            if file and file.filename.endswith(".csv"):
                content = file.file.read()
                read_df = pd.read_csv(io.StringIO(content.decode("utf-8")))
                read_df.columns = read_df.columns.str.lower()
                if file.filename.lower().startswith("t_"):
                    tables_data.append(read_df)
                elif file.filename.lower().startswith("c_"):
                    columns_data.append(read_df)
        if not tables_data:
            raise ValueError("Missing Table files, please upload Table data with files starting with T_.csv")
        if not columns_data:
            raise ValueError("Missing Columns files, please upload Column data with files starting with C_.csv")
        tables_df = pd.concat(tables_data).drop_duplicates(subset=["database_name", "table_name"], keep="first")
        columns_df = pd.concat(columns_data)
        # Replace out-of-range float values (NaN, inf, -inf) with None for JSON compliance
        import numpy as np
        tables_df = tables_df.replace({np.nan: None, np.inf: None, -np.inf: None})
        columns_df = columns_df.replace({np.nan: None, np.inf: None, -np.inf: None})
        # --- Domain-level duplicate check: ensure no table is uploaded twice under the same domain ---
        domainid = catalog_domain_map[catalog_ids[0]] if catalog_ids else None
        existing_tables = set()
        if domainid is not None:
            from src.database_connections.app_database.models import TableMapping, Table
            session = AppDatabase.get_session()
            try:
                mappings = session.query(TableMapping).filter_by(domainid=domainid).all()
                tableids = [m.tableid for m in mappings]
                if tableids:
                    tables = session.query(Table).filter(Table.tableid.in_(tableids)).all()
                    existing_tables = set(t.tablename for t in tables)
            finally:
                session.close()
        duplicate_tables = []
        for _, table_row in tables_df.iterrows():
            db_name = table_row["database_name"]
            tble_name = table_row["table_name"]
            project_name = table_row.get("project_name", None)
            if project_name and str(project_name).strip().lower() not in ("nan", "none", "null", ""):
                tablename = f"{project_name}.{db_name}.{tble_name}"
            else:
                tablename = f"{db_name}.{tble_name}"
            if tablename in existing_tables:
                duplicate_tables.append(tablename)
        if duplicate_tables:
            raise ValueError(f"Duplicate table(s) for this domain: {', '.join(duplicate_tables)}. Table(s) already exist under this domain and cannot be uploaded again.")
        # --- End duplicate check ---
        # --- Domain+Catalog-level duplicate check: ensure no table is uploaded twice under the same domain+catalog ---
        duplicate_tables = []
        if domainid is not None:
            from src.database_connections.app_database.models import TableMapping, Table
            session = AppDatabase.get_session()
            try:
                # For each table in the upload, check if a mapping exists for same tablename, domain, and any of the catalogs
                for _, table_row in tables_df.iterrows():
                    db_name = table_row["database_name"]
                    tble_name = table_row["table_name"]
                    project_name = table_row.get("project_name", None)
                    if project_name and str(project_name).strip().lower() not in ("nan", "none", "null", ""):
                        tablename = f"{project_name}.{db_name}.{tble_name}"
                    else:
                        tablename = f"{db_name}.{tble_name}"
                    # Find all tables with this name in this domain
                    table_objs = session.query(Table).filter(Table.tablename == tablename).all()
                    for t in table_objs:
                        mappings = session.query(TableMapping).filter_by(tableid=t.tableid, domainid=domainid).all()
                        for m in mappings:
                            # If any catalog in upload overlaps with mapping, it's a duplicate
                            if set(m.catalogid or []) & set(catalog_ids):
                                duplicate_tables.append(tablename)
                                break
            finally:
                session.close()
        if duplicate_tables:
            raise ValueError(f"Duplicate table(s) for this domain/catalog: {', '.join(set(duplicate_tables))}. Table(s) already exist under this domain and catalog and cannot be uploaded again.")
        # --- End duplicate check ---
        # --- End duplicate check ---
        for _, table_row in tables_df.iterrows():
            # Compose tablename with project_name if present
            db_name = table_row["database_name"]
            tble_name = table_row["table_name"]
            project_name = table_row.get("project_name", None)
            if project_name and str(project_name).strip().lower() not in ("nan", "none", "null", ""):
                tablename = f"{project_name}.{db_name}.{tble_name}"
            else:
                tablename = f"{db_name}.{tble_name}"
            table_desc = table_row.get("table_description", "")
            usage_patterns = [item.strip() for item in str(table_row.get("usage patterns", "")).split(";") if item.strip()]
            archive = archive_value
            from datetime import datetime
            catalogid = catalog_ids if catalog_ids else None
            domainid = catalog_domain_map[catalog_ids[0]] if catalog_ids else None
            # --- Embedding generation and logging ---
            from src.utils.vectorstore import VectorStore
            
            embedding_input = f"{tablename} {table_desc} {'; '.join(usage_patterns)}"
            try:
                embedding = VectorStore.vegas_embedding(embedding_input)
                logger_capture.log_info(session_logger,f"[DEBUG] Generated embedding for table '{tablename}': {embedding if isinstance(embedding, list) and len(embedding) < 10 else '[truncated]'}")
            except Exception as e:
                logger_capture.log_error(session_logger,f"[DEBUG] Failed to generate embedding for table '{tablename}': {e}")
                embedding = None
            table_values = {
                "tablename": tablename,
                "table_description": table_desc,
                "usage_patterns": usage_patterns,
                "archive": archive,
                "date_created": datetime.utcnow(),
                "date_updated": datetime.utcnow(),
                "embedding": embedding,
            }
            tableid = AppDatabase.insert_table({**table_values, "domainid": domainid, "catalogid": catalogid})
            # Audit log for table creation
            logger_capture.log_info(session_logger,f"[DEBUG] Table '{tablename}' inserted with embedding: {embedding if isinstance(embedding, list) and len(embedding) < 10 else '[truncated]'}")
            AppDatabase.log_audit(
                entity_type="Table",
                entity_id=tableid,
                action="create",
                session_id=session_id,
                change_details={"new": {**table_values, "domainid": domainid, "catalogid": catalogid}},
                domainid=domainid,
                catalogids=catalogid
            )
            class TableObj:
                def __init__(self, tableid):
                    self.tableid = tableid
            table_obj = TableObj(tableid)
            table_columns_df = columns_df[(columns_df["database_name"] == db_name) & (columns_df["table_name"] == tble_name)]
            # Drop rows with blank or null column_name in columns_df
            if 'column_name' in columns_df.columns:
                columns_df = columns_df[columns_df['column_name'].notnull()]
                columns_df = columns_df[columns_df['column_name'].astype(str).str.strip().str.lower().isin(['', 'nan', 'none', 'null']) == False]
            for _, col_row in table_columns_df.iterrows():
                import math, json
                # Skip columns with missing or empty column_name
                col_name = col_row.get("column_name", None)
                if not col_name or str(col_name).strip().lower() in ("nan", "none", "null", ""):
                    # Optionally log or collect skipped columns here
                    continue
                
                # Debug: Print available columns for first row
                if col_name == list(table_columns_df['column_name'])[0]:  # First column only
                    logger_capture.log_info(session_logger, f"[DEBUG] Available columns in CSV: {list(col_row.keys())}")
                
                cat_values = col_row.get("possible values", "")
                # Safely check for NaN values to avoid ambiguous truth value error
                try:
                    cat_values_is_na = pd.isna(cat_values)
                except ValueError:
                    # Handle array-like objects
                    cat_values_is_na = hasattr(cat_values, '__iter__') and not isinstance(cat_values, str) and (len(cat_values) == 0 or (hasattr(cat_values, 'isna') and cat_values.isna().all()))
                
                if cat_values_is_na or str(cat_values).strip().lower() in ("nan", "none", "null", ""):
                    categorical_values = None
                else:
                    categorical_values = [v.strip() for v in str(cat_values).split(";") if v.strip()]
                
                # Debug: Try different column name variations
                cat_defs = None
                for col_variant in ["categorical definitions", "categorical_definitions", "Categorical Definitions", "Categorical_Definitions"]:
                    if col_variant in col_row:
                        cat_defs = col_row.get(col_variant, None)
                        logger_capture.log_info(session_logger, f"[DEBUG] Found categorical definitions column: '{col_variant}' for column '{col_name}', raw value: {repr(cat_defs)}")
                        break
                
                if cat_defs is None:
                    logger_capture.log_info(session_logger, f"[DEBUG] No categorical definitions column found for column '{col_name}'")
                
                # Safely check for NaN values to avoid ambiguous truth value error
                try:
                    cat_defs_is_na = pd.isna(cat_defs)
                except ValueError:
                    # Handle array-like objects
                    cat_defs_is_na = hasattr(cat_defs, '__iter__') and not isinstance(cat_defs, str) and (len(cat_defs) == 0 or (hasattr(cat_defs, 'isna') and cat_defs.isna().all()))
                
                if cat_defs_is_na or str(cat_defs).strip().lower() in ("nan", "none", "null", ""):
                    categorical_definitions = None
                    logger_capture.log_info(session_logger, f"[DEBUG] Column '{col_name}': categorical_definitions set to None (empty/null)")
                else:
                    try:
                        if isinstance(cat_defs, str):
                            # First try standard JSON parsing
                            try:
                                categorical_definitions = json.loads(cat_defs)
                                logger_capture.log_info(session_logger, f"[DEBUG] Column '{col_name}': Successfully parsed JSON categorical_definitions: {categorical_definitions}")
                            except json.JSONDecodeError:
                                # If standard JSON fails, try Python literal_eval for dict-style format with single quotes
                                import ast
                                categorical_definitions = ast.literal_eval(cat_defs)
                                logger_capture.log_info(session_logger, f"[DEBUG] Column '{col_name}': Successfully parsed Python dict categorical_definitions: {categorical_definitions}")
                        else:
                            categorical_definitions = cat_defs
                            logger_capture.log_info(session_logger, f"[DEBUG] Column '{col_name}': Using non-string categorical_definitions: {categorical_definitions}")
                    except Exception as e:
                        categorical_definitions = None
                        logger_capture.log_error(session_logger, f"[DEBUG] Column '{col_name}': Failed to parse categorical_definitions '{cat_defs}': {e}")
                column_data = {
                    "name": col_name,
                    "data_type": col_row.get("column_datatype", "STRING"),
                    "description": col_row.get("column_description", ""),
                    "column_alias": col_row.get("column_alias", None),
                    "categorical_values": categorical_values,
                    "categorical_definitions": categorical_definitions,
                    "archive": False
                }
                logger_capture.log_info(session_logger, f"[DEBUG] Column '{col_name}': Storing column_data with categorical_definitions: {categorical_definitions}")
                column = AppDatabase.add_table_column(
                    tableid=table_obj.tableid,
                    column_data=column_data,
                    session_id=session_id,
                    domainid=domainid,
                    catalogids=catalogid
                )
                # Note: add_table_column already logs audit internally
            rules_str = table_row.get("rules", "")
            if rules_str:
                for rule in [r.strip() for r in str(rules_str).split(";") if r.strip()]:
                    rule_id = AppDatabase.add_table_rule(
                        tableid=table_obj.tableid,
                        rule_data={
                            "rule_definition": rule,
                        },
                        session_id=session_id,
                        domainid=domainid,
                        catalogids=catalogid
                    )
                    # Note: add_table_rule already logs audit internally, so no need for duplicate logging here
        return {"status": "success", "message": "Contexts uploaded successfully"}

    @staticmethod
    def add_column(session_logger, column_data):
        # Handle both single dict and list of dicts
        if isinstance(column_data, dict):
            column_data = [column_data]
        
        results = {"created": [], "failed": []}
        
        for data in column_data:
            try:
                session_id = data.get("session_id")
                tableid = data["tableid"]
                # Get domain/catalog information from the table
                domainid, catalogids = AppDatabase.get_table_domain_catalog(tableid)
                # Remove tableid and session_id from column_data to avoid duplicate/invalid keyword arguments
                column_data_clean = {k: v for k, v in data.items() if k not in ("tableid", "session_id")}
                result = AppDatabase.add_table_column(tableid, column_data_clean, session_id=session_id, domainid=domainid, catalogids=catalogids)
                results["created"].append({"columnid": result.columnid, "name": data.get("name"), "tableid": tableid})
            except Exception as e:
                results["failed"].append({"tableid": data.get("tableid"), "name": data.get("name"), "error": str(e)})
        
        return results

    @staticmethod
    def update_column(session_logger, columnid: int, update_data: dict):
        session_id = update_data.get("session_id")
        # Get table ID from column, then get domain/catalog info
        column = AppDatabase.get_table_column(columnid)
        if column:
            domainid, catalogids = AppDatabase.get_table_domain_catalog(column.tableid)
            return AppDatabase.update_table_column(columnid, update_data, session_id=session_id, domainid=domainid, catalogids=catalogids)
        return False

    @staticmethod
    def delete_column(session_logger, tableid: int, columnid: int, session_id=None):
        old_column = AppDatabase.get_table_column(columnid)
        # Get domain/catalog information from the table
        domainid, catalogids = AppDatabase.get_table_domain_catalog(tableid)
        # Serialize old_column for audit log
        old_column_dict = {c.name: getattr(old_column, c.name) for c in old_column.__table__.columns} if old_column else None
        result = AppDatabase.delete_table_column(columnid, session_id=session_id, domainid=domainid, catalogids=catalogids)
        # Note: delete_table_column already logs audit internally, so remove this duplicate
        return result

    @staticmethod
    def add_rule(session_logger, rule_data):
        # Handle both single dict and list of dicts
        if isinstance(rule_data, dict):
            rule_data = [rule_data]
        
        results = {"created": [], "failed": []}
        
        for data in rule_data:
            try:
                session_id = data.get("session_id")
                tableid = data["tableid"]
                # Get domain/catalog information from the table
                domainid, catalogids = AppDatabase.get_table_domain_catalog(tableid)
                # Remove tableid and session_id from rule_data to avoid duplicate/invalid keyword arguments
                rule_data_clean = {k: v for k, v in data.items() if k not in ("tableid", "session_id")}
                rule = AppDatabase.add_table_rule(tableid, rule_data_clean, session_id=session_id, domainid=domainid, catalogids=catalogids)
                # Note: add_table_rule already logs audit internally, so remove this duplicate
                results["created"].append({"tableruleid": rule, "rule_definition": data.get("rule_definition"), "tableid": tableid})
            except Exception as e:
                results["failed"].append({"tableid": data.get("tableid"), "rule_definition": data.get("rule_definition"), "error": str(e)})
        
        return results

    @staticmethod
    def update_rule(session_logger, ruleid: int, update_data: dict):
        session_id = update_data.get("session_id")
        old_rule = AppDatabase.get_table_rule(ruleid)
        result = AppDatabase.update_table_rule(ruleid, update_data)
        AppDatabase.log_audit(
            entity_type="TableRule",
            entity_id=ruleid,
            action="update",
            session_id=session_id,
            change_details={"old": old_rule, "new": update_data},
            domainid=update_data.get("domainid"),
            catalogids=update_data.get("catalogids") or update_data.get("catalog_ids")
        )
        return result

    @staticmethod
    def delete_rule(session_logger, tableid: int, ruleid: int, session_id=None):
        old_rule = AppDatabase.get_table_rule(ruleid)
        result = AppDatabase.delete_table_rule(ruleid)
        AppDatabase.log_audit(
            entity_type="TableRule",
            entity_id=ruleid,
            action="delete",
            session_id=session_id,
            change_details={"old": old_rule},
            domainid=getattr(old_rule, 'domainid', None),
            catalogids=getattr(old_rule, 'catalogids', None),
        )
        return result

    @staticmethod
    def update_table_metadata(session_logger, metadata: dict):
        tableid = metadata["tableid"]
        update_data = {}
        if metadata.get("table_description") is not None:
            update_data["table_description"] = metadata["table_description"]
        if metadata.get("usage_patterns") is not None:
            update_data["usage_patterns"] = metadata["usage_patterns"]
        # Resolve catalog_names to catalog_ids
        catalog_names = metadata.get("catalog_names")
        if catalog_names is not None:
            # Find domainid for the table from TableMapping
            from src.database_connections.app_database.models import TableMapping, Catalog
            session = AppDatabase.get_session()
            try:
                # Get the table mapping to find domainid
                mapping = session.query(TableMapping).filter_by(tableid=tableid).first()
                if not mapping:
                    raise ValueError("Table mapping not found")
                domainid = mapping.domainid
                # Get catalogids for the given names in this domain
                catalogids = []
                for name in catalog_names:
                    catalog = session.query(Catalog).filter_by(catalogname=name, domainid=domainid).first()
                    if not catalog:
                        raise ValueError(f"Catalog '{name}' not found in domain")
                    catalogids.append(catalog.catalogid)
                # Add to update_data
                update_data["catalog_ids"] = catalogids
            finally:
                session.close()
        return AppDatabase.update_table(tableid, update_data)

    @staticmethod
    def generate_business_rule(session_logger, historyid, suggested_sql, feedback_dump={}):
        
        chat_artifacts = Utils.get_cache_metadata(historyid)
        question = chat_artifacts["user_query"]
        generated_query = chat_artifacts["output"]["generated_query"]
        db_type = chat_artifacts["db_type"]
        metadata = chat_artifacts["output"].get("metadata", {})
        domain = chat_artifacts["domain"]
        catalog = chat_artifacts["catalog"]  # Remove the list wrapper since we need the catalog name
        
        config = AppDatabase.get_domain_catalog_config(domain, catalog)
        use_thinking = config.get("use_thinking", False)
        thinking_budget = config.get("thinking_budget", 0)
        Parameters.set_thoughts(use_thinking)
        Parameters.set_thinkingbudget(thinking_budget)

        # Get existing rules as a list of dictionaries (not JSON string)
        # Filter by both domain and catalog for more relevant rules
        existing_rules_df = AppDatabase.get_active_catalogrules(domain, catalog)
        existing_rules = existing_rules_df.to_dict("records")
        
        # Convert timestamp objects to strings for JSON serialization
        from src.utils.utils import sanitize_dataframe_for_json
        existing_rules = sanitize_dataframe_for_json(existing_rules)
        
        if not suggested_sql:
            suggested_sql = "User did not provide any expected SQL Query. Please refer to provided feedback for generating the business rules."
        
        generated_business_rule = LLM.generate_business_rule(
            session_logger,
            historyid,
            question,
            generated_query,
            db_type,
            suggested_sql,
            metadata,
            feedback_dump,
            existing_rules,
        )  

        return generated_business_rule

    @staticmethod
    def add_catalog_rule(session_logger, payload: dict):
        """
        Add new catalog rules with the new schema.
        Expected payload format:
        {
            "domain": "domain_name",
            "catalogs": ["catalog1", "catalog2"],
            "catalog_rules": [
                {
                    "description": "Rule description",
                    "rule_detail": {"key": "value"},  # JSONB data
                    "tables": ["table1", "table2"]  # Optional
                }
            ]
        }
        """
        try:
            domain_name = payload.get("domain")
            catalog_names = payload.get("catalogs", [])
            catalog_rules = payload.get("catalog_rules", [])
            
            if not domain_name:
                raise ValueError("Domain name is required")
            if not catalog_rules:
                raise ValueError("At least one catalog rule is required")
            
            # Get domain ID
            domain_info = AppDatabase.get_domain_by_name(domain_name)
            if not domain_info:
                raise ValueError(f"Domain '{domain_name}' not found")
            domain_id = domain_info.get("domainid")
            
            # Get catalog IDs if provided
            catalog_ids = []
            if catalog_names:
                for catalog_name in catalog_names:
                    catalog_info = AppDatabase.get_catalog_by_name(catalog_name, domain_id)
                    if catalog_info:
                        catalog_ids.append(catalog_info.get("catalogid"))
                    else:
                        logger_capture.log_error(session_logger,f"Catalog '{catalog_name}' not found in domain '{domain_name}'")
            
            created_ids = []
            for rule in catalog_rules:
                if not rule.get("description"):
                    raise ValueError("Rule description is required")
                
                # Handle two formats: 
                # 1. rule_detail as a separate field
                # 2. all rule details at the root level (needs to be moved to rule_detail)
                rule_detail = rule.get("rule_detail")
                
                if rule_detail is None:
                    # Format 2: Extract rule details from root level
                    rule_detail = {}
                    detail_fields = ["section", "tables", "joins", "select", "custom_dimensions", "group_by", "filters"]
                    for field in detail_fields:
                        if field in rule:
                            rule_detail[field] = rule[field]
                    
                    # If no detail fields found, use empty dict
                    if not rule_detail:
                        rule_detail = {}
                    
                    logger_capture.log_info(session_logger, f"Extracted rule_detail from root level: {rule_detail}")
                
                # Validate rule_detail is a dict
                if not isinstance(rule_detail, dict):
                    logger_capture.log_warning(session_logger, f"rule_detail is not a dict, converting: {type(rule_detail)} -> {rule_detail}")
                    if isinstance(rule_detail, str):
                        try:
                            import json
                            rule_detail = json.loads(rule_detail)
                        except json.JSONDecodeError:
                            logger_capture.log_warning(session_logger, "Failed to parse rule_detail as JSON, using empty dict")
                            rule_detail = {}
                    else:
                        rule_detail = {}
                
                logger_capture.log_info(session_logger, f"Processing rule with rule_detail: {rule_detail} (type: {type(rule_detail)})")
                
                # Extract tables for the tables field (list of table names)
                tables_list = rule.get("tables", [])
                
                # If tables is empty, try to extract from rule_detail.tables
                if not tables_list and "tables" in rule_detail:
                    tables_from_detail = rule_detail["tables"]
                    if isinstance(tables_from_detail, list):
                        # Extract table names from objects or use strings directly
                        tables_list = []
                        for table in tables_from_detail:
                            if isinstance(table, dict) and "name" in table:
                                tables_list.append(table["name"])
                            elif isinstance(table, str):
                                tables_list.append(table)
                
                logger_capture.log_info(session_logger, f"Extracted tables list: {tables_list}")
                
                # Generate embedding for description
                from src.utils.vectorstore import VectorStore
                desc_embedding = VectorStore.vegas_embedding(rule["description"])
                
                # Prepare rule data for insertion
                from datetime import datetime
                rule_data = {
                    "description": rule["description"],
                    "rule_detail": rule_detail,  # Use the validated rule_detail
                    "desc_embedding": desc_embedding,
                    "domainid": domain_id,
                    "catalogid": catalog_ids if catalog_ids else None,
                    "tables": tables_list,
                    "created_at": datetime.utcnow(),
                    "updated_at": datetime.utcnow()
                }
                
                logger_capture.log_info(session_logger, f"Inserting catalog rule with data: {rule_data}")
                rule_id = AppDatabase.insert_catalogrule(rule_data)
                logger_capture.log_info(session_logger, f"Successfully created catalog rule with ID: {rule_id}")
                created_ids.append(rule_id)
                
            return {"status": "success", "created_catalog_rule_ids": created_ids}
            
        except Exception as e:
            logger_capture.log_error(session_logger,f"Error adding catalog rule: {str(e)}")
            raise

    @staticmethod
    def update_catalog_rule(session_logger, catalog_ruleid: int, payload: dict):
        """
        Update an existing catalog rule.
        Expected payload format:
        {
            "domain": "domain_name",
            "catalogs": ["catalog1", "catalog2"],
            "catalog_rules": [
                {
                    "description": "rule description",
                    "section": "...",
                    "tables": [{"name": "table1", "alias": "t1"}, ...],
                    "select": [...],
                    "filters": [...],
                    "joins": [...],
                    // ... other fields
                }
            ]
        }
        """
        try:
            catalog_rules = payload.get("catalog_rules", [])
            
            if not catalog_rules or len(catalog_rules) != 1:
                raise ValueError("Update requires exactly one catalog rule")
            
            rule_object = catalog_rules[0]
            update_data = {}
            
            # Extract description separately
            if "description" in rule_object:
                update_data["description"] = rule_object["description"]
                
                # Regenerate embedding for new description
                from src.utils.vectorstore import VectorStore
                desc_embedding = VectorStore.vegas_embedding(rule_object["description"])
                update_data["desc_embedding"] = desc_embedding
            
            # Extract tables for the separate tables field (list of table names)
            if "tables" in rule_object:
                tables = rule_object["tables"]
                table_names = []
                if isinstance(tables, list):
                    for table in tables:
                        if isinstance(table, dict):
                            table_names.append(table.get("name", str(table)))
                        else:
                            table_names.append(str(table))
                update_data["tables"] = table_names
            
            # Store the entire rule object in rule_detail (pydantic ensures it's valid)
            update_data["rule_detail"] = rule_object
            
            # Handle domain update if provided
            domain_name = payload.get("domain")
            if domain_name:
                domain_info = AppDatabase.get_domain_by_name(domain_name)
                if not domain_info:
                    raise ValueError(f"Domain '{domain_name}' not found")
                update_data["domainid"] = domain_info.get("domainid")
            
            # Handle catalog updates if provided
            catalog_names = payload.get("catalogs")
            if catalog_names:
                # Need domain ID for catalog lookup
                domain_id = update_data.get("domainid")
                if not domain_id:
                    # Get current domain ID if not updating domain
                    current_rule = AppDatabase.get_catalogrule_by_id(catalog_ruleid)
                    if current_rule:
                        domain_id = current_rule.get("domainid")
                    else:
                        raise ValueError(f"Catalog rule with ID {catalog_ruleid} not found")
                
                catalog_ids = []
                for catalog_name in catalog_names:
                    catalog_info = AppDatabase.get_catalog_by_name(catalog_name, domain_id)
                    if catalog_info:
                        catalog_ids.append(catalog_info.get("catalogid"))
                    else:
                        logger_capture.log_error(session_logger, f"Catalog '{catalog_name}' not found in domain '{domain_name}'")
                        raise ValueError(f"Invalid catalog(s): {catalog_name}")

                update_data["catalogid"] = catalog_ids if catalog_ids else None
            
            # Perform the update
            update_result = AppDatabase.update_catalogrule(catalog_ruleid, update_data)
            return {"status": "success", "updated": update_result}
            
        except Exception as e:
            logger_capture.log_error(session_logger, f"Error updating catalog rule: {str(e)}")
            raise

    @staticmethod
    def delete_catalog_rule(session_logger, catalog_ruleid: int):
        delete_result = AppDatabase.delete_catalogrule(catalog_ruleid)
        return {"status": "success", "deleted": delete_result}

    @staticmethod
    def get_all_tables(session_logger, domain=None, catalog=None):
        # Fetch all tables, optionally filtered by domain/catalog
        tables_data = AppDatabase.get_all_tables(domain, catalog)
        
        return tables_data

    @staticmethod
    def get_table_details(session_logger, tableid):
        # Fetch table details by tableid
        table_details = AppDatabase.get_table_details(tableid)
        # Convert categorical_definitions format for API response
        if table_details and "columns" in table_details:
           
            for column in table_details["columns"]:
                if "categorical_definitions" in column:
                    column["categorical_definitions"] = convert_categorical_definitions_to_list(
                        column["categorical_definitions"]
                    )
        return table_details

    @staticmethod
    def add_table(session_logger, table_data: dict, session_id=None):
        tableid = AppDatabase.insert_table(table_data)
        AppDatabase.log_audit(
            entity_type="Table",
            entity_id=tableid,
            action="create",
            session_id=session_id,
            change_details={"new": table_data},
            domainid=table_data.get("domainid"),
            catalogids=table_data.get("catalogids") or table_data.get("catalog_ids")
        )
        # Return a simple object with tableid for compatibility
        class TableResult:
            def __init__(self, tableid):
                self.tableid = tableid
        return TableResult(tableid)

    @staticmethod
    def update_table(session_logger, tableid: int, update_data: dict, session_id=None):
        old_table = AppDatabase.get_table_details(tableid)
        result = AppDatabase.update_table(tableid, update_data)
        AppDatabase.log_audit(
            entity_type="Table",
            entity_id=tableid,
            action="update",
            session_id=session_id,
            change_details={"old": old_table, "new": update_data},
            domainid=update_data.get("domainid"),
            catalogids=update_data.get("catalogids") or update_data.get("catalog_ids")
        )
        return result

    @staticmethod
    def delete_table(session_logger, tableid: int, session_id=None):
        old_table = AppDatabase.get_table_details(tableid)
        result = AppDatabase.delete_table(tableid)
        AppDatabase.log_audit(
            entity_type="Table",
            entity_id=tableid,
            action="delete",
            session_id=session_id,
            change_details={"old": old_table},
            domainid=old_table.get("domainid") if isinstance(old_table, dict) else None,
            catalogids=old_table.get("catalogids") if isinstance(old_table, dict) else None
        )
        return result

    @staticmethod
    def update_rule(session_logger, ruleid: int, update_data: dict, session_id=None):
        old_rule = AppDatabase.get_table_rule(ruleid)
        # Get domain/catalog information from the table
        if old_rule:
            domainid, catalogids = AppDatabase.get_table_domain_catalog(old_rule.tableid)
        else:
            domainid, catalogids = None, None
        result = AppDatabase.update_table_rule(ruleid, update_data, session_id=session_id, domainid=domainid, catalogids=catalogids)
        # Note: update_table_rule already logs audit internally
        return result

    @staticmethod
    def delete_rule(session_logger, tableid: int, ruleid: int, session_id=None):
        old_rule = AppDatabase.get_table_rule(ruleid)
        result = AppDatabase.delete_table_rule(ruleid)
        AppDatabase.log_audit(
            entity_type="TableRule",
            entity_id=ruleid,
            action="delete",
            session_id=session_id,
            change_details={"old": old_rule},
            domainid=getattr(old_rule, 'domainid', None),
            catalogids=getattr(old_rule, 'catalogids', None),
        )
        return result

    @staticmethod
    def archive_column(session_logger, tableid: int, columnid: int, archive: bool, session_id=None):
        old_column = AppDatabase.get_table_column(columnid)
        # Serialize old_column for audit log
        old_column_dict = {c.name: getattr(old_column, c.name) for c in old_column.__table__.columns} if old_column else None
        result = AppDatabase.update_table_column(columnid, {"archive": archive}, session_id=session_id)
        AppDatabase.log_audit(
            entity_type="TableColumn",
            entity_id=columnid,
            action="archive" if archive else "unarchive",
            session_id=session_id,
            change_details={"old": old_column_dict, "archive": archive},
            domainid=getattr(old_column, 'domainid', None),
            catalogids=getattr(old_column, 'catalogids', None),
        )
        return {"status": "success", "archived": archive}

    @staticmethod
    def archive_rule(session_logger, tableid: int, ruleid: int, archive: bool, session_id=None):
        old_rule = AppDatabase.get_table_rule(ruleid)
        result = AppDatabase.update_table_rule(ruleid, {"archive": archive})
        AppDatabase.log_audit(
            entity_type="TableRule",
            entity_id=ruleid,
            action="archive" if archive else "unarchive",
            session_id=session_id,
            change_details={"old": old_rule, "archive": archive},
            domainid=getattr(old_rule, 'domainid', None),
            catalogids=getattr(old_rule, 'catalogids', None),
        )
        return {"status": "success", "archived": archive}

    @staticmethod
    def update_columns(session_logger, update_payloads: list):
        results = {"updated": [], "failed": []}
        for payload in update_payloads:
            columnid = payload.get("columnid")
            session_id = payload.get("session_id")
            old_column = AppDatabase.get_table_column(columnid)
            if not old_column:
                results["failed"].append({"id": columnid, "status": "not found"})
                continue
            try:
                AppDatabase.update_table_column(columnid, payload, session_id=session_id)
                results["updated"].append({"id": columnid, "status": "success"})
            except Exception as e:
                results["failed"].append({"id": columnid, "status": str(e)})
        return results

    @staticmethod
    def update_rules(session_logger, update_payloads: list):
        results = {"updated": [], "failed": []}
        for payload in update_payloads:
            ruleid = payload.get("ruleid")
            session_id = payload.get("session_id")
            old_rule = AppDatabase.get_table_rule(ruleid)
            if not old_rule:
                results["failed"].append({"id": ruleid, "status": "not found"})
                continue
            try:
                AppDatabase.update_table_rule(ruleid, payload)
                AppDatabase.log_audit(
                    entity_type="TableRule",
                    entity_id=ruleid,
                    action="update",
                    session_id=session_id,
                    change_details={"old": old_rule, "new": payload}
                )
                results["updated"].append({"id": ruleid, "status": "success"})
            except Exception as e:
                results["failed"].append({"id": ruleid, "status": str(e)})
        return results

    @staticmethod
    def generate_reasoning_traces(session_logger, payload: dict):
        domain = payload.get("domain")
        catalog = payload.get("catalog")
        question = payload.get("question")
        sql_query = payload.get("sql_query")

        config = AppDatabase.get_domain_catalog_config(domain, catalog)
        use_thinking = config.get("use_thinking", False)
        thinking_budget = config.get("thinking_budget", 0)
        Parameters.set_thoughts(use_thinking)
        Parameters.set_thinkingbudget(thinking_budget)
        # 1. Extract table names from SQL
        extract_cols = LLM.extract_from_query(session_logger, sql_query)
        table_names = [table.lower() for table in extract_cols.keys()]

        # 2. Fetch metadata for each table
        metadata = {}
        for table_name in table_names:
            
            table_meta = AppDatabase.get_table_details_by_name(table_name, domain=domain, catalog=catalog)
            if table_meta:
                metadata[table_name] = table_meta

        # 3. Call LLM.reasoning_traces with all metadata
        reasoning_traces = LLM.reasoning_traces(session_logger, question, sql_query, metadata)
        return {"traces": reasoning_traces}

    @staticmethod
    def create_contexts(session_logger, contexts: list, domain: str, catalogs: list, session_id=None):
        """
        Create tables, columns, and rules from a list of context objects (JSON input).
        domain: domain name (str)
        catalogs: list of catalog names (str)
        """
        # Resolve domain name to domainid
        catalogs_df = AppDatabase.get_allcatalogs(domain)
        valid_catalogs = dict(zip(catalogs_df["catalogname"], catalogs_df["catalogid"]))
        catalog_domain_map = dict(zip(catalogs_df["catalogid"], catalogs_df["domainid"]))
        invalid_catalogs = [name for name in catalogs if name not in valid_catalogs]
        if invalid_catalogs:
            raise ValueError(f"Invalid catalog(s): {', '.join(invalid_catalogs)}")
        catalog_ids = [valid_catalogs[name] for name in catalogs]
        domainid = catalog_domain_map[catalog_ids[0]] if catalog_ids else None
        results = {"created": [], "failed": []}
        for ctx in contexts:
            try:
                table_meta = ctx.get("table", {})
                columns = ctx.get("columns", [])
                rules = ctx.get("rules", [])
                # --- Embedding generation and logging ---
                from src.utils.vectorstore import VectorStore
                
                embedding_input = f"{table_meta['tablename']} {table_meta.get('table_description', '')} {'; '.join(table_meta.get('usage_patterns', []))}"
                try:
                    embedding = VectorStore.vegas_embedding(embedding_input)
                    logger_capture.log_info(session_logger,f"[DEBUG] Generated embedding for table '{table_meta['tablename']}': {embedding if isinstance(embedding, list) and len(embedding) < 10 else '[truncated]'}")
                except Exception as e:
                    logger_capture.log_error(session_logger,f"[DEBUG] Failed to generate embedding for table '{table_meta['tablename']}': {e}")
                    embedding = None
                table_values = {
                    "tablename": table_meta["tablename"],
                    "table_description": table_meta.get("table_description", ""),
                    "usage_patterns": table_meta.get("usage_patterns", []),
                    "archive": table_meta.get("archive", False),
                    "date_created": table_meta.get("date_created"),
                    "date_updated": table_meta.get("date_updated"),
                    "domainid": domainid,
                    "catalogid": catalog_ids,
                    "embedding": embedding,
                }
                tableid = AppDatabase.insert_table(table_values)
                logger_capture.log_info(session_logger,f"[DEBUG] Table '{table_meta['tablename']}' inserted with embedding: {embedding if isinstance(embedding, list) and len(embedding) < 10 else '[truncated]'}")
                AppDatabase.log_audit(
                    entity_type="Table",
                    entity_id=tableid,
                    action="create",
                    session_id=session_id,
                    change_details={"new": table_values},
                    domainid=domainid,
                    catalogids=catalog_ids
                )
                # Columns
                for col in columns:
                    col_data = {
                        "name": col["name"],
                        "data_type": col.get("data_type", "STRING"),
                        "description": col.get("description", ""),
                        "column_alias": col.get("column_alias"),
                        "categorical_values": col.get("categorical_values"),
                        "categorical_definitions": col.get("categorical_definitions"),
                        "archive": col.get("archive", False)
                    }
                    colid = AppDatabase.add_table_column(tableid, col_data, session_id=session_id, domainid=domainid, catalogids=catalog_ids)
                    # Note: add_table_column already logs audit internally
                # Rules
                for rule in rules:
                    rule_data = {"rule_definition": rule.get("rule_definition")}
                    ruleid = AppDatabase.add_table_rule(tableid, rule_data, session_id=session_id, domainid=domainid, catalogids=catalog_ids)
                    # Note: add_table_rule already logs audit internally
                results["created"].append({"tableid": tableid, "tablename": table_meta["tablename"]})
            except Exception as e:
                results["failed"].append({"table": ctx.get("table", {}).get("tablename"), "error": str(e)})
        return results

    @staticmethod
    def get_catalog_rules(session_logger, domain: str):
        """
        Fetch catalog rules for a domain.
        
        Args:
            domain: Domain name to filter by
        
        Returns:
            List of catalog rules with structure:
            [
                {
                    rule_id: int,
                    domain: string,
                    catalogs: string[],
                    rule: {
                        description: string,
                        tables: string[],
                        rule_detail: {}
                    },
                    created_at: string,
                    updated_at: string
                }
            ]
        """
        try:
            rules_df = AppDatabase.get_catalogrules(domain)

            # Convert DataFrame to dict for JSON serialization
            rules_data = rules_df.to_dict("records")
            
            # Sanitize timestamps for JSON serialization
            from src.utils.utils import sanitize_dataframe_for_json
            rules_data = sanitize_dataframe_for_json(rules_data)
            
            # Transform to the new structure
            catalog_rules = []
            for rule in rules_data:
                # Extract rule details
                rule_detail = rule.get("rule_detail") or {}
                description = rule.get("description", "")
                tables = rule.get("tables") or []
                catalogs = rule.get("catalogs") or []
                
                # Ensure tables is a list of strings
                if not isinstance(tables, list):
                    tables = []
                
                # Ensure catalogs is a list of strings
                if not isinstance(catalogs, list):
                    catalogs = []
                
                catalog_rule = {
                    "catalog_ruleid": rule.get("rule_id"),
                    "domain": domain,
                    "catalogs": catalogs,
                    "rule": {
                        "description": description,
                        "tables": tables,
                        "rule_detail": rule_detail
                    },
                    "created_at": rule.get("created_at", ""),
                    "updated_at": rule.get("updated_at", "")
                }
                
                catalog_rules.append(catalog_rule)
            
            return catalog_rules
            
        except Exception as e:
            logger_capture.log_error(session_logger,f"Error fetching catalog rules: {str(e)}")
            raise
========================================
============================================================================================
#####config controller.py###
"""
Configuration controller module for system configuration management.
FastAPI router for configuration endpoints.
"""

from fastapi import APIRouter, HTTPException, Query, Path, Body
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field

from src.app.routers.configurations.service import ConfigurationService
from src.database_connections.app_database.service import AppDatabase
from src.utils.exceptions import ValidationException
from src.utils.utils import require_session_id


# Pydantic models for request/response validation
class ConfigurationUpdate(BaseModel):
    """Model for configuration update requests."""
    use_cache: Optional[bool] = Field(None, description="Enable/disable cache usage")
    use_contexts: Optional[bool] = Field(None, description="Enable/disable context usage")
    use_history: Optional[bool] = Field(None, description="Enable/disable history usage")
    use_subqueries: Optional[bool] = Field(None, description="Enable/disable subqueries usage")

    class Config:
        schema_extra = {
            "example": {
                "use_cache": True,
                "use_contexts": True,
                "use_history": False,
                "use_subqueries": True
            }
        }


class ConfigurationResponse(BaseModel):
    """Model for configuration response."""
    catalogid: Optional[int] = None
    catalogname: Optional[str] = None
    domainid: int
    domainname: str
    use_cache: bool
    use_contexts: bool
    use_history: bool
    use_subqueries: bool
    date_updated: Optional[str] = None
    updated_fields: Optional[list] = None


class DomainCatalogsResponse(BaseModel):
    """Model for domain catalogs list response."""
    domain: Dict[str, Any]
    catalogs: list


class UserOnboardRequest(BaseModel):
    """Model for user onboarding requests."""
    vzid: str = Field(..., description="Verizon ID of the user")
    role: str = Field(..., description="User role (admin, user, viewer)")
    domain_ids: Optional[List[int]] = Field(None, description="List of domain IDs user has access to (null means all domains)")

    class Config:
        schema_extra = {
            "example": {
                "vzid": "john.doe",
                "role": "user",
                "domain_ids": [1, 2, 3]
            }
        }


class UserUpdateRequest(BaseModel):
    """Model for user access update requests."""
    role: Optional[str] = Field(None, description="New role for the user")
    domain_ids: Optional[List[int]] = Field(None, description="New list of domain IDs (null means all domains)")

    class Config:
        schema_extra = {
            "example": {
                "role": "admin",
                "domain_ids": [1, 2]
            }
        }


class UserAccessResponse(BaseModel):
    """Model for user access response."""
    id: int
    vzid: str
    role: str
    domain_ids: Optional[List[int]]
    domains: List[Dict[str, Any]]
    access_level: str


# Create router
router = APIRouter()


def validate_admin_access() -> str:
    """
    Validate that the user has admin access for configuration management.
    
    Returns:
        Username if valid admin
        
    Raises:
        HTTPException: If invalid session or insufficient permissions
    """
    session_id = require_session_id()
    
    # Validate session
    platform = AppDatabase.valid_sessionid(session_id)
    if not platform:
        raise HTTPException(status_code=401, detail="Invalid or expired session")
    
    # Get username and check admin access
    username = AppDatabase.get_username_from_session(session_id)
    if not username:
        raise HTTPException(status_code=401, detail="Unable to determine user")
    
    # Check if user has admin role (using vzid which is typically the username)
    access_level = AppDatabase.get_access_level(username)
    if access_level.lower() != "admin":
        raise HTTPException(status_code=403, detail="Admin access required for configuration management")
    
    return username


@router.get("/catalog/{domain}/{catalog}", response_model=ConfigurationResponse)
async def get_catalog_configuration(
    domain: str = Path(..., description="Domain name"),
    catalog: str = Path(..., description="Catalog name")
):
    """
    Get configuration for a specific catalog.
    
    **Admin only endpoint**
    
    Args:
        domain: Name of the domain
        catalog: Name of the catalog
        
    Returns:
        Catalog configuration details
    """
    try:
        validate_admin_access()
        config = ConfigurationService.get_catalog_config(domain, catalog)
        return ConfigurationResponse(**config)
    except ValidationException as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.put("/catalog/{domain}/{catalog}", response_model=ConfigurationResponse)
async def update_catalog_configuration(
    domain: str = Path(..., description="Domain name"),
    catalog: str = Path(..., description="Catalog name"),
    config_updates: ConfigurationUpdate = Body(..., description="Configuration updates")
):
    """
    Update configuration for a specific catalog.
    
    **Admin only endpoint**
    
    Args:
        domain: Name of the domain
        catalog: Name of the catalog
        config_updates: Configuration updates to apply
        
    Returns:
        Updated catalog configuration
    """
    try:
        validate_admin_access()
        session_id = require_session_id()
        
        # Convert to dict and filter out None values
        updates = {k: v for k, v in config_updates.dict().items() if v is not None}
        
        if not updates:
            raise HTTPException(status_code=400, detail="No configuration updates provided")
        
        config = ConfigurationService.update_catalog_config(
            domain, catalog, updates, session_id
        )
        return ConfigurationResponse(**config)
    except ValidationException as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.get("/domain/{domain}", response_model=ConfigurationResponse)
async def get_domain_configuration(
    domain: str = Path(..., description="Domain name")
):
    """
    Get configuration for a specific domain.
    
    **Admin only endpoint**
    
    Args:
        domain: Name of the domain
        
    Returns:
        Domain configuration details
    """
    try:
        validate_admin_access()
        config = ConfigurationService.get_domain_config(domain)
        return ConfigurationResponse(**config)
    except ValidationException as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.put("/domain/{domain}", response_model=ConfigurationResponse)
async def update_domain_configuration(
    domain: str = Path(..., description="Domain name"),
    config_updates: ConfigurationUpdate = Body(..., description="Configuration updates")
):
    """
    Update configuration for a specific domain.
    
    **Admin only endpoint**
    
    Args:
        domain: Name of the domain
        config_updates: Configuration updates to apply
        
    Returns:
        Updated domain configuration
    """
    try:
        validate_admin_access()
        session_id = require_session_id()
        
        # Convert to dict and filter out None values
        updates = {k: v for k, v in config_updates.dict().items() if v is not None}
        
        if not updates:
            raise HTTPException(status_code=400, detail="No configuration updates provided")
        
        config = ConfigurationService.update_domain_config(domain, updates, session_id)
        return ConfigurationResponse(**config)
    except ValidationException as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.get("/domain/{domain}/catalogs", response_model=DomainCatalogsResponse)
async def list_domain_catalogs(
    domain: str = Path(..., description="Domain name")
):
    """
    List all catalogs in a domain with their configurations.
    
    **Admin only endpoint**
    
    Args:
        domain: Name of the domain
        
    Returns:
        Domain info and list of catalogs with configurations
    """
    try:
        validate_admin_access()
        result = ConfigurationService.list_domain_catalogs(domain)
        return DomainCatalogsResponse(**result)
    except ValidationException as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.get("/health")
async def configuration_health_check():
    """
    Health check endpoint for configuration service.
    
    Returns:
        Health status
    """
    return {"status": "ok", "service": "configuration-management"}


@router.post("/users/onboard", response_model=UserAccessResponse)
async def onboard_user(
    user_request: UserOnboardRequest
):
    """
    Onboard a new user into the access controls table.
    
    **Admin only endpoint**
    
    Args:
        user_request: User onboarding request with vzid, role, and domain_ids
        
    Returns:
        Created user access information
    """
    try:
        validate_admin_access()
        session_id = require_session_id()
        result = ConfigurationService.onboard_user(
            vzid=user_request.vzid,
            role=user_request.role,
            domain_ids=user_request.domain_ids,
            session_id=session_id
        )
        return UserAccessResponse(**result)
    except ValidationException as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.get("/users/{vzid}", response_model=UserAccessResponse)
async def get_user_access(
    vzid: str = Path(..., description="Verizon ID of the user")
):
    """
    Get access control information for a user.
    
    **Admin only endpoint**
    
    Args:
        vzid: Verizon ID of the user
        
    Returns:
        User access information
    """
    try:
        validate_admin_access()
        result = ConfigurationService.get_user_access(vzid)
        return UserAccessResponse(**result)
    except ValidationException as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.patch("/users/{vzid}", response_model=UserAccessResponse)
async def update_user_access(
    vzid: str = Path(..., description="Verizon ID of the user"),
    user_update: UserUpdateRequest = Body(...)
):
    """
    Update access control information for a user.
    
    **Admin only endpoint**
    
    Args:
        vzid: Verizon ID of the user
        user_update: User update request with optional role and domain_ids
        
    Returns:
        Updated user access information
    """
    try:
        validate_admin_access()
        session_id = require_session_id()
        result = ConfigurationService.update_user_access(
            vzid=vzid,
            role=user_update.role,
            domain_ids=user_update.domain_ids,
            session_id=session_id
        )
        return UserAccessResponse(**result)
    except ValidationException as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")


@router.get("/users", response_model=List[UserAccessResponse])
async def list_users():
    """
    List all users in the access control table.
    
    **Admin only endpoint**
    
    Returns:
        List of user access information
    """
    try:
        validate_admin_access()
        result = ConfigurationService.list_users()
        return [UserAccessResponse(**user) for user in result]
    except ValidationException as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")
=====================================
===========================================

