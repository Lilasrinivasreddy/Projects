0:00) Right, right, yes. (0:03) Yeah, so he basically sends five requests at once, then 30 requests and then 25 requests or something. (0:08) Yes, yes, yes, yeah, I do know. (0:09) So, we have that as a separate script right now. (0:13) Okay, okay. (0:14) So, what are we going to do is, we will create a framework. (0:18) Okay. (0:19) Okay, we call it a test framework, where you have this load or balance testing as one component. (0:27) Okay, okay. (0:28) Then the second component would be accuracy testing, where you have some golden set of (0:32) questions defined for each domain catalog. (0:34) And you will just click, you'll have a button saying that, you know, run test. (0:39) Okay. (0:40) You go back in the backend, you run some tests on those golden set of questions, (0:44) you do the accuracy comparison, come back with the results. (0:48) Okay, okay. (0:48) Even the load balance testing also the same, like if I select a batch number, (0:52) like if I say batch 20, then you send 20 requests at once, (0:55) do the load testing and come back with the results. (0:59) Okay, okay. (1:00) Right. (1:01) All of these results, you'll store it in a database, you'll keep track of it. (1:06) And then, you know, you can have a dashboard where you can come and monitor your performance, (1:10) like day to day, like, you know, over time, however, how many of this you do, (1:15) you can see how your tool is performing. (1:18) Okay. (1:19) Right. (1:20) So, if we can build that as a complete dashboard. (1:24) Okay, okay. (1:26) Which will again have the domain catalog section divided, right? (1:30) We'll have it at entire Q-verse level and then we'll put some filters for domains and catalogs. (1:35) And any coming in, like any admin coming in, (1:38) they can view the results for their particular domain catalog. (1:42) Okay, okay. (1:43) So, in the existing Q-verse framework only, that will be right, Akesh? (1:48) Yeah, within existing Q-verse only, we'll do it. (1:53) Okay, okay. (1:54) On Q-verse, we have one small dashboard, right? (1:56) On the homepage, you have four charts that we show. (1:59) Yes, yes, yes. (2:00) Yeah, if we mark the underscore, that breakdown, (2:04) markdown button, then we will get it, I think, charts. (2:09) Right, yeah. (2:10) So, now instead of those charts, I mean, those charts will still be there, (2:15) but we'll bring in this complete framework in place. (2:18) Okay. (2:18) Those charts are basically giving you user level history, right? (2:21) We'll create two more screens, one for catalog history and then system level history. (2:27) And on top of that, we'll add the second dashboard for our testing. (2:30) Okay, okay. (2:32) Got it, Akesh. (2:33) All right, so this, you have the entire analytics of your platform, right? (2:37) Okay, okay. (2:40) So, I thought if you could work on this, (2:42) but then if you are occupied with something else for next two days, then... (2:47) The thing is, I want you and Yogesh to work on this, both of you. (2:50) Okay. (2:51) It's going to be good. (2:52) So, Monday I can work, Akilesh. (2:55) Like, as I said, right, like, how I am doing is, see, generally in the morning session, (2:59) I complete my regular thing. (3:02) From afternoon session, I start working on keywords. (3:04) So, that's how I used to split. (3:09) So, actually, I can start on Monday, but... (3:13) If you can start on Monday, then I think it's fine. (3:15) But the target was, the way I was thinking is, (3:20) we'll use it in two days or three days max, (3:22) because it's going to be just APIs, (3:25) like there's no functionality there. (3:26) It's just APIs that you're writing. (3:28) And the scripts are already there. (3:29) Okay, okay. (3:30) So, I got two or three days. (3:32) But again, if you need some time, if you're occupied with something else, (3:34) you can give an additional day, I guess. (3:37) Yeah, mostly I can start on Monday. (3:41) I will try to finish, Akilesh, like by Tuesday only. (3:44) But Tuesday night, I have one again. (3:47) There is one more release. (3:50) So, it's like I cannot completely sit 50%, I can say. (3:55) But yeah, I will try to complete by Tuesday only. (3:57) I will target for Tuesday, better. (3:59) Tuesday or max Wednesday then. (4:02) Okay, okay. (4:03) Yeah, you have Yogesh also, right? (4:05) I mean... (4:06) Yeah, Yogesh Gupta, right? (4:07) Yeah, yeah. (4:08) Okay, okay. (4:09) I will connect with him. (4:11) I didn't get a chance to talk to him today. (4:13) So, you know, you had a chance either probably on Monday morning, right? (4:17) Uh-huh, sure. (4:17) So, even he will be working on FATEL for the LSP testing. (4:20) He's working day to day. (4:21) So, Monday, I think he can also start on Monday. (4:25) Okay, okay. (4:27) So, if both of you can build those APIs and bring the dashboards. (4:30) Because he's the one who has that script. (4:32) Okay, okay. (4:32) For batch testing. (4:34) Okay, okay, Akilesh. (4:35) I will ask him regarding that. (4:38) Okay, got it, Akilesh. (4:42) So, when you do it, right, just think of various filters you can put on the API, (4:46) like domain catalog or no filter at all. (4:48) Or like, you know, where exactly you need filters. (4:51) And also, if you're doing a test pipeline, accuracy testing, right? (4:54) Then you will put our domain catalog filter. (4:57) Okay, okay. (4:58) Because you're picking any golden questions for that particular domain catalog. (5:02) Then if you're doing a load balancing testing, (5:04) then you don't have to put any domain catalog, right? (5:06) It's just an overall tool that you're looking at. (5:08) Okay, okay. (5:09) So, we'll just, when we say batch of 30, (5:12) we'll send random domains, catalogs and the 30 requests. (5:16) Okay. (5:17) Something like that. (5:18) Okay, okay, yeah. (5:20) So, I will connect with Yogesh and I will get that script. (5:24) So, the major thing is this filter part, we have to work on the backend. (5:31) Okay, okay. (5:32) Yeah, he will have the hardcoded request body. (5:36) Instead, you'll have to get it on API. (5:38) Okay, okay, yeah, I understand. (5:42) Sure, Aklesh. (5:43) Yeah, I will connect with him and I will let you know. (5:46) If anything is there, I will connect on Monday. (5:49) At least I will start and implement some thing part. (5:54) This API part, I can start, I think. (5:57) So, yeah, then we can get a clear idea and Monday we can discuss on that, Aklesh. (6:03) Okay, okay. (6:04) And if you want to write, I mean, maybe do it this way. (6:10) Don't put it on QoS yet. (6:12) Just create it as a separate tool. (6:15) Okay, okay. (6:17) We'll host it as an API for now. (6:19) Okay, okay.

=========================

(0:17) This should be like a dashboard, however, any example like looker dashboard or whatever (0:28) we take, it should be a kind of dashboard where we have to show the filters as well (0:36) as. (0:38) So, this testing whatever latency testing is doing, those results only we need to show (0:43) right at this. (0:45) Correct, latency testing and also accuracy testing.
(0:48) Yeah, latency and accuracy. (0:49) So, what you wish to do is latency, yeah. (0:51) So, what you wish to do is latency, but you will have accuracy testing also.
(0:55) Okay. (0:55) Right. (0:56) And based on that accuracy testing you can do this.
(0:58) Okay. (0:58) So, one thing is, one more thing, for this accuracy testing we need golden questions. (1:05) Golden questions, right.
(1:06) Yes. (1:08) So, for the golden questions what we can do is on the UI itself for any domain catalog. (1:14) Okay.
(1:14) We will give an option to upload a csv file which has the questions and SQL queries. (1:20) Okay. (1:21) Right.
(1:22) We will take that csv file, store that csv file in gcp bucket. (1:27) Okay. (1:28) Okay.
(1:28) So, we will put it in a gcp bucket and in that gcp bucket we will create a folder called (1:33) golden questions and in that golden questions we will create a domain catalog folder and (1:37) in that domain catalog we will have that particular csv file. (1:41) Okay. (1:42) And there also we can give the golden questions which are specific name . We will create folder (1:44) and add a standard format.
(1:48) We will put it on gcp bucket. (1:50) Right. (1:50) Okay.
(1:50) Now whenever someone comes to do the test they can just say run test, it will go back (1:54) in the gcp bucket, pull that file, run the accuracy test check. (1:58) Okay. (1:59) Okay.
(2:00) And one more question, here whatever yogesh script, right, there we will be getting the (2:05) complete details in a csv format. (2:07) So, there also we have the questions, right, golden questions. (2:11) So, that way we can use it? (2:13) Right.
(2:15) You can use the same data. (2:17) Okay. (2:17) Okay.
(2:19) Okay. (2:22) Understand. (2:22) Understand.
(2:23) And also, in terms of dashboard, you want me to show like anything, anything like, you (2:28) know, trend or trend, some time series analysis, those kind of charts are sufficient. (2:34) That's right. (2:35) Okay.
(2:35) Okay. (2:36) Correct. (2:36) So, on do nothing, forget about UI for now, just go and go to the backend, take the latest (2:43) backend and ask for trend or the QoS development or anything.
(2:47) Okay. (2:47) Or you can also take, okay, there's a new branch, I think it's a branch name, it already (2:52) has the GCP bucket storage port. (2:54) Okay.
(2:54) Okay. (2:55) Okay. (2:55) Okay.
(2:56) The code is already there. (2:57) So, there is a helper function where you can just call the function with the CSV file (3:01) or whatever details, right. (3:02) and it will upload the file in that particular folder structure (3:07) okay (3:08) so you can take that code, you can create your own (3:12) like you know, in the routers where we have (3:15) genie reports, you can create a new router (3:17) or a test framework or something (3:19) yeah, okay (3:20) and on the test, you can create a test framework and add your APIs to it (3:25) okay, okay, I understand (3:27) okay, okay (3:28) yeah, I mean (3:29) eventually I think we need to bring it out and operate too (3:32) but for now, I'm just thinking (3:35) or even if you think you can do it as a separate tool, that's totally fine (3:40) like if you think of creating as a separate tool, it's easier to do that (3:43) take whatever code references you need from QoS (3:46) like for the storage bucket, so you know, type of creations, not database connections (3:50) take your RFNs tool and put it in your script (3:54) okay, okay, got it, got it (3:56) sure Akhilesh, I started, I mean (3:58) I was also occupied with other work like (4:01) this morning I was in continuous calls and other tasks (4:03) so probably by this weekend, like by Friday (4:06) we will make it as a target, Akhilesh (4:09) tomorrow I have some time, so I will start completely focusing (4:12) almost 80% I can focus on this (4:15) so we can have a review tomorrow (4:18) but by Friday, I will try to finish it (4:22) yeah, yeah, try to do that (4:23) and since enough code file is not read (4:26) what you can do is (4:28) give the QoS, whatever branch I will share (4:31) just give that branch to QoS store (4:34) and tell it that, you know, use those code pieces as a reference (4:38) to create a new tool by itself (4:41) instead of putting on QoS, just ask it to create a new tool (4:44) just give it all the functionalities (4:46) so what I do is, I create a .md file (4:48) where I give, you know, phase 1, phase 2, phase 3 instructions to it (4:53) so that way it will just follow and also try that (4:56) and since you are making it as a sub-trade framework (5:00) you can also add a filter for environment (5:05) like if you want to do the testing on dev environment (5:07) you can do that, if you want to do it on PLU (5:09) you can do that, if you want to go to prod trade (5:10) just switch the APIs to prod (5:13) so you will have one framework which can connect to all the three environments (5:18) and you just switch, you give the user the option to switch (5:23) ok, ok Akhilesh, got it (5:26) sure Akhilesh, I will check that (5:31) ok, you can share those details (5:34) I will share that, yeah (5:37) sure Akhilesh, I will connect more (5:39) yeah, only thing is your GCP bucket (5:42) it might not work in dev, it might just throw you an error saying (5:45) file not saved or something (5:47) so you will have to use prod GCP account for that (5:52) ok, and I also think I won't get any kind of access to the GCS bucket (5:59) personal access, no I don't think so (6:02) yeah, I won't get it (6:03) so let me test it tomorrow so I will come to know (6:08) like whether any kind of error (6:11) on logs if it says it successfully uploaded, I think it's fine (6:15) when I come back, we can now look at it (6:19) ok, sure Akhilesh, I will connect more (6:24) ok, ok (6:26) thank you Akhilesh, bye (6:28) thanks
Ready to Go Unlimited?
Get immediate access to...

Unlimited Transcriptions
Unlimited transcriptions for one person.
🚀
10 Hour Uploads
Each file can be up to 10 hours long / 5 GB. Upload 50 files at a time.
Whale
All Features
Translation to 134+ languages. Bulk exports. All transcription modes. Unlimited storage.
⚡️
Highest Priority
We'll always transcribe your files ASAP with the highest priority.
==============================

🛠️ Your Work (after Akhilesh’s instructions)
1. Backend APIs

Build a test framework router (like /test-framework) with APIs for:

Latency testing (using Yogesh’s script logic).

Accuracy testing (using golden questions).

Add environment filter → option to run tests on Dev / PLU / Prod.

Keep it as a separate tool (don’t push into QVerse UI yet), but reuse helper functions from QVerse codebase (like GCP upload).

2. Golden Questions (Accuracy Testing)

On UI (later) → allow admin to upload a CSV (questions + SQL).

Backend should:

Store file in GCP bucket under folder structure:
golden_questions/<domain>/<catalog>/<filename.csv>

When accuracy test runs → pull the file from bucket → execute test → compare results.

Use existing helper function (already in branch Akhilesh mentioned) for uploading files to GCS.

3. Dashboard Data

The dashboard must show:

Latency trends (time-series charts, p95/avg/max).

Accuracy trends (per domain/catalog).

Filters for domain, catalog, environment.

For now → just ensure backend APIs return summaries that can feed charts later.

4. Environment Handling

Framework must work across Dev / PLU / Prod by switching URLs + Auth.

Expose environment as a parameter in API → user can select where to run the test.

Note: GCS bucket works only with Prod account (Dev may throw “file not saved” error).

5. Phased Delivery

Phase 1: APIs + environment filter + CSV upload to GCS.

Phase 2: Accuracy testing integrated with golden questions.

Phase 3: Dashboard integration with charts/filters.

✅ What You Should Confirm with Akhilesh

Golden Questions Format

Should the CSV always have question + SQL query?

Do we need to store expected answers too, or just SQL?

Accuracy Checking Logic

Compare model output vs SQL result → exact match or similarity threshold?

Dashboard Metrics

For latency: avg / p95 / max?

For accuracy: % correct per domain/catalog?

Any other KPIs (throughput, error rate)?

Timeline

You mentioned Friday as target — confirm if this is okay for delivering Phase 1 (backend + upload + basic APIs).

👉 In short:
Your job is to build a backend test framework (APIs + GCS integration + env filter), feed results for latency & accuracy, and later extend to dashboard.
