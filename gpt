[TERADATA]
host = your_teradata_host
username = your_teradata_username
password = your_teradata_password

[GOOGLE_CLOUD]
project_id = your_gcp_project_id
dataset = your_bigquery_dataset
table = your_bigquery_table

[OIDC]
url = your_oidc_provider_url
client_id = your_client_id
client_secret = your_client_secret



import argparse
import teradatasql
import pandas as pd
from google.cloud import bigquery
import configparser
import os
import requests
import json
import time

# Function to load config file
def load_config(config_file='config.ini'):
    config = configparser.ConfigParser()
    config.read(config_file)
    return config

# Function to exchange and save OIDC token
def exchange_and_save_oidc_token_for_jwt(url: str, client_id: str, client_secret: str) -> None:
    print("Retrieving JWT from OIDC provider...")
    payload = {
        'grant_type': 'client_credentials',
        'client_id': client_id,
        'client_secret': client_secret,
        'scope': 'read'
    }
    
    try:
        response = requests.post(url, params=payload)
        response.raise_for_status()
        token = response.json()
        print("Saving token...")
        
        oidc_token_file_name = "oidc_token.json"
        oidc_token_path = oidc_token_file_name
        if os.path.isfile(oidc_token_path):
            os.remove(oidc_token_path)
        time.sleep(7)
        
        with open(oidc_token_path, 'w') as f:
            json.dump(token, f)
    except requests.HTTPError as e:
        raise e

# Function to set up BigQuery client with dynamic authentication using OIDC token
def bigquery_client():
    config = load_config()
    
    exchange_and_save_oidc_token_for_jwt(
        url=config['OIDC']['url'],
        client_id=config['OIDC']['client_id'],
        client_secret=config['OIDC']['client_secret']
    )

    print("Setting environment variables...")
    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'oidc_token.json'
    os.environ['GOOGLE_CLOUD_PROJECT'] = config['GOOGLE_CLOUD']['project_id']
    
    credentials, project_id = None, config['GOOGLE_CLOUD']['project_id']  # Placeholder for actual credentials logic
    client = bigquery.Client()
    
    print(f"project_id={project_id}, credentials={credentials}")
    
    return client, credentials

# Function to fetch data from Teradata table
def fetch_data_from_teradata(query, teradata_config):
    with teradatasql.connect(
        host=teradata_config['host'], 
        user=teradata_config['username'], 
        password=teradata_config['password']
    ) as connection:
        df = pd.read_sql(query, connection)
        return df

# Function to load data from CSV stored in Teradata
def load_data_from_csv_in_teradata(csv_file_path, teradata_config):
    query = f"SELECT * FROM {csv_file_path}"  # Assuming Teradata allows querying CSV files in this manner
    return fetch_data_from_teradata(query, teradata_config)

# Load DataFrame to BigQuery
def load_data_to_bigquery(df, gcloud_config):
    client = bigquery_client()[0]  # Get the BigQuery client
    
    table_id = f'{gcloud_config["project_id"]}.{gcloud_config["dataset"]}.{gcloud_config["table"]}'
    
    # Define the schema based on the DataFrame
    job_config = bigquery.LoadJobConfig()
    
    # Convert Pandas DataFrame dtypes to BigQuery dtypes
    job_config.schema = [
        bigquery.SchemaField(column, get_bq_type(df[column].dtype)) 
        for column in df.columns
    ]
    
    # Load data into BigQuery
    job = client.load_table_from_dataframe(df, table_id, job_config=job_config)
    
    # Wait for the load job to complete
    job.result()
    print(f"Loaded {job.output_rows} rows into {table_id}.")

# Function to map Pandas dtypes to BigQuery types
def get_bq_type(dtype):
    if pd.api.types.is_integer_dtype(dtype):
        return 'INTEGER'
    elif pd.api.types.is_float_dtype(dtype):
        return 'FLOAT'
    elif pd.api.types.is_bool_dtype(dtype):
        return 'BOOLEAN'
    elif pd.api.types.is_datetime64_any_dtype(dtype):
        return 'TIMESTAMP'
    else:
        return 'STRING'

# Main function to handle logic of CSV or Table in Teradata
def main():
    # Load configuration from config file
    config = load_config()

    # Setup argument parser for dynamic input
    parser = argparse.ArgumentParser(description="Fetch data from Teradata and load it into BigQuery")
    parser.add_argument('--data_type', type=str, required=True, choices=['csv', 'table'],
                        help="Type of data source: 'csv' for CSV file or 'table' for Teradata table")
    parser.add_argument('--source', type=str, required=True,
                        help="For CSV: Path to the CSV file in Teradata, for Table: Teradata table name")
    
    args = parser.parse_args()
    
    # Fetch Teradata config
    teradata_config = config['TERADATA']
    gcloud_config = config['GOOGLE_CLOUD']
    
    # Fetch data based on the data type
    if args.data_type == "csv":
        print("Loading data from CSV stored in Teradata...")
        df = load_data_from_csv_in_teradata(args.source, teradata_config)
    elif args.data_type == "table":
        print("Fetching data from Teradata table...")
        query = f"SELECT * FROM {args.source}"
        df = fetch_data_from_teradata(query, teradata_config)
    else:
        raise ValueError("Invalid data type. Choose 'csv' or 'table'.")
    
    # Load the data into BigQuery
    load_data_to_bigquery(df, gcloud_config)

if __name__ == "__main__":
    main()
