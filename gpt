import logging
import re
import pandas as pd
from django.http import JsonResponse
from django.utils.decorators import method_decorator
from django.views import View
from django.views.decorators.csrf import csrf_exempt
import load_result_to_bq as load_bq  # Custom module for database interactions

@method_decorator(csrf_exempt, name='dispatch')
class ExecuteHistorySQL(View):  
    def dispatch(self, request, *args, **kwargs):
        if not hasattr(self, 'logger'):
            self.logger = logging.getLogger(__name__)
            logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
        return super().dispatch(request, *args, **kwargs)

    def post(self, request, *args, **kwargs):
        try:
            if not hasattr(self, 'logger'):
                self.logger = logging.getLogger(__name__)

            print(f"DEBUG - Request method: {request.method}, Content Type: {request.content_type}")
            print(f"DEBUG - request.FILES: {request.FILES}") 

            # File key handling
            file_key = 'fileName' if 'fileName' in request.FILES else 'file' 

            if file_key not in request.FILES or not request.FILES[file_key]:
                self.logger.error("No file uploaded.")
                return JsonResponse({"status": "failure", "message": "No file uploaded."}, status=400)

            file = request.FILES[file_key]
            self.logger.info(f"Received file: {file.name}")

            # Read queries from file
            queries = self.read_queries_from_uploaded_file(file)
            if not queries:
                self.logger.error("No valid queries found.")
                return JsonResponse({"status": "failure", "message": "No valid queries found in the file."}, status=400)

            # Execute Queries
            execution_status = self.execute_queries(queries)
            if execution_status["status"] == "failure":
                return JsonResponse(execution_status, status=500)

            return JsonResponse({"status": "success", "message": "Queries executed successfully."}, status=200)

        except Exception as e:
            self.logger.error(f"Error processing request: {e}")
            return JsonResponse({"status": "failure", "message": f"Error processing request: {str(e)}"}, status=500)

    def execute_queries(self, queries):
        """
        Dynamically execute queries either on Teradata or BigQuery based on query content.
        If the query requires fetching data from Teradata, it first fetches the data and then loads it into BigQuery.
        """
        try:
            # Initialize BigQuery client
            bq_client, bq_creds = load_bq.bigquery_client(load_bq.dq_config)
            td_engine = None  # Initialize Teradata connection placeholder

            for query in queries:
                try:
                    # Identify if the query is for Teradata
                    if "from vz-it-pr-gk1v-cwlspr-0" in query.lower():  # Adjust condition for Teradata queries
                        if td_engine is None:
                            td_engine, _ = load_bq.teradata_client(load_bq.dq_td_config)

                        self.logger.info("Executing query on Teradata...")
                        df_results = pd.read_sql(query, td_engine)
                        df_results = df_results.rename(columns={str(col): str(col).lower() for col in df_results.columns.to_list()})

                    else:  # Execute in BigQuery
                        self.logger.info("Executing query on BigQuery...")
                        df_results = bq_client.query(query).to_dataframe()

                    self.logger.info(f"Fetched {len(df_results)} rows.")

                    # Load results into BigQuery
                    load_bq.load_result_to_bq_table(
                        column_details=self.get_bq_schema(),
                        df_load_data=df_results,
                        dq_bq_client=bq_client,
                        dq_credentials=bq_creds,
                        dq_dest_table_name=self.get_bq_table()
                    )

                    self.logger.info("Query executed and data loaded into BigQuery successfully.")

                except Exception as e:
                    self.logger.error(f"Query execution error: {e}")
                    return {"status": "failure", "message": f"Query execution error: {str(e)}"}

            return {"status": "success", "message": "All queries executed successfully."}

        except Exception as e:
            self.logger.error(f"Error initializing database connection: {e}")
            return {"status": "failure", "message": f"Error initializing database connection: {str(e)}"}

    def read_queries_from_uploaded_file(self, file):
        """
        Reads SQL queries from an uploaded file and returns a list of queries.
        """
        try:
            content = file.read().decode('utf-8')
            queries = [q.strip() for q in re.split(r';\s*\n', content) if q.strip()]
            self.logger.info(f"Read {len(queries)} queries from file.")
            return queries
        except Exception as e:
            self.logger.error(f"Error reading file: {e}")
            return []

    def get_bq_schema(self):
        """
        Returns the schema for BigQuery table.
        """
        return {
            'INTEGER': ['prfl_id', 'weekday', 'rpt_seq_num'],
            'DATE': ['data_dt'],
            'STRING': ['feature_name'],
            'NUMERIC': ['count_curr'],
            'TIMESTAMP': ['prfl_run_ts']
        }

    def get_bq_table(self):
        """
        Returns the target BigQuery table name.
        """
        return "vz-it-pr-izcv-idmcdo-0.dga_dq_tbls.dqaas_profile_rpt"
