INSERT INTO `<your_project_id>.<your_dataset>.dqaas_job_monitor_report`
(job_id, job_name, job_start_ts, job_end_ts, step_code, comments, user_id)
VALUES (1001, 'AutoProfile', TIMESTAMP('2025-02-05 08:00:00'), NULL, 'request_auto_profile_engine', 'Pending execution', 'test_user');


def read_metadata(self):
    """
    Fetch metadata for testing a single table.
    """
    query = f"""
    SELECT * FROM `{config.dqaas_mtd}`
    WHERE data_src = '{self.data_src}' AND active_flag = 'Y'
    LIMIT 1  -- Fetch only one table for testing
    """
    metadata_df = self.utils.run_bq_sql(
        bq_auth=config.dq_gcp_auth_payload,
        select_query=query
    )
    self.logger.info(f"Fetched 1 table metadata for testing: {metadata_df['table_name'].values}")
    return metadata_df


import argparse
import os
import sys
import traceback
from datetime import datetime
from google.cloud import bigquery
import logging

# Configure Logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

class JobMonitor:
    def __init__(self, project_id, dataset_name):
        """
        Initialize BigQuery client and setup configuration.
        """
        self.client = bigquery.Client()
        self.project_id = project_id
        self.dataset_name = dataset_name
        self.monitor_table = "dqaas_job_monitor_report"  # Table for monitoring logs
        logging.info("JobMonitor initialized.")

    def log_monitoring(self, job_id, job_name, step_code, user_id, status, comments):
        """
        Inserts or updates job monitoring details into BigQuery.
        """
        job_start_ts = datetime.now()
        
        query = f"""
        INSERT INTO `{self.project_id}.{self.dataset_name}.{self.monitor_table}`
        (job_id, job_name, job_start_ts, job_end_ts, step_code, user_id, comments)
        VALUES (@job_id, @job_name, @job_start_ts, @job_end_ts, @step_code, @user_id, @comments)
        """
        
        params = {
            "job_id": job_id,
            "job_name": job_name,
            "job_start_ts": job_start_ts,
            "job_end_ts": datetime.now(),  # Updated dynamically
            "step_code": step_code,
            "user_id": user_id,
            "comments": comments or "Execution successful."
        }

        logging.info(f"Logging job monitoring: {params}")
        try:
            self.client.query(query, params).result()
            logging.info("Inserted job monitoring details into `dqaas_job_monitor_report`.")
        except Exception as e:
            logging.error(f"Error logging monitoring: {str(e)}")

    def execute_autoprofile(self):
        """
        Executes the AutoProfile function and logs the result.
        """
        job_id = int(datetime.now().timestamp())  # Unique Job ID
        job_name = "AutoProfile"
        step_code = "request_auto_profile_engine"  # Capturing function dynamically
        user_id = os.popen("whoami").read().strip()  # Fetch user from server
        comments = "Execution started."

        # Insert Initial Log
        self.log_monitoring(job_id, job_name, step_code, user_id, "Running", comments)

        job_start_ts = datetime.now()

        try:
            # Simulating AutoProfile Execution
            from scripts.dq_processor import DQProcessor  # Importing from dq_processor
            dq_processor = DQProcessor(data_src="BQ")  # Using BigQuery as data source
            dq_processor.request_auto_profile_engine(logging, None, "BQ", None)  # Dummy execution
            
            comments = "Execution successful."
            status = "Success"

        except Exception as e:
            comments = f"Error: {str(e)} \n {traceback.format_exc()}"
            status = "Failure"

        job_end_ts = datetime.now()

        # Update Log Entry
        self.log_monitoring(job_id, job_name, step_code, user_id, status, comments)

def parse_arguments():
    """
    Parse command-line arguments for job monitoring.
    """
    parser = argparse.ArgumentParser(description="Job Monitoring Script.")
    parser.add_argument("--project_id", required=True, help="GCP Project ID")
    parser.add_argument("--dataset_name", required=True, help="BigQuery Dataset Name")

    return parser.parse_args()

if __name__ == "__main__":
    args = parse_arguments()
    monitor = JobMonitor(args.project_id, args.dataset_name)
    monitor.execute_autoprofile()




import argparse
import logging
import inspect
import uuid  # For unique job_id generation
import subprocess  # To get user_id from server
from datetime import datetime
from google.cloud import bigquery
import sys
import os
import traceback

# Import necessary modules
sys.path.insert(1, os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))
from scripts.auto_profile import AutoProfileEngine
import scripts.config_params as config
from scripts.common_handlers import CommonUtils

# Configure Logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


class JobMonitoring:
    def __init__(self, data_src):
        """
        Initialize BigQuery client and logging.
        """
        self.client = bigquery.Client()
        self.data_src = data_src
        self.utils = CommonUtils(logObj=logging)
        self.project_id = config.dq_gcp_data_project_id
        self.dataset_name = config.dq_bq_dataset

        # Table Names (Loaded from config)
        self.job_monitor_table = config.dqaas_job_monitor_report  # Job Monitoring Table
        self.rule_ctrl_table = config.dqaas_run_rule_ctrl_tbl  # Rule Control Table
        self.metadata_table = config.dqaas_mtd  # Metadata Table

        logging.info("JobMonitoring initialized.")

    # --- MONITORING FUNCTIONS ---

    def get_current_function(self):
        """
        Dynamically gets the function name where it is called from.
        """
        return inspect.stack()[1].function

    def get_user_id(self):
        """
        Retrieves the user ID from the server using 'whoami'.
        """
        return subprocess.getoutput("whoami").strip()

    def log_job_monitoring(self, job_id, job_name, job_start_ts, job_end_ts, comments):
        """
        Logs job monitoring details in `dqaas_job_monitor_report`
        """
        step_code = self.get_current_function()  # Get the function name dynamically
        user_id = self.get_user_id()  # Get user_id from the server

        query = f"""
        INSERT INTO `{self.project_id}.{self.dataset_name}.{self.job_monitor_table}`
        (job_id, job_name, job_start_ts, job_end_ts, step_code, comments, user_id)
        VALUES (@job_id, @job_name, @job_start_ts, @job_end_ts, @step_code, @comments, @user_id)
        """

        params = {
            "job_id": job_id,
            "job_name": job_name,
            "job_start_ts": job_start_ts,
            "job_end_ts": job_end_ts,
            "step_code": step_code,
            "comments": comments or "N/A",
            "user_id": user_id
        }

        logging.info(f"Logging job monitoring: {params}")
        try:
            self.client.query(query, params).result()
            logging.info(f"Inserted job monitoring details into `{self.job_monitor_table}`.")
        except Exception as e:
            logging.error(f"Error logging job monitoring: {str(e)}")

    # --- RULE CONTROL TABLE FUNCTIONS ---

    def insert_initial_metadata(self):
        """
        Inserts initial metadata records into `dqaas_run_rule_ctrl_tbl` at the start of the day.
        """
        try:
            logging.info("Inserting initial metadata records into rule control table...")

            query = f"""
            INSERT INTO `{self.project_id}.{self.dataset_name}.{self.rule_ctrl_table}`
            (profile_id, table_name, run_status, profile_date)
            SELECT profile_id, table_name, 'Pending', CURRENT_DATE()
            FROM `{self.project_id}.{self.dataset_name}.{self.metadata_table}`
            WHERE active_flag = 'Y'
            """
            self.client.query(query).result()
            logging.info("Initial metadata records inserted into rule control table.")

        except Exception as e:
            logging.error(f"Error inserting initial metadata records: {str(e)}")

    def update_rule_ctrl_table(self, profile_id, run_status, comments):
        """
        Updates the rule control table with success/failure status.
        """
        try:
            query = f"""
            UPDATE `{self.project_id}.{self.dataset_name}.{self.rule_ctrl_table}`
            SET run_status = @run_status, comments = @comments
            WHERE profile_id = @profile_id AND profile_date = CURRENT_DATE()
            """

            params = {
                "profile_id": profile_id,
                "run_status": run_status,
                "comments": comments
            }

            self.client.query(query, params).result()
            logging.info(f"Updated rule control table for profile_id {profile_id} with status {run_status}.")
        except Exception as e:
            logging.error(f"Error updating rule control table: {str(e)}")

    # --- MAIN PROCESS FUNCTION ---

    def run_auto_profile(self):
        """
        Runs AutoProfileEngine and captures monitoring data dynamically.
        """
        job_id = str(uuid.uuid4())  # Unique job_id
        job_name = "AutoProfileEngine"
        job_start_ts = datetime.now()

        try:
            logging.info("Running AutoProfileEngine...")
            auto_profile = AutoProfileEngine(data_src=self.data_src)

            # Insert Initial Metadata at Start of Day
            self.insert_initial_metadata()

            # Process the AutoProfile
            auto_profile.process_main()
            job_end_ts = datetime.now()

            # Capture logs dynamically from exceptions & execution logs
            comments = "Execution successful."

            # Log Job Monitoring (Success)
            self.log_job_monitoring(job_id, job_name, job_start_ts, job_end_ts, comments)

            # Update Rule Control Table (Success)
            self.update_rule_ctrl_table(profile_id="ALL", run_status="Success", comments=comments)

        except Exception as e:
            job_end_ts = datetime.now()
            error_trace = traceback.format_exc()
            comments = f"Error: {str(e)}\n{error_trace}"

            # Log Job Monitoring (Failure)
            self.log_job_monitoring(job_id, job_name, job_start_ts, job_end_ts, comments)

            # Update Rule Control Table (Failure)
            self.update_rule_ctrl_table(profile_id="ALL", run_status="Failure", comments=comments)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Job Monitoring for AutoProfile")
    parser.add_argument("--data_src", required=True, help="Data Source")
    args = parser.parse_args()

    monitor = JobMonitoring(args.data_src)
    monitor.run_auto_profile()
