import pandas as pd
import logging
import argparse
import sys
import os
from datetime import datetime, timedelta, time
from configparser import ConfigParser
import traceback

## Importing User Defined Modules
sys.path.insert(1,os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))
import scripts.config_params as config
from scripts.common_handlers import CommonUtils, set_logger
from scripts.auto_profile import AutoProfileEngine
from scripts.sql_rule_profile_bkp import RuleProfileEngine
from scripts.sql_rule_profile import RuleProfile
from scripts.source_chk_avail import SourceCheckAvailability




## Validating all the crons
def cron_validation(logger: logging, utils: CommonUtils, df_val: pd.DataFrame):
    cron_scheduler_list = df_val["PROFILE_SCHEDULE_TS"][~df_val["PROFILE_SCHEDULE_TS"].isna()].unique().tolist()
    logger.info(f"Scheduler List: {cron_scheduler_list}")

    start_min, end_min = utils.get_minute_range()
    logger.info('-------------------------------------------------------------')
    cron_schd_for_curr_run: list = []
    for cron_schd in cron_scheduler_list:

        cron_trigger_yn = utils.validate_croniter(
            # logger=logger,
            cron_schd_format=cron_schd,
            start_min_range=start_min
        )
        
        if cron_trigger_yn == 'Y':
            cron_schd_for_curr_run.append(cron_schd)
        
        logger.info('-------------------------------------------------------------')
    
    return cron_schd_for_curr_run           






## Argument Parser - For getting the Data Source
def get_profile_input_details():
    message = None
    try:
        if len(sys.argv[1:]) > 0:
          parser_args = argparse.ArgumentParser()
          parser_args.add_argument('--data_src', dest='data_src', type=str, required=True, help="Data Source is Mandatory")
          parser_args.add_argument('--profile_type', dest='profile_type', type=str, required=True, help="Profile Type is Mandatory")
          args = parser_args.parse_args()
          
          data_src = args.data_src
          data_src = data_src.upper()

          profile_type = args.profile_type
          profile_type = profile_type.upper()
          
          if data_src in config.APPL_DATA_SRC and profile_type in config.APPL_PRFL_TYPE:
            return data_src, profile_type.lower()
         
        message = f"""\n
        Data Source and Profile Type Not Found for Auto/Rule Profile Scheduled Tables
        Flag                    : --data_src
        Applicable Data Source  : {config.APPL_DATA_SRC}
        Example for Teradata    : python3.9 table_watcher_auto_profile_cron --data_src=TD
        Example for GCP         : python3.9 table_watcher_auto_profile_cron --data_src=GCP
        
        ** Data Source and Profile Type are Mandatory
        """
    except Exception as err:
        message = f"Error Occurred in  Argument Flag Validation. Error: {err}"
        
    raise Exception(message)


# @staticmethod
def get_run_process_mtd_condition(run_type: str, schd_type:str):
    if run_type in ("DR", "RR") and schd_type == "DAILY":
        return f" and upper(is_daily_flg) = 'Y' "
    elif run_type in ("MR", "RR") and schd_type == "MONTHLY":
        return f" and upper(is_daily_flg) = 'Y'  and upper(is_monthly_flg) = 'Y' "
    elif run_type == "AR" and schd_type == "ADHOC":
        return " and upper(is_adhoc_flg) = 'Y' "      
    return None

# @staticmethod
def get_run_process_details(run_type: str, schd_type:str):
    if run_type == "DR" and schd_type == "DAILY":
        return f"Daily Run Profiling Process"
    if run_type == "MR" and schd_type == "MONTHLY":
        return f"Monthly Run Profiling Process"
    if run_type == "AR" and schd_type == "ADHOC":
        return "Adhoc Run Profiling Process" 
    if run_type == "RR" and schd_type == "DAILY":
        return f"Rerun for Daily Profiling Process"
    if run_type == "RR" and schd_type == "MONTHLY":
        return f"Rerun for Monthly Profiling Process"
        
    return None

# @staticmethod
def data_avail_retry_time_range(current):
    retry_flag = False
    start = current.replace(hour=0, minute=0, second=0)
    end = start + timedelta(days=1)
    intervals = pd.date_range(start, end, 24//4+1).tolist()  #Hardcoded to run every 4 Hours
    for interval in intervals:
        if interval <= current <= interval+timedelta(minutes=29):
            retry_flag = True
    return retry_flag

## Main Function
def main(run_type:str, schd_type:str):
    try:
    
        data_src, profile_type = get_profile_input_details()
        
        ## Creating Logger File and Object
        logger: logging = set_logger(
            logger_path=config.LOGS_DIR,
            log_filename=f'{data_src}_time_based_{profile_type}_profile_cron',
            process_name=f'{profile_type}-Cron',
            # date_with_minutes_yn='Y'
        )
        utils: CommonUtils = CommonUtils(logObj=logger)
        
        #metadata_where_condition: str = get_run_process_mtd_condition(run_type=run_type, schd_type=schd_type)
        query = f"""
        select *
        from {config.dqaas_mtd} T1 join 
        {config.dqaas_taxonomy} T2 on
        T1.product_name = T2.product_name and T1.database_name=T2.database_name and T1.table_name=T2.table_name
        WHERE  active_flag = 'Y'
        AND data_src = '{data_src}'
        AND profile_type = '{profile_type}'
        AND (profile_schedule_ts IS NOT NULL OR profile_schedule_ts <> '')
        ORDER BY profile_id;
        """

        df_val = utils.run_bq_sql(
            bq_auth=config.dq_gcp_auth_payload,
            select_query=query
        )
        logger.info(f"Records Found: {len(df_val)}")
            
        cron_schd_for_curr_run = []
        if len(df_val) > 0:
            df_val = df_val.rename(columns={col: str(col).upper() for col in df_val.columns.tolist()})
            cron_schd_for_curr_run = cron_validation(
                logger=logger,
                utils=utils,
                df_val=df_val
            )

            if len(cron_schd_for_curr_run) > 0:
                df_val = df_val[df_val["PROFILE_SCHEDULE_TS"].isin(cron_schd_for_curr_run)]
                logger.info(f"Records Found for this hour: {len(df_val)}")
                logger.info(f"Original df val: {df_val}")
                

            if len(cron_schd_for_curr_run) > 0:
                df_val = df_val[df_val["PROFILE_SCHEDULE_TS"].isin(cron_schd_for_curr_run)]


        if len(df_val) == 0 or len(cron_schd_for_curr_run) == 0:
            logger.warning("No Tables Scheduled for Current Hour")
            return
        
        logger.info(f"Records Count Before Source Check: {len(df_val)}")
        
        #Rerun Source Check every 4 Hrs for all tables where is_available='N'
        if data_avail_retry_time_range(datetime.now()) :
             logger.info("Rerun all today's table with source check indicator 'N' and current time - 4 Hrs")
             src_chk_not_avail_query = f"""select a.* from {config.dqaas_mtd} a join {config.dqaas_src_chk_avail} b on a.DATABASE_NAME=b.DATABASE_NAME and a.TABLE_NAME=b.TABLE_NAME where cast(run_dt as DATE) = date(current_timestamp(),'US/Eastern') and data_availability_indicator='N' and b.profile_type='{profile_type}' and cast(b.update_made_ts AS datetime) <= TIMESTAMP_SUB(datetime(current_timestamp(),'US/Eastern'),INTERVAL 4 HOUR)"""
             src_chk_not_avail_mtd = utils.run_bq_sql(
                 bq_auth=config.dq_gcp_auth_payload,
                 select_query=src_chk_not_avail_query
             )
             df_val=pd.concat([df_val,src_chk_not_avail_mtd])
             df_val = df_val.drop_duplicates()
        src_chk = SourceCheckAvailability(data_src,df_val)
        src_chk.main(data_src, profile_type)
    
        #Execute only the tables which has data in source
        src_chk_avail_query = f"""WITH CTE AS(select DATABASE_NAME,TABLE_NAME,ROW_NUMBER() over (partition by DATABASE_NAME,TABLE_NAME order by insert_made_ts desc) as rn from {config.dqaas_src_chk_avail} where CAST(run_dt AS DATE) = date(current_timestamp(),'US/Eastern') and profile_type='{profile_type}' and data_availability_indicator='Y') select DATABASE_NAME,TABLE_NAME from CTE where rn=1"""
        src_chk_avail_y = utils.run_bq_sql(
             bq_auth=config.dq_gcp_auth_payload,
             select_query=src_chk_avail_query
         )
        src_chk_avail_y = src_chk_avail_y.rename(columns={col: str(col).upper() for col in src_chk_avail_y.columns.tolist()})
        df_val = pd.merge(df_val, src_chk_avail_y, how='inner',on=['DATABASE_NAME','TABLE_NAME'])

        #Load balacer code starts from here
        load_balancer_query = f"""WITH
            subdomain_table_counts AS (
            SELECT
                data_sub_dmn,
                profile_type,
                COUNT(table_name) AS table_count
            FROM
            `{config.dqaas_src_chk_avail}` where data_availability_indicator = 'Y'
            GROUP BY
                data_sub_dmn,profile_type ),
            total_tables AS (
            SELECT
            profile_type,
                SUM(table_count) AS total_table_count
            FROM
                subdomain_table_counts GROUP BY profile_type),
            server_check AS (
            SELECT
                stc.data_sub_dmn,
                stc.profile_type,
                stc.table_count,
                SUM(stc.table_count) over (partition by stc.profile_type order by stc.table_count) as check_table_count from subdomain_table_counts stc),
            final_check as (
                select sc.data_sub_dmn, sc.profile_type, sc.table_count,
                CAST(CASE 
                    WHEN sc.profile_type='auto' and sum(sc.table_count)  over (PARTITION BY sc.profile_type order by sc.table_count)  <= ( SELECT total_table_count / 2 FROM total_tables ts where profile_type='auto') THEN '002' 
                    WHEN sc.profile_type='auto' then '004'
                
                    WHEN sc.profile_type='rule' and sum(sc.table_count) over (PARTITION BY sc.profile_type order by sc.table_count)  <= ( SELECT total_table_count / 2 FROM total_tables ts where profile_type='rule') THEN '006' 
                    WHEN sc.profile_type='rule' then '007'
                
                
                END AS INT) AS assigned_server

            FROM
                server_check sc)
            SELECT
            sc.data_sub_dmn,
            sc.profile_type,
            sc.table_count,
            sc.assigned_server
            FROM
            final_check  sc
            ORDER BY
            sc.profile_type,    
            sc.assigned_server,
            sc.table_count DESC"""
        #Using Load balancer query data to update the control table with server status
        try:
            update_query = f"""UPDATE `{config.dqaas_src_chk_avail}` AS CT SET CT.server_name=LB.assigned_server FROM ({load_balancer_query}) AS LB WHERE CT.data_sub_dmn = LB.data_sub_dmn AND CT.profile_type=LB.profile_type AND CT.data_availability_indicator = 'Y'"""
            load_ct_table_with_server_details = utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=update_query
            )
            logger.info(f"{load_ct_table_with_server_details.to_string()}")
            logger.info(f"Server Assignment details updated in control table")
        except Exception as e:
            logger.error(f"Error in running load balancer update query. \nError: {err}")
        
    
    except Exception as err:
        traceback.print_exc()
        logger.error(f"Error in Source Check/Load balancer. \nError: {err}")
            

if __name__ == "__main__":
    main(run_type="DR", schd_type="DAILY")
=======================================================================================================================================================
=====================================================================================================================================
import base64
from pathlib import Path
import requests
import json
import os
import sys
import numpy as np
import pandas as pd
import google.auth
from requests.exceptions import HTTPError
from google.cloud import bigquery
from datetime import datetime, timedelta
import pandas_gbq
import time
import decimal
from functools import reduce
# from logger import Logger
import logging
from dqaas_opsgenie import Alert
import traceback

sys.path.insert(1,os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))
from utils.send_email import SendEmail
from config_data import get_config, set_logger
import scripts.config_params as config
from scripts.common_handlers import CommonUtils




"""

Run Types   | Description   
-----------------------------
DR          | Daily Run     
MR          | Monthly Run
AR          | Adhoc Run
RR          | Rerun Request


Scheduled Types | Description
-----------------------------------------
DAILY           | Daily Scheduled Rules
MONTHLY         | Monthly Scheduled Rules
ADHOC           | Rules for Adhoc Run


"""

class RuleProfile(object):
    def __init__(self, data_src: str=None):
        self.config = get_config()
        self.data_src = data_src
        if self.data_src not in config.APPL_DATA_SRC:
            raise Exception(f"Data Source not Provided. Error: Data Source Value is {data_src}")
            
        self.log = self.set_rule_profile_logger(
            process_name="RP-Main",
            data_src=data_src
        )

        self.utils = CommonUtils(logObj=self.log)
        self._set_attributes(self.config)
        self.run_process_details = ""

    # @staticmethod
    def set_rule_profile_logger(self, process_name:str, data_src: str):
        
        timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
        filename = f'{data_src}_rule_profile_{timestamp}'
        
        process_id = os.getpid()
        log: logging = set_logger(
            logger_path=config.SQL_RULE_PROFILE_LOGS,
            log_filename=filename,
            process_name=f'{process_name}-{process_id}',
            only_date_yn="Y",
        )
        
        return log
    
    def __del__(self):
        self.log.info("Closing the Class Object")
        
    def _set_attributes(self, config):
        bq_cred_dtls = config['gcp_metadata_db']
        profile_dtls = config['sql_rule_profile']
        master_mtd_table = config['master_mtd_table']
        home_path: str = self.config["dir"]["home_dir"]
        config_path = self.config["dir"]["config_dir"]
        
        # self.log_path = os.path.abspath(os.path.join(home_path, "logs"))
        # self.log_file_path = os.path.abspath(os.path.join(self.log_path, 'rule_based_1corpdata_' + datetime.now().strftime('%Y-%m-%d-%H-%M-%S')+'.log'))
        # self.log = Logger(name='1corpdata', path=self.log_file_path).log
        # self.log = self._set_logger(logger_path=self.config["dir"]["logs_dir"], filename="rule_based_1corpdata")
        # self.log: logging = set_logger(logger_path=self.config["dir"]["logs_dir"], log_filename="rule_based_1corpdata")
        
        
        self.email_template = os.path.abspath(os.path.join(self.config["dir"]["template_dir"], r'dq_common_message.html'))

        self.dq_opsgenie_client = Alert(
            api_key=config["ops_genie"]["dq_ops_genie_api_key"],
            proxy=bq_cred_dtls['gcp_http_proxy_url']
        )
        self.od_opsgenie_client = Alert(
            api_key=config["ops_genie"]["od_ops_genie_api_key"],
            proxy=bq_cred_dtls['gcp_http_proxy_url']
        )

        ##  Data Quality Service Account
        self.dq_project_id = bq_cred_dtls['dq_project_id']
        self.dq_auth_payload = {
            "client_id": bq_cred_dtls['dq_client_id'],
            "client_secret": bq_cred_dtls['dq_client_secret_key'],
            "token_url": bq_cred_dtls['gcp_token_url'],
            "conn_project_id": self.dq_project_id,
            "sa_json_file_dtls": os.path.abspath(os.path.join(config_path, bq_cred_dtls['dq_sa_json'])),
            "project_space": os.path.join(config_path, "dq_oidc_token.json")
        }

        ##  One Corp Data Service Account
        self.od_conn_project_id = bq_cred_dtls['od_data_project_id']
        self.od_mtd_project_id = bq_cred_dtls['od_mtd_project_id']
        self.one_corp_auth_payload = {
            "client_id": bq_cred_dtls['od_client_id'],
            "client_secret": bq_cred_dtls['od_client_secret_key'], 
            "token_url": bq_cred_dtls['gcp_token_url'],
            "conn_project_id": self.od_conn_project_id,
            "sa_json_file_dtls": os.path.abspath(os.path.join(config_path, bq_cred_dtls['od_sa_json'])),
            "project_space": os.path.join(config_path, "od_oidc_token.json")
            }

        # DQ Space Metadata and Report Table Details
        # dq_dataset_name = self.dq_project_id + "." + profile_dtls['dq_dataset_name']
        self.dq_mtd_table_name =  self.dq_project_id + "." + profile_dtls['dq_dataset_name'] + "." + master_mtd_table['dq_metadata_table']
        self.dq_report_table_name = self.dq_project_id + "." + profile_dtls['dq_dataset_name'] + "." + profile_dtls['dq_rpt_table_name']

        # OneCorp Space Metadata and Report Table Details
        # od_dataset_name = self.od_mtd_project_id + "." + profile_dtls['od_dataset_name']
        self.od_report_table_name = self.od_mtd_project_id + "." + profile_dtls['od_dataset_name'] + "." + profile_dtls['od_rpt_table_name']
        self.od_invalid_table_name = self.od_mtd_project_id + "." + profile_dtls['od_dataset_name'] + "." + profile_dtls['od_invalid_table_name']
        self.n_days_interval = int(profile_dtls["n_days_limit"])

        ##  Proxy ## Commented on 2024-07-25 
        # os.environ['http_proxy'] = bq_cred_dtls['gcp_http_proxy_url']
        # os.environ['https_proxy'] = bq_cred_dtls['gcp_https_proxy_url']
        # os.environ['no_proxy'] = bq_cred_dtls['gcp_no_proxy_urls']

        ## Mail Distro
        self.failure_alert_email_group = str(profile_dtls['default_failure_mail_group']).split(',')
        self.summary_alert_email_group = str(profile_dtls['default_summary_mail_group']).split(',')
        
        self.monthly_process_date, self.monthly_process_yn = self.validate_monthly_process(profile_dtls['monthly_process_day'])
        self.log.info(f'monthly_process_yn:{self.monthly_process_yn}, monthly_process_date:{self.monthly_process_date}')

        self.current_datetime = datetime.now()
        self.current_date = datetime.strftime(self.current_datetime, '%Y-%m-%d')
        self.previous_day_date = datetime.strftime((self.current_datetime - timedelta(days=self.n_days_interval)), '%Y-%m-%d %H:%M:%S')
        
        ##  Email Configuration
        self.email = SendEmail(loggerObj=self.log,
                               smtp=config["email_configuration"]["smtp_server_name"],
                               mail_from=config["email_configuration"]["sender_email_id"])

    # @staticmethod
    # def _set_logger(logger_path, filename: str):
    #     logger_filename = filename+ '_' + datetime.now().strftime('%Y-%m-%d-%H-%M-%S')+'.log'
    #     log_file_path = os.path.abspath(os.path.join(logger_path, logger_filename))
    #     logger_obj  = Logger(name='1corpdata', path=log_file_path).log
    #     return logger_obj
    
    @staticmethod
    def validate_monthly_process(day: int):
        current_date = datetime.strftime(datetime.now(), '%Y-%m-%d')
        monthly_process_date = datetime.strftime(datetime.now(), f'%Y-%m-{day}')
        monthly_process_yn: str =  "Y" if current_date == monthly_process_date else "N"
        return monthly_process_date, monthly_process_yn
    
        
    ## Create Ops Genie Ticket
    def opsgenie_alert(self, priority, message, description, details, one_corp_yn: str="N"):
        try:
            if one_corp_yn.upper() == "Y":
                # response = f"1Corp Opsgenie Alert.message:{message}, description:{description}, details:{details}, priority:{priority}"
                response = self.od_opsgenie_client.create_alert(message, description, details, priority)
            else:
                # response = f"DQ Opsgenie Alert.message:{message}, description:{description}, details:{details}, priority:{priority}"
                response = self.dq_opsgenie_client.create_alert(message, description, details, priority) 

            self.log.info(f"Opsgenies Response: {response}")
        except Exception as err:
            self.log.error(f"Error Occurred while Ops Genie Alert. Error: {err}")


    ## JWT Token Creation
    @staticmethod
    def exchange_and_save_oidc_token_for_jwt(self, url: str, client_id: str, client_secret: str, oidc_token_file_name:str) -> None:
        try:
            self.log.info('Retrieving JWT from OIDC provider...')
            payload = {'grant_type': 'client_credentials', 'client_id': client_id,
                       'client_secret': client_secret, 'scope': 'read'}
        
            response = requests.post(url=url, params=payload)
            response.raise_for_status()
            token = response.json()
            self.log.info('Saving token...')
            # Serializing json
            # oidc_token_file_name = "oidc_token.json"
            oidc_token_path = oidc_token_file_name
            if os.path.isfile(oidc_token_path):
                os.remove(oidc_token_path)
                time.sleep(7)

            with open(oidc_token_path, 'w') as f:  # don't change the file name
                json.dump(token, f)
        except HTTPError as e:
            raise HTTPError(f"Http Error. Error:{e}")
        except Exception as e:
            raise Exception(f"Error Ocurred in Generating the OIDC Token. Error:{e}")

    def dq_bigquery_client(self, auth: dict):
        self.log.info(f"Default Auth: {auth}")
        self.log.debug(f'url={auth["token_url"]}, client_id={auth["client_id"]}, client_secret={auth["client_secret"]}')
        
        self.exchange_and_save_oidc_token_for_jwt(self,
                                            url=auth["token_url"],
                                            client_id=auth["client_id"],
                                            client_secret=auth["client_secret"], 
                                            oidc_token_file_name=auth["project_space"]
                                            )
        
        self.log.info('Setting environment variable...')
        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = auth["sa_json_file_dtls"]
        os.environ['GOOGLE_CLOUD_PROJECT'] = auth["conn_project_id"]
        
        credentials, _ = google.auth.default()
        
        client = bigquery.Client(credentials=credentials, project=auth["conn_project_id"])
        self.log.info(f'Connected to {auth["conn_project_id"]} project space')
        
        return client, credentials
    #Used to connect to user mentioned project space dynamically using the json file path from dqaas_product_features_meta table
    def dq_bigquery_client_dynamic(self,json_folder_path:str=None):
        json_folder_path='/apps/opt/application/dev_smartdq/dev/dqaas/keys'
        if not json_folder_path:
            raise Exception("JSON folder path is required for remote client connection")
        folder =Path(json_folder_path)
        if not folder.exists() or not folder.is_dir():
            raise Exception(f"Invalid path")
        gcp_creds_file = str(folder / "gcp_creds.json")
        
        with open(gcp_creds_file, 'r') as file:
            gcp_creds_content = json.load(file)
        oidc_token_file_path = str(folder / "od_oidc_token.json")
        sa_config_file_path = str(folder / "sa-qa-j0nv-dqaas-oddo-0-oidc-27519-config.json")
        auth = {
            "client_id": gcp_creds_content['client_id'],
            "client_secret": gcp_creds_content['client_secret'], 
            "token_url": gcp_creds_content['token_url'],
            "conn_project_id": gcp_creds_content["conn_project_id"],
            "sa_json_file_dtls": sa_config_file_path,
            "project_space": oidc_token_file_path
        }
        self.log.info(f"Dynamic Auth: {auth}")
        
        self.log.debug(f'url={auth["token_url"]}, client_id={auth["client_id"]}, client_secret={auth["client_secret"]}')
        
        self.exchange_and_save_oidc_token_for_jwt(self,
                                            url=auth["token_url"],
                                            client_id=auth["client_id"],
                                            client_secret=auth["client_secret"], 
                                            oidc_token_file_name=auth["project_space"]
                                            )
        
        self.log.info('Setting environment variable...')
        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = auth["sa_json_file_dtls"]
        os.environ['GOOGLE_CLOUD_PROJECT'] = auth["conn_project_id"]
        
        credentials, _ = google.auth.default()
        
        client = bigquery.Client(credentials=credentials, project=auth["conn_project_id"])
        self.log.info(f'Connected to {auth["conn_project_id"]} project space')
        
        return client, credentials
        
    ## Retrieve Max Sequence
    def get_max_sequence(self, bq_client: bigquery.Client, sq_column: str, bq_tablename: str) -> int:
        try:
            if bq_client is None:
                raise Exception('BQ Client not Found')
            
            count = bq_client.query(f"select IFNULL(MAX({sq_column}), 0) from {bq_tablename}").to_dataframe()
            rpt_max_sequence = count.iloc[0, 0]
            self.log.info(f'Max Sequence: {rpt_max_sequence}')
            return rpt_max_sequence
        except Exception as err:
            self.log.error(f'Error While Finding Max Sequence for the Table {bq_tablename}. Error: {err}')

        return -1
        
    @staticmethod
    def round_off(val):  
        d = decimal.Decimal(val)
        return d.quantize(decimal.Decimal('.01'), decimal.ROUND_DOWN)

    def get_date_details(self, dq_bq_client:bigquery.Client, incr_dt_dict:dict):
        try:
            date_interval_query = f"""
                select  format_date('%Y-%m-%d', date({incr_dt_dict['start_date']})),
                        format_date('%Y-%m-%d', date({incr_dt_dict['end_date']})),
                        format_date('%Y%m', date({incr_dt_dict['start_year_month']})),
                        format_date('%Y%m', date({incr_dt_dict['end_year_month']})),
                        parse_date('%Y%m%d', format_date('%Y%m%d', date({incr_dt_dict['start_date']}))),
                        parse_date('%Y%m%d', format_date('%Y%m%d', date({incr_dt_dict['end_date']}))),
                        parse_date('%Y%m%d', concat(format_date('%Y%m', date({incr_dt_dict['start_year_month']})), '01')),
                        format_timestamp('%Y-%m-%d %T', timestamp({incr_dt_dict['invld_dt']}));
            """
            self.log.info(date_interval_query)
            
            result= dq_bq_client.query(date_interval_query).to_dataframe()
            self.log.info(result)
            
            return {
                'START_DATE': result.iat[0, 0],
                'END_DATE': result.iat[0, 1],
                'START_YEAR_MONTH': result.iat[0, 2],
                'END_YEAR_MONTH': result.iat[0, 3],
                'START_DATE_SRC_COL_DT_VAL': pd.Timestamp(datetime.strptime(str(result.iat[0, 4]), '%Y-%m-%d')),
                'END_DATE_SRC_COL_DT_VAL': pd.Timestamp(datetime.strptime(str(result.iat[0, 5]), '%Y-%m-%d')),
                'START_YEAR_MONTH_SRC_COL_DT_VAL': pd.Timestamp(datetime.strptime(str(result.iat[0, 6]), '%Y-%m-%d')),
                'INVL_DT': f"timestamp('{result.iat[0, 7]}')"
            }
        except Exception as err:
            raise RuntimeError(f"Error Occurred in Data Interval. Error: {err}")

    @staticmethod
    def get_run_process_mtd_condition(run_type: str, schd_type:str):
        if run_type in ("DR", "RR") and schd_type == "DAILY":
            return f" and upper(is_daily_flg) = 'Y' "
        elif run_type in ("MR", "RR") and schd_type == "MONTHLY":
            return f" and upper(is_daily_flg) = 'Y'  and upper(is_monthly_flg) = 'Y' "
        elif run_type == "AR" and schd_type == "ADHOC":
            return " and upper(is_adhoc_flg) = 'Y' "      
        return None
    
    @staticmethod
    def get_run_process_details(run_type: str, schd_type:str):
        if run_type == "DR" and schd_type == "DAILY":
            return f"Daily Run Profiling Process"
        if run_type == "MR" and schd_type == "MONTHLY":
            return f"Monthly Run Profiling Process"
        if run_type == "AR" and schd_type == "ADHOC":
            return "Adhoc Run Profiling Process" 
        if run_type == "RR" and schd_type == "DAILY":
            return f"Rerun for Daily Profiling Process"
        if run_type == "RR" and schd_type == "MONTHLY":
            return f"Rerun for Monthly Profiling Process"
          
        return None
    
    
    # def get_rule_metadata_details(self, dq_bq_client:bigquery.Client, metadata_condition: str) -> pd.DataFrame:
    def get_rule_metadata_details(self, dq_bq_client:bigquery.Client) -> pd.DataFrame:
        try:
            mtd_query = f""" 
                select *
                from {self.dq_mtd_table_name}
                where upper(is_active_flg) = 'Y'
                --{metadata_condition}
                order by profile_id;
            """.replace('[','(').replace(']',')')
            
            self.log.info(f"Metadata Query: {mtd_query}")
            return dq_bq_client.query(mtd_query).to_dataframe()
        
        except Exception as e:
            self.log.error(f"Error: {e}")
            
        return pd.DataFrame()
    def send_failure_status_email(self, table_name: str, rule: str, data_sub_dmn: str='', persona: str=''):
        try:
            receipents_email_addr_list = None
            if len(data_sub_dmn) > 0 and len(persona) > 0:
                receipents_email_addr_list = self.utils.get_mail_distro(df_val=self.mail_list, sub_dmn=data_sub_dmn, persona=persona)
            self.log.info(f"Receipents email details - Table: {receipents_email_addr_list}")

            if receipents_email_addr_list is None:
                receipents_email_addr_list = config.RP_DEFAULT_MAIL_GROUP
            self.log.info(f"Receipents email details - default: {receipents_email_addr_list}")
            
            self.log.info(f'Initiating eMail for failed rule {rule}')
            self.email.send_rules_error_report(
                email_template_filepath=config.rp_error_report_email_template,
                mail_subject=r'Rule Failure Report',
                receipents_email_id=receipents_email_addr_list,
                rule_id=rule,
                table_name=table_name
            )
        except Exception as e:
            self.log.error(f'Error while triggering error status.\n {e}')
    
    def run_td_rule_profile_engine(self, df_rule_data, rules_name):
        df_rule_data['col_vld_cnt'] = ""
        df_rule_data['col_invld_cnt'] = ""
        # df_rule_data['VALID_Prcnt'] = ""
        df_rule_data['col_invld_pct'] = ""
        df_rule_data['col_null_cnt'] = ""
        df_rule_data['col_dist_cnt'] = ""
        df_rule_data['col_min_val'] = ""
        df_rule_data['col_max_val'] = ""
        df_rule_data['col_vld_pct'] = ""
        df_rule_data['col_tot_cnt'] = ""      

        rules_error_list: list = []
        for count, rule in enumerate(rules_name):        
            valid_percent = 0.00
            invalid_percent = 0.00
            valid_count = 0
            invalid_count = 0
            null_records = 0
            unique_records = 0
            min = 0
            max = 0
            tablename = ''
            data_sub_dmn = ''
            rule_sql = ""
            try:
                self.log.info(f'Rule:{rule}')
                rule_id = df_rule_data.loc[count, 'PROFILE_ID']
                df_rule_data["RULE_ID"] = rule_id
                dbname = df_rule_data['DATABASE_NAME'][count]
                rule_sql = df_rule_data['RULE_SQL'][count]  # get the query for respective rule
                tablename = df_rule_data['TABLE_NAME'][count]
                data_sub_dmn = df_rule_data['DATA_SUB_DMN'][count]
                # db_engine = self.get_connection(dbname=dbname, section='profile_database')
                # df = pd.read_sql(query, db_engine)
                
                df = self.utils.run_teradata_sql(
                    db_name=dbname, 
                    query=rule_sql, 
                    td_auth=config.src_tbl_db_config
                )
                self.log.info(f"dataframe: {df}")
                dx = df[rule].value_counts()
                null_count = int(df.iloc[:, 0].replace(r'^\s*$', np.nan, regex=True).isnull().sum())
                unique = len(df.iloc[:, 0].replace(r'^\s*$', np.nan, regex=True).unique())
                try:
                    valid_percent = float(dx.loc['VALID']/len(df))
                    self.log.info(f"valid_percent: {valid_percent}")
                except:
                    valid_percent = 0.00
                try:
                    invalid_percent = float(dx.loc['INVALID']/len(df))
                    self.log.info(f"invalid_percent: {invalid_percent}")
                except:
                    invalid_percent = 0.00
                try:
                    valid_count = dx.loc['VALID']
                    self.log.info(f"valid_count: {valid_count}")
                except:
                    valid_count = 0
                try:
                    invalid_count = dx.loc['INVALID']
                    self.log.info(f"invalid_count: {invalid_count}")
                except:
                    invalid_count = 0
                try:
                    null_records = null_count
                except:
                    null_records = 0
                try:
                    unique_records = unique
                except:
                    unique_records = 0
                try:
                    min_val = df.iloc[:, 0].replace(r'^\s*$', 0, regex=True).min()
                    min = int(min_val)
                    self.log.info(f'min:{min}')
                except:
                    min = 0
                try:
                    max_val = df.iloc[:, 0].replace(r'^\s*$', 0, regex=True).max()
                    max = int(max_val)
                    self.log.info(f'max:{max}')
                except:
                    max = min
            except Exception as e:
                self.log.error(f'Error in Validating the Rule({rule}).\nError info:{e}')
                # eMail Error Report
                self.send_failure_status_email(tablename, rule)
                rules_error_list.append({'table':tablename, 'rules': rule , 'query': rule_sql})

            # df_rule_data.loc[count, 'VALID_Prcnt'] = valid_percent*100
            df_rule_data.loc[count, 'col_vld_pct'] = valid_percent*100  # df_rule_data['VALID_Prcnt']
            df_rule_data.loc[count, 'col_invld_pct'] = invalid_percent*100
            df_rule_data.loc[count, 'col_vld_cnt'] = int(valid_count)
            df_rule_data.loc[count, 'col_invld_cnt'] = int(invalid_count)
            df_rule_data['col_tot_cnt'] = df_rule_data['col_vld_cnt'] + df_rule_data['col_invld_cnt']
            df_rule_data.loc[count, 'col_null_cnt'] = int(null_records)
            df_rule_data.loc[count, 'col_dist_cnt'] = int(unique_records)
            df_rule_data.loc[count, 'col_min_val'] = min
            df_rule_data.loc[count, 'col_max_val'] = max


        return df_rule_data, rules_error_list
    
    def run_gcp_rule_profile_engine(self, df_rules_list: pd.DataFrame, val_to_replace: dict):
        try:
            df_rules_list.to_csv("/apps/opt/application/dev_smartdq/dev/dqaas/dqaas/df_rules_list.csv")
            self.log.info('-------------------------------------------------------------------------')
            self.log.info('Rule Profile Engine - Initiated')
            self.log.info('-------------------------------------------------------------------------')

            error_list = []
            df_rules_list['COL_VLD_CNT'] = ""
            df_rules_list['COL_INVLD_CNT'] = ""
            df_rules_list['COL_INVLD_PCT'] = ""
            df_rules_list['COL_NULL_CNT'] = ""
            df_rules_list['COL_DIST_CNT'] = ""
            df_rules_list['COL_MIN_VAL'] = ""
            df_rules_list['COL_MAX_VAL'] = ""
            df_rules_list['COL_VLD_PCT'] = ""
            df_rules_list['COL_TOT_CNT'] = "" 
            
            for idx in df_rules_list.index:
                self.log.info('-------------------------------------------------------------------------')
                is_remote = df_rules_list.iloc[idx]["run_queries_on_remote"].upper()
                if is_remote.upper() == 'Y':
                    folder_path = df_rules_list.iloc[0]["WIF_JSON_PATH"]
                    bq_client, _ = self.dq_bigquery_client_dynamic(folder_path)
                else:
                    bq_client, _ = self.dq_bigquery_client(self.dq_auth_payload)
                rule = df_rules_list.loc[idx, 'RULE_NAME']
                table_name = df_rules_list.loc[idx, 'TABLE_NAME']
                rule_sql = df_rules_list.loc[idx, 'RULE_SQL']
                valid_count, invalid_count = 0, 0
                try:
                    rule_id = df_rules_list.loc[idx, 'PROFILE_ID']
                    df_rules_list["profile_id"] = rule_id
                    self.log.info(f'index:{idx}, profile_id:{rule_id}, rule_name:{rule}')
                    
                    ##  Replacing the placeholder in SQL Query with actual values.
                    ##  val_to_replace argument is a dict variable which has values to replace in sql
                    rule_sql = reduce(lambda sql, replace_str: sql.replace(*replace_str), [rule_sql, *list(val_to_replace.items())])
                    self.log.info(f"RuleSQL:{rule_sql}")
                    
                    df_result = bq_client.query(rule_sql).to_dataframe()

                    self.log.info(f'\n{df_result}')

                    try:
                        valid_count =  df_result[df_result[rule] == 'VALID']['count']
                        self.log.info(f"valid count1: {valid_count}")
                        valid_count = valid_count.iloc[0] if len(valid_count) > 0 else 0
                        self.log.info(f"valid count2: {valid_count}")
                    except:
                        valid_count = 0

                    try:
                        invalid_count =  df_result[df_result[rule] == 'INVALID']['count']
                        self.log.info(f"invalid count1: {invalid_count}")
                        invalid_count = invalid_count.iloc[0] if len(invalid_count) > 0 else 0
                        self.log.info(f"invalid count2: {invalid_count}")
                    except:
                        invalid_count = 0

                except Exception as e:
                    self.log.error(f'Error in executing Rule SQL. Error:{e}')

                    error_list.append({'table': table_name,
                                        'rules': rule,
                                        'query': rule_sql})
                    
                    self.email.send_common_message(mail_subject='Rule Failure',
                                                    message=f'Rule Failed.<br>Rule: {rule}<br>Query: {rule_sql}',
                                                    receipents_email_id=self.failure_alert_email_group,
                                                    email_template_filepath=self.email_template)

                    self.opsgenie_alert(one_corp_yn="Y",
                                        priority="P2",
                                        message='DQ-2.0 Rule Failure',
                                        description=f'DQ-2.0 Rule Failed.<br>Rule: {rule}<br>Query: {rule_sql}',
                                        details={'Message': 'DQ-2.0 Rule Failure', 'Rule': rule, 'Query': rule_sql})
                    
                total_record_count, valid_percent, invalid_percent = 0, 0.0, 0.0
                total_record_count = int(valid_count) + int(invalid_count)
                try:
                    valid_percent = float(valid_count/total_record_count) * 100
                except:
                    valid_percent = 0.00
                try:
                    invalid_percent = float(invalid_count/total_record_count) * 100
                except:
                    invalid_percent = 0.00

                self.log.info(f'valid_count:{valid_count}, invalid_count:{invalid_count}, total_record_count:{total_record_count}')
                self.log.info(f'valid_percent:{valid_percent}, invalid_percent:{invalid_percent}')

                if valid_percent < 100:
                    self.opsgenie_alert(one_corp_yn="Y",
                                priority="P3",
                                message='DQ-2.0 Score less than 100',
                                description=f'Rule Score less than 100 <br> Rule: {rule}',
                                details={'Message': 'DQ-2.0 Score less than 100', 'Rule': rule})
                
                df_rules_list.loc[idx, 'COL_VLD_PCT'] = self.round_off(float(valid_percent))
                df_rules_list.loc[idx, 'COL_INVLD_PCT'] =  self.round_off(float(invalid_percent))
                df_rules_list.loc[idx, 'COL_VLD_CNT'] = int(valid_count)
                df_rules_list.loc[idx, 'COL_INVLD_CNT'] = int(invalid_count)
                df_rules_list.loc[idx, 'COL_TOT_CNT'] = total_record_count
                df_rules_list.loc[idx, 'COL_NULL_CNT'] = 0
                df_rules_list.loc[idx, 'COL_DIST_CNT'] = 0
                df_rules_list.loc[idx, 'COL_MIN_VAL'] = 0
                df_rules_list.loc[idx, 'COL_MAX_VAL'] = 0
            df_rules_list.to_csv("/apps/opt/application/dev_smartdq/dev/dqaas/dqaas/out.csv")    
            return df_rules_list, error_list
        except Exception as err:
            self.log.error(f"Error Occurred in Rules Profiling. Error: {err}")
        
        return pd.DataFrame(), []

    @staticmethod
    def critical_type_header_name(flag: str = None):
        if flag is None:
            return ""
        if flag == 'Y':
            return 'Critical '
        if flag == 'N':
            return 'Non Critical '
        return ""
    
    def send_summary_sub_dmn_level_mail(self, sub_domain: str, rules_data=pd.DataFrame(), df_rules_error_list=pd.DataFrame(), critical_flag=None):
        try:
            self.log.info('Preparing Overall Summary for Persona Group ------------------')
            email_cols_list = config.RP_REQD_SUMMARY_COLS
            df_email_rules_data = rules_data.loc[:, rules_data.columns.isin(email_cols_list)]
            df_email_rules_data = df_email_rules_data.sort_values(by=['col_vld_pct'])
            df_email_rules_data['col_vld_pct'] = round(df_email_rules_data['col_vld_pct'].astype(float), 2)
            # df_email_rules_data['RULE_RUN_DT'] = pd.to_datetime(df_email_rules_data['RULE_RUN_DT']).dt.date
            self.log.info(f'SQL Profile eMail Columns: {df_email_rules_data.columns.tolist()}')
            df_email_rules_data = df_email_rules_data[email_cols_list]
            df_email_rules_data = df_email_rules_data.rename(columns=config.RP_EMAIL_SUMMARY_COL_RENAME)
            df_email_rules_data = df_email_rules_data.reset_index(drop=True)

            report_header_name = self.critical_type_header_name(flag=critical_flag)
            data_sub_dmn=sub_domain
            self.log.info(f'Sub Domain:{data_sub_dmn}')
            self.mail_list = self.utils.get_email_distros_from_table(data_sub_dmn_list=[sub_domain])
            
            email_group = self.utils.get_mail_distro(df_val=self.mail_list, sub_dmn=data_sub_dmn, persona='PERSONA_3')
            self.log.info(f'Email Group:{email_group}')
            
            if len(email_group ) == 0:
                email_group = config.RP_DEFAULT_MAIL_GROUP ## receipents_email_id
                
            self.email.send_rules_email_message(
                df_val=df_email_rules_data,
                df_err_val=df_rules_error_list,
                email_template_filepath=config.rp_summary_report_email_template,
                receipents_email_id=email_group, #receipents_email_id,
                mail_subject=report_header_name + 'Rule Summary for '+ data_sub_dmn,
                report_header_name=report_header_name + 'Rule Summary for '+ data_sub_dmn
            )
            
        except Exception as e:
            self.log.error('Error While Initiating Overall Summary Email.\n Error Info:', e)

    def send_summary_table_level_mail(self, df_mail_summary: pd.DataFrame, error_rules_list: list, rule_run_dt,schd_type):
        try:

            self.log.info(f'Summary Result Len: {len(df_mail_summary)}, Error list : {error_rules_list}')
            if len(df_mail_summary) > 0:
                src_table_list = df_mail_summary["table_name"].unique().tolist()
                self.log.info(f'Source Table List: {src_table_list}')
                
                df_rules_error_list = pd.DataFrame.from_records(error_rules_list)
                df_rules_error_list = df_rules_error_list.reset_index(drop=True)
                for tbl in src_table_list:
                    product_name = df_mail_summary[df_mail_summary['table_name']==tbl]['product_name'][0]
                    data_sub_dmn =df_mail_summary[df_mail_summary['table_name']==tbl]['data_sub_dmn'][0]
                    self.log.info(f'Product Name: {product_name}')
                    self.log.info(f'Data Sub Domain: {data_sub_dmn}')
                    self.log.info(f'Source Table: {tbl}')
                    try:
                        self.log.info(f'{schd_type} run: {tbl} Summary Email - DQ-2.0')
                        if schd_type == "DAILY":
                            subject = f'Daily run: DQ-2.0 - {product_name}|{data_sub_dmn}|{tbl} Summary Mail - {datetime.strftime(rule_run_dt, "%Y-%m-%d")}'
                        elif schd_type== "MONTHLY":
                            subject = f'Monthly run: DQ-2.0 - {product_name}|{data_sub_dmn}|{tbl} Summary Mail - {datetime.strftime(rule_run_dt, "%Y-%m-%d")}'
                        else:
                            subject = f'Ad-hoc run: DQ-2.0 - {product_name}|{data_sub_dmn}|{tbl} Summary Mail - {datetime.strftime(rule_run_dt, "%Y-%m-%d")}'

                        message = 'Please find the below summary.<br>'
                        #here removed data_dmn from the below email_cols_list
                        email_cols_list = ['rule_run_dt', 'db_name', 'table_name', 'src_col', 'dq_pillar',
                                        'rule_name', 'col_vld_cnt', 'col_invld_cnt', 'col_vld_pct', 'dq_status']

                        df_email_rules_data: pd.DataFrame = df_mail_summary[email_cols_list][df_mail_summary["table_name"]==tbl]
                        if len(df_email_rules_data) > 0:
                            df_email_rules_data = df_email_rules_data.sort_values(by=['col_vld_pct'])
                            df_email_rules_data['col_vld_pct'] = df_email_rules_data['col_vld_pct'].astype(float).map(self.round_off)

                            self.log.info(f'SQL Profile eMail Columns: {df_email_rules_data.columns.tolist()}')
                            df_email_rules_data = df_email_rules_data.rename(columns={  
                                                                                        'db_name': 'Database',
                                                                                        'table_name': 'Table',
                                                                                        'src_col': 'Column',
                                                                                        'dq_pillar': 'DQ Pillar',
                                                                                        'rule_name': 'Measure',
                                                                                        'col_vld_cnt': 'Valid Count',
                                                                                        'col_invld_cnt': 'Invalid Count',
                                                                                        'col_vld_pct': 'DQ Score',
                                                                                        'dq_status': 'Indicator',
                                                                                        # 'data_dmn': 'Domain',
                                                                                        'rule_run_dt': 'Date'})
                            df_email_rules_data = df_email_rules_data.reset_index(drop=True)

                        if len(df_rules_error_list) > 0:
                            df_rules_error_list_email = df_rules_error_list[df_rules_error_list["table"]==tbl]
                            df_rules_error_list_email = df_rules_error_list_email.reset_index(drop=True)
                            if len(df_rules_error_list_email) > 0:
                                addl_msg = f'<br><b>Rule Profile Error List:</b>{df_rules_error_list_email.to_html()}'
                                message += addl_msg

                        self.email.send_common_message(email_template_filepath=self.email_template,
                                                       mail_subject=subject,
                                                       message=message,
                                                       receipents_email_id=self.summary_alert_email_group,
                                                       df_val=df_email_rules_data)
                        self.log.info(f'Successfully Triggered {tbl} Summary Email for DQ-2.0')
                    except Exception as e:
                        self.log.error( f'Error Occurred in {tbl} Summary Email for DQ-2.0. Error:{e}')
                    continue

        except Exception as e:
            self.log.error( f'Error Occurred in Summary Email for  DQ-2.0. Error:{e}')
            self.opsgenie_alert(priority="P1",
                           message='DQ-2.0 - Summary Email Error',
                           description=f'Failure Occurred while sending summary email',
                           details={'Message':'DQ-2.0 - Summary Email Error'})


    def execute_invalid_sqls(self, df_invalid_rec:pd.DataFrame, val_to_replace: dict, date_interval: dict, rule_run_dt):
        try:

            self.log.info('-------------------------------------------------------------------------')
            self.log.info('Invalid SQL Query Execution - Initiated')
            self.log.info('-------------------------------------------------------------------------')
            
            df_invalid_rec = df_invalid_rec[(df_invalid_rec['invalid_records_flag']=='Y') & (df_invalid_rec['col_invld_cnt'] > 0)]
            df_invalid_rec = df_invalid_rec.reset_index(drop=True)
            self.log.info(f'Invalid data rules length: {len(df_invalid_rec)}')

            if len(df_invalid_rec) ==  0:
                self.log.warning("No Records Found for Populating Invalid Records")
            else:
                import re
                od_bq_client, _ = self.dq_bigquery_client(auth=self.one_corp_auth_payload)
                for idx in df_invalid_rec.index:
                    self.log.info('-------------------------------------------------------------------------')
                    rule = df_invalid_rec.loc[idx, 'rule_name']
                    rule_id = df_invalid_rec.loc[idx, 'profile_id']
                    #invalid_rec_sql ## Changed
                    invalid_sql_query = df_invalid_rec.loc[idx, 'invld_rec_sql']
                    try:

                        invalid_sql_query = reduce(lambda sql, replace_str: sql.replace(*replace_str), [invalid_sql_query, *list(val_to_replace.items())])
                        self.log.info(f"RuleSQL:{invalid_sql_query}")
                        
                        invalid_sql_query = re.sub("CURRENT_TIMESTAMP()", date_interval['INVL_DT'], invalid_sql_query, flags=re.IGNORECASE)
                        invalid_sql_query = re.sub("CURRENT_TIMESTAMP", date_interval['INVL_DT'], invalid_sql_query, flags=re.IGNORECASE)
                        
                        self.log.info(f'Invalid SQL index: {idx}, profile_id: {rule_id}, rule: {rule}')
                        self.log.info(f"Invalid Records Query: {invalid_sql_query}")   
                        result = od_bq_client.query(invalid_sql_query)
                        result.result()
                        self.log.info(f'result: {result.num_dml_affected_rows}')


                    except Exception as e:
                        self.log.error(f'Error Occurred while validating invalid_rec_sql: {e}')
                        self.email.send_common_message(mail_subject=f'Invalid SQL Failed',
                                                message=f'Invalid SQL Failed.<br>Rule: {rule}<br>Invalid SQL: {invalid_sql_query}',
                                                receipents_email_id=self.failure_alert_email_group,
                                                email_template_filepath=self.email_template)
                        self.opsgenie_alert(one_corp_yn="Y",
                                    priority="P2",
                                    message='DQ-2.0 Invalid SQL Failure',
                                    description=f'Invalid SQL Failed.<br>Rule: {rule}<br>Invalid SQL: {invalid_sql_query}',
                                    details={'Message': 'DQ-2.0 Invalid SQL Failure', "Rule":rule, "Invalid SQL": invalid_sql_query})
                        
                        continue

        except Exception as err:
            raise RuntimeError(f"Error Occurred in Invalid SQLs execution block. Error:{err} ")

    def load_result_to_bq_tables(self, df_rules_list: pd.DataFrame, rules_execution_process_details:dict):
        try:
            self.log.info('-------------------------------------------------------------------------')
            self.log.info('Loading Result to Report Table - Initiated')
            self.log.info('-------------------------------------------------------------------------')
            self.log.info(f"rules_execution_process_details:{rules_execution_process_details}")
            # df_rules_list['rule_prfl_num'] = ''
            df_rules_list['prfl_run_dt'] = rules_execution_process_details['rules_execution_date']

            # df_rules_list['src_col_dt_val'] = rules_execution_process_details['business_date']
            report_reference_key = datetime.now().strftime("%Y%m%d%H%M%S%f")
            df_rules_list['rpt_ref_key'] = report_reference_key

            df_rules_list['threshold_limit'] = df_rules_list['threshold_limit'].fillna(float(self.config['sql_rule_profile']["default_min_thrsd"])).astype('float64')
            df_rules_list['max_threshold_limit'] = df_rules_list['max_threshold_limit'].fillna(float(self.config['sql_rule_profile']["default_max_thrsd"])).astype('float64')

            df_rules_list['dq_status'] = ''           
            df_rules_list['dq_status'] = np.where((df_rules_list['col_vld_pct'] >= df_rules_list['max_threshold_limit']),
                                               'Good', df_rules_list['dq_status'])
            df_rules_list['dq_status'] = np.where((df_rules_list['col_vld_pct'] <= df_rules_list['threshold_limit']),
                                               'Bad', df_rules_list['dq_status'])
            df_rules_list['dq_status'] = np.where((df_rules_list['col_vld_pct'] < df_rules_list['max_threshold_limit']) &
                                               (df_rules_list['col_vld_pct'] > df_rules_list['threshold_limit']),
                                               'Average', df_rules_list['dq_status'])
            df_rules_list['dq_status'] = np.where((df_rules_list['col_vld_cnt'] == 0) & (df_rules_list['col_invld_cnt'] == 0),
                                               'No Data', df_rules_list['dq_status'])
            df_rules_list['col_vld_pct'] = np.where((df_rules_list['col_vld_cnt'] == 0) & (df_rules_list['col_invld_cnt'] == 0),
                                               np.nan, df_rules_list['col_vld_pct'])

            # df_rules_list['run_type'] = rules_execution_process_details['run_type']
            # df_rules_list['schd_type'] = rules_execution_process_details['schd_type']
            
            ##  Column Details for loading data into Reporting Tables
            col_list = ['rpt_ref_key', 'profile_id', 'prfl_run_dt', 'dq_pillar','table_name','col_name','col_completeness','col_uniqueness','col_conformity','col_validity','col_integrity', 'col_tot_cnt',
                        'col_vld_cnt', 'col_invld_cnt', 'col_vld_pct', 'col_invld_pct','profile_type','dq_status']
            decimal_cols_list = ['col_vld_pct', 'col_invld_pct']
            int_cols_list = [  'col_tot_cnt', 'col_vld_cnt', 'col_invld_cnt']
            str_cols_list = [ 'dq_pillar' ,'dq_status']

            self.log.info('-------------------------------------------------------------------------')
            self.log.info('Loading Results into DQ Report Table')
            self.log.info('-------------------------------------------------------------------------')

            try:
                
                
                # rpt_max_sequence = self.get_max_sequence(bq_client=dq_bq_client,
                #                                          bq_tablename=self.dq_report_table_name,
                #                                          sq_column='rule_prfl_num'
                #                                         )
                
                # self.log.info(f"Max Sequence in DQ Space:{rpt_max_sequence}")
                # if rpt_max_sequence == -1:
                #     raise RuntimeError("Error Occurred while Identifying the Max Sequence in DQ Space")
                
                # dq_df_rules_list = df_rules_list.assign(rule_prfl_num=[rpt_max_sequence +i for i in range(1, len(df_rules_list)+1)])
                dq_df_rules_list = df_rules_list.copy()
                for col in decimal_cols_list:
                    dq_df_rules_list[col] = dq_df_rules_list[col].astype(str).map(decimal.Decimal)
                for col in int_cols_list:
                    dq_df_rules_list[col] = dq_df_rules_list[col].astype('int64')
                for col in str_cols_list:
                    dq_df_rules_list[col] = dq_df_rules_list[col].astype(str)
                load_rules_data = dq_df_rules_list.loc[:, dq_df_rules_list.columns.isin(col_list)]
                load_rules_data = load_rules_data.reset_index(drop=True)
                load_rules_data = load_rules_data.drop_duplicates()
                load_rules_data = load_rules_data.loc[:, ~load_rules_data.columns.duplicated()]
                load_rules_data.to_csv("/apps/opt/application/dev_smartdq/dev/dqaas/dqaas/load_rules_data.csv")
                
                self.log.info(f'Report Length:{len(load_rules_data)}, Table: {self.dq_report_table_name} \n{load_rules_data}')
                dq_bq_client, dq_credentials = self.dq_bigquery_client(self.dq_auth_payload)
                pandas_gbq.to_gbq(dataframe=load_rules_data,
                                destination_table=self.dq_report_table_name,
                                if_exists='append',
                                credentials=dq_credentials,
                                project_id=self.dq_project_id,
                                )
                self.log.info(f'Data Loaded Successfully to the table({self.dq_report_table_name})')
            except Exception as e:
                print(traceback.format_exc())
                self.log.error(f'Error Occurred While Loading Data into DQ Dataset. Error Info: {e}')
                self.opsgenie_alert(priority="P1",
                               message='DQaaS - Failure when Loading summary',
                               description=f'Rule Summary Loading Failed in DQaaS Project Space.<br>Table Name: {self.dq_report_table_name}',
                               details={'Message':'Rule Summary Loading Failed in DQaaS Project Space', "Table Name": self.dq_report_table_name})
                
            # Check and load to respective project space if the remote flag is enabled.   
            self.log.info(f"df_rules_lits :: {df_rules_list.columns}")
            self.log.info(f"df_rules_lits :: {df_rules_list.to_string()}")
            df_rules_list_remote = df_rules_list[df_rules_list["run_queries_on_remote"] == 'Y']
            if len(df_rules_list_remote) > 0:
                self.log.info('-------------------------------------------------------------------------')
                self.log.info('Loading Results into One Corp Data Dataset')
                self.log.info('-------------------------------------------------------------------------')
                try:
                    folder_path = df_rules_list.iloc[0]["wif_json_path"]
                    bq_client, credentials = self.dq_bigquery_client_dynamic(folder_path)
                    # od_bq_client, od_credentials = self.dq_bigquery_client(self.one_corp_auth_payload)
                    rpt_max_sequence = self.get_max_sequence(bq_client=bq_client,
                                                            bq_tablename=self.od_report_table_name,
                                                            sq_column='rule_prfl_num'
                                                            )

                    self.log.info(f"Max Sequence in OneCorp Space:{rpt_max_sequence}")
                    if rpt_max_sequence == -1:
                        raise RuntimeError("Error Occurred while Identifying the Max Sequence in Onecorp Space")
                    
                    od_df_rules_list = df_rules_list.assign(rule_prfl_num=[rpt_max_sequence +i for i in range(1, len(df_rules_list)+1)])
                    for col in decimal_cols_list:
                        od_df_rules_list[col] = od_df_rules_list[col].astype(str).map(decimal.Decimal)
                    for col in int_cols_list:
                        od_df_rules_list[col] = od_df_rules_list[col].astype('int64')
                    for col in str_cols_list:
                        od_df_rules_list[col] = od_df_rules_list[col].astype(str)
                    load_rules_data = od_df_rules_list.loc[:, od_df_rules_list.columns.isin(col_list)]
                    load_rules_data = load_rules_data.reset_index(drop=True)

                    self.log.info(f'Report Length:{len(load_rules_data)}, Table: {self.od_report_table_name} \n{load_rules_data}')

                    pandas_gbq.to_gbq(dataframe=load_rules_data,
                                    destination_table=self.od_report_table_name,
                                    if_exists='append',
                                    credentials=credentials,
                                    project_id=self.od_conn_project_id,
                                    )
                    self.log.info(f'Data Loaded Successfully to the table({self.od_report_table_name})')
                except Exception as e:
                    self.log.error(f'Error Occurred While Loading Data into One Corp Data Dataset. Error Info: {e}')     
                    self.opsgenie_alert(priority="P1",
                                message='DQ-2.0 - Failure when Loading summary',
                                description=f'Rule Summary Loading Failed in One Corp Project Space.<br>Table Name: {self.od_report_table_name}',
                                details={'Message':'Rule Summary Loading Failed in One Corp Project Space', "Table Name": self.od_report_table_name})

        except Exception as e:
            self.log.error( f'Error Occurred in loading Data. Error:{e}')

        
    # def get_rule_metrics_details(self, rule_run_dt, metadata_condition:str):
    def get_rule_metrics_details(self, rule_run_dt):
        try:
            
            # summary_query = f"""
            #     select  a.*,c.*, col_vld_cnt, col_invld_cnt, col_vld_pct, dq_ind, rule_run_dt
            #     from    `{self.dq_mtd_table_name}` a,
            #             `{self.dq_report_table_name}` b,
            #             vz-it-np-izcv-dev-idmcdo-0.dga_dq_tbls.dqaas_product_features_meta c
            #     where a.profile_id = b.rule_id and a.product_name=c.product_name
            #     and upper(a.is_active_flg) = 'Y'
            #     --{metadata_condition}
            #     and b.rule_run_dt = '{rule_run_dt}'
            #     order by a.profile_id ;
            # """
            summary_query = f"""select  a.*,c.*, col_vld_cnt, col_invld_cnt, col_vld_pct,dq_status, prfl_run_dt
                from    `vz-it-np-izcv-dev-idmcdo-0.dga_dq_tbls.dqaas_meta` a,
                        `vz-it-np-izcv-dev-idmcdo-0.dga_dq_tbls.dqaas_column_report` b,
                        vz-it-np-izcv-dev-idmcdo-0.dga_dq_tbls.dqaas_taxonomy_meta c
                where a.profile_id = b.profile_id and a.product_name=c.product_name
                and upper(a.active_flag) = 'Y'
                and b.prfl_run_dt = '{rule_run_dt}'
                order by a.profile_id ;"""
            
            self.log.info(f"Metrics Query: {summary_query}")
            dq_bq_client, _ = self.dq_bigquery_client(self.dq_auth_payload)
            return dq_bq_client.query(summary_query).to_dataframe()
        except Exception as err:
            raise RuntimeError(f"Error Occurred while Identifying the Rule Metrics. Error: {err}")

    def profile_engine(self, df_rules_list:pd.DataFrame, incr_dt_dict:dict, run_type:str, schd_type:str, src_tablename:str = None):
        exec_status, exec_msg = None, None
        try:
            ##  DQ Space BigQuery Client
            dq_bq_client, _ = self.dq_bigquery_client(self.dq_auth_payload)
            
            ##  Get Date Details for Profiling 
            date_interval = self.get_date_details(dq_bq_client=dq_bq_client,
                                                  incr_dt_dict=incr_dt_dict)
            self.log.info(f'Data Interval for Queries: {date_interval}')
            
            ## Values for replacing the placeholders in SQL Query
            val_to_replace_str = {
                "$start_dt": date_interval['START_DATE'],
                "$end_dt": date_interval['END_DATE'],
                "$start_yr_mnth": date_interval['START_YEAR_MONTH'],
                "$end_yr_mnth": date_interval['END_YEAR_MONTH'],
                "$incr_col1": self.config['sql_rule_profile']["incr_col1"],
                "$incr_col2": self.config['sql_rule_profile']["incr_col2"],
                "$incr_col3": self.config['sql_rule_profile']["incr_col3"],
                "$incr_col4": self.config['sql_rule_profile']["incr_col4"],
                "$incr_col5": self.config['sql_rule_profile']["incr_col5"],
                "$incr_col6": self.config['sql_rule_profile']["incr_col6"],
            }
            self.log.info(f'Values to replace in SQL: {val_to_replace_str}')
            
            ## Additional Details Like Run Type, Schedule Type, Execution Date and Business Date For Profiling
            rules_execution_process_details: dict = {
                "run_type": run_type,
                "schd_type": schd_type,
                "rules_execution_date": pd.Timestamp(datetime.now() - timedelta(days=self.n_days_interval)),
                "business_date": date_interval['START_DATE_SRC_COL_DT_VAL'] if run_type == 'DR' else date_interval['END_DATE_SRC_COL_DT_VAL']
            }
            self.log.info(f"Rules Execution Process details :{rules_execution_process_details}")
            
            ##  Get Where Clause condition details for Metadata Query. Cannot Be Null
            # metadata_where_condition: str = self.get_run_process_mtd_condition(run_type=run_type,
            #                                                               schd_type=schd_type)
            
            # if metadata_where_condition is None:
            #     raise RuntimeError("Run Type or Schedule Type not Found")
            
            if src_tablename is not None and run_type in ("AR", "RR"):
                metadata_where_condition += f" and upper(table_name) = upper('{src_tablename}') "
                
            ## Get Metadata Details for Rules Execution
            # df_rules_list = self.get_rule_metadata_details(dq_bq_client=dq_bq_client,
            #                                                metadata_condition=metadata_where_condition)
            
            self.log.info(f"Rules Length: {len(df_rules_list)}")
            #df_rules_list.to_csv("/apps/opt/application/smartdq/DQaaS2.0/dqaas/scripts/out.csv")
            if len(df_rules_list) == 0:
                raise RuntimeError("No Rules found for profiling")
            data_src = df_rules_list.loc[0, 'DATA_SRC']
            # df_rules_list = df_rules_list.head(10)
            rules_name = df_rules_list['RULE_NAME'].to_list()
            self.log.info(f"rules_name: {rules_name}")
            self.log.info(f"data_src : {data_src}")
            ##  Run Rule Profile Engine based on data source
            if data_src in config.RP_AGG_RULES_APPL_DATA_SRC:
                self.log.info(f"Inside GCP Rule Engine")
                df_rules_list, error_list = self.run_gcp_rule_profile_engine(df_rules_list=df_rules_list,
                                                                     val_to_replace=val_to_replace_str)
            
            if data_src in config.RP_NON_AGG_RULES_APPL_DATA_SRC:
                self.log.info(f"Inside TD Rule Engine")
                df_rules_list, error_list = self.run_td_rule_profile_engine(df_rules_list, rules_name)
                #df_rules_list.to_csv("/apps/opt/application/smartdq/DQaaS2.0/dqaas/scripts/out.csv")
            ##  Run Rule Profile Engine
            # df_rules_list, error_list = self.run_rule_profile_engine(df_rules_list=df_rules_list,
            #                                                          val_to_replace=val_to_replace_str)
               
            ##  Loading Results to BQ tables in DQ and OneCorp Space  
            df_rules_list.columns = df_rules_list.columns.str.lower()  
            self.load_result_to_bq_tables(df_rules_list,
                                          rules_execution_process_details)
            
            ##  Get Rules Metric Report for Send Summary mail and Run Invalid Rec SQls
            df_rules_result = self.get_rule_metrics_details(rule_run_dt=rules_execution_process_details["rules_execution_date"])
                                                            # metadata_condition=metadata_where_condition)
            
            self.log.info(f"Rule Summary Length: {len(df_rules_result)}")
            if len(df_rules_result) == 0:
                raise Exception(" No Results found for Summary Mail and Run Invalid SQLs.")
            df_rules_result_tbl_level = df_rules_result[df_rules_result['email_type']== 'TABLE']
            df_rules_result_sub_dmn_level = df_rules_result[df_rules_result['email_type']== 'SUB_DOMAIN']
            ##  Send Summary Report Table Level
            if len(df_rules_result_tbl_level) > 0:
                self.send_summary_table_level_mail(df_mail_summary=df_rules_result_tbl_level,
                                       error_rules_list=error_list,
                                       rule_run_dt=rules_execution_process_details["rules_execution_date"],
                                       schd_type = schd_type)

            ##  Send Summary Report Sub Domain Level
            if len(df_rules_result_sub_dmn_level) > 0:
                sub_domain_list = df_rules_result_sub_dmn_level['data_sub_dmn'].unique().tolist()
                for domain in sub_domain_list:    
                    self.send_summary_sub_dmn_level_mail(
                        sub_domain=domain,
                        rules_data=df_rules_result_sub_dmn_level,
                        df_rules_error_list=error_list,
                        critical_flag=''
                    )

            if src_tablename is not None and run_type in ("RR"):
                self.del_invalid_records(tablename=src_tablename,
                                         start_dt=date_interval['START_DATE'],
                                         end_dt=date_interval['END_DATE'],
                                         schd_type=schd_type)

            ##  Invalid Rec SQLs Execution Block
            self.execute_invalid_sqls(df_invalid_rec=df_rules_result,
                                      rule_run_dt=rules_execution_process_details["rules_execution_date"],
                                      date_interval=date_interval,
                                      val_to_replace=val_to_replace_str)   

            exec_status, exec_msg = "SUCCESS", f"{self.run_process_details} Completed"
            self.log.info(exec_msg)
        except RuntimeError as err:
            exec_status, exec_msg = "ERROR", f"Run Time Error. Error:{err}"
            self.log.error(exec_msg)
        except ValueError as err:
            exec_status, exec_msg = "ERROR", f"Value Error. Error:{err}"
            self.log.error(exec_msg)
        except HTTPError as err:
            exec_status, exec_msg = "ERROR", f"HTTP Error. Error:{err}"
            self.log.error(exec_msg)
        except Exception as err:
            exec_status, exec_msg = "ERROR", f"Exception Occurred in Main Block. Error:{err}"
            self.log.error(exec_msg)

        #if schd_type == "DAILY"  :
        #        mail_subject_msg = f"DQ-2.0 Rule Profiling completed for the daily run on ({self.current_date})"
        #elif schd_type == "MONTHLY":
        #    mail_subject_msg = f"DQ-2.0 Rule Profiling completed for the monthly run on ({self.current_date})"
        #else:
        #    mail_subject_msg = f"DQ-2.0 Rule Profiling completed for the adhoc run on ({self.current_date})"
        #    
        ## MJ:
        #self.email.send_common_message(email_template_filepath=self.email_template,
        #                               #mail_subject=f"1CorpData Rule Profiling Completed",
        #                               mail_subject = mail_subject_msg,
        #                               message="DQ-2.0 rule Profiling Initiation completed",
        #                               receipents_email_id=self.summary_alert_email_group)
        
        return exec_status, exec_msg
    
    def del_invalid_records(self, tablename:str, start_dt:str, end_dt: str, schd_type: str):
        try:
            del_query = f"""
                delete
                from {self.od_invalid_table_name}
                where incrm_dt_val is not null
                and upper(db_name) = upper('{tablename}')
                and incrm_dt_val = '{start_dt}';
            """
            ##  and date(dq_rule_run_dt) between '{start_dt}' and '{end_dt}' 
            od_bq_client, _ = self.dq_bigquery_client(auth=self.one_corp_auth_payload)
            del_rec_count = od_bq_client.query(del_query)
            del_rec_count.result()
            self.log.info(f"Invalid records del count : {del_rec_count.num_dml_affected_rows}")
        except Exception as err:
            self.log.info(f"Error occurred while deleting the invalid records for rerun. Error: {err}")

    # Method to Initiate Daily Process 
    def daily_run_process(self,df_rules_list):
        try:
            self.log.info('-------------------------------------------------------------------------')
            self.log.info('Daily Run Process - Initiated')
            self.log.info('-------------------------------------------------------------------------')
            
            incr_dt_dict_val = {'start_date': self.config['sql_rule_profile']['start_date'],
                                'end_date': self.config['sql_rule_profile']['end_date'],
                                'start_year_month': self.config['sql_rule_profile']['start_year_month'],
                                'end_year_month': self.config['sql_rule_profile']['end_year_month'],
                                'invld_dt': self.config['sql_rule_profile']['invld_dt']
                                }
            
            exec_status, exec_msg = self.profile_engine(df_rules_list,incr_dt_dict=incr_dt_dict_val,
                                                        run_type="DR",
                                                        schd_type="DAILY")

            self.log.info('-------------------------------------------------------------------------')
            self.log.info(f"Daily Run. Execution Status:{exec_status}, Message:{exec_msg}")
            self.log.info('-------------------------------------------------------------------------')
            return exec_status, exec_msg
        except Exception as err:
            exec_msg = f"Error Occurred in Daily Run Process Main block. Error: {err}"
            self.log.error(exec_msg)
            return "ERROR", exec_msg
        

    # Method to Initiate Monthly Process 
    def monthly_run_process(self,df_rules_list):
        try:
            self.log.info('-------------------------------------------------------------------------')
            self.log.info('Monthly Run Process - Initiated')
            self.log.info('-------------------------------------------------------------------------')
            incr_dt_dict_val = {'start_date': f"date_trunc(date_sub('{self.monthly_process_date}', interval 1 month), month)",
                                'end_date': f"last_day(date_sub('{self.monthly_process_date}', interval 1 month), month)",
                                'start_year_month': f"date_trunc(date_sub('{self.monthly_process_date}', interval 1 month), month)",
                                'end_year_month': f"last_day(date_sub('{self.monthly_process_date}', interval 1 month), month)",
                                'invld_dt': self.config['sql_rule_profile']['invld_dt']
                                }

            exec_status, exec_msg = self.profile_engine(df_rules_list,incr_dt_dict=incr_dt_dict_val,
                                                        run_type="MR",
                                                        schd_type="MONTHLY")
            
            self.log.info('-------------------------------------------------------------------------')
            self.log.info(f"Monthly Run. Execution Status:{exec_status}, Message:{exec_msg}")
            self.log.info('-------------------------------------------------------------------------')
            return exec_status, exec_msg
        except Exception as err:
            exec_msg = f"Error Occurred in Monthly Run Process Main block. Error: {err}"
            self.log.error(exec_msg)
            return "ERROR", exec_msg

    ##  Adhoc and Rerun Execution Process - Called while receiving the Trigger Files 
    def adhoc_rerun_process(self, tablename:str, start_dt:str, end_dt: str, schd_type: str, trigger_file: str):
        try:
            self.log.info('-------------------------------------------------------------------------')
            self.log.info('Adhoc / Rerun Process - Initiated')
            self.log.info('-------------------------------------------------------------------------')
            self.log.info(f"Arrived Inputs for Initiating the Process. tablename:{tablename}, start date:{start_dt}, end date: {end_dt}, schedule type: {schd_type}, Trigger File: {trigger_file}")
            
            run_type = ""
            if schd_type == "ADHOC":
                run_type = "AR"
            elif schd_type in ("DAILY", "MONTHLY"):
                run_type = "RR"
            
            if len(run_type) == 0:
                raise ValueError("Rerun or Adhoc Process Not identified")
            else:
                incr_dt_dict_val: dict = {'start_date': f"'{start_dt}'",
                                          'end_date': f"'{end_dt}'",
                                          'start_year_month': f"'{start_dt}'",
                                          'end_year_month': f"'{end_dt}'",
                                          'invld_dt': self.config['sql_rule_profile']['invld_dt']
                                          }

                exec_status, exec_msg = self.profile_engine(incr_dt_dict=incr_dt_dict_val,
                                                            run_type=run_type,
                                                            schd_type=schd_type.upper(),
                                                            src_tablename=tablename)
                
                
                self.log.info('-------------------------------------------------------------------------')
                self.log.info(f"Adhoc/Rerun Process. Execution Status:{exec_status}, Message:{exec_msg}")
                self.log.info('-------------------------------------------------------------------------')
            
                return exec_status, exec_msg
       
        except ValueError as verr:
            exec_msg = f"Value Error. Error:{verr}"
            self.log.error(exec_msg)
            return "ERROR", exec_msg
            
        except Exception as err:
            exec_msg = f"Error Occurred in Adhoc / Rerun Process Main block. Error: {err}"
            self.log.error(exec_msg)
            return "ERROR", exec_msg
        
        
    ## Main Block for Running Daily and Monthly Process
    def run_regular_process(self,df_rules_list):
        exec_status, exec_msg, sched_type = None, None, 'DAILY'
        try:
            #mail_subject_msg = f"DQ-2.0 Rule Profiling started for the daily run on ({self.current_date})"
            #if self.monthly_process_yn == "MONTHLY":
            #    mail_subject_msg = f"DQ-2.0 Rule Profiling started for the monthly run on ({self.current_date})"
#
            #self.email.send_common_message(email_template_filepath=self.email_template,
            #                               mail_subject = mail_subject_msg,
            #                               message="DQ-2.0 rule profiling have started",
            #                               receipents_email_id=self.summary_alert_email_group)
            
            exec_status, exec_msg = self.daily_run_process(df_rules_list)

            if exec_status == "ERROR":
                return "ERROR", f"Error in {sched_type} Run Process: {exec_msg}"
            
            #Monthly Process Starts
            if self.monthly_process_yn == "Y":
                sched_type = 'MONTHLY'
                exec_status, exec_msg = self.monthly_run_process(df_rules_list)
                if exec_status == "ERROR":
                    return "ERROR", f"Error in {sched_type} Run Process: {exec_msg}"
            
            #Send Profile Completed Alert
            #mail_subject_msg = f"DQ-2.0 Rule Profiling ended for the daily run on ({self.current_date})"
            #if self.monthly_process_yn == "MONTHLY":
            #    mail_subject_msg = f"DQ-2.0 Rule Profiling ended for the monthly run on ({self.current_date})"
#
            #self.email.send_common_message(email_template_filepath=self.email_template,
            #                               mail_subject = mail_subject_msg,
            #                               message="DQ-2.0 rule profiling have ended",
            #                               receipents_email_id=self.summary_alert_email_group)
            
            return "SUCCESS", "Rule Profiling Completed"    
        except Exception as err:
            exec_msg = f"Error Occurred in Regular Execution Block: {err}"
            self.log.error(exec_msg)
            return "ERROR", exec_msg

## Method to initiate the Execution Process for Daily and Monthly
def daily_monthly_normal_execution_process():

    config = get_config()
    if config is not None:
        ruleprofile = RuleProfile()
        exec_status, exec_msg = ruleprofile.run_regular_process()
        del ruleprofile
        
        
        
"""
def daily_monthly_normal_execution_process():
    
    import config_data
    config = config_data.get_config()
    
    if config is not None:
        ruleprofile = RuleProfile(config_data=config)
        ruleprofile.run_regular_process()
        
        del ruleprofile
""" 
    
if __name__ == "__main__":
    daily_monthly_normal_execution_process()
        
=================================================================================================================================================
-------------------------------------------------------------------------------------------------------------------------
import re
import pandas as pd
import numpy as np
import os
import logging
import sys
from datetime import datetime, timedelta
from functools import reduce
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)


## Importing User Defined Modules
import config_params as config
from utils.send_email import SendEmail
from scripts.custom_common_handlers import CommonUtils, set_logger, get_args_parser



class CustomeMetrics:
    def __init__(self, data_src = None) -> None:
        self.__set_default_values()
        self.logger: logging = set_logger(
            logger_path=config.LOGS_DIR,
            log_filename="CustomMetricsLogs",
            process_name='CRM',
            # no_date_yn="Y",
        )
        
        self.utils = CommonUtils(config, self.logger)
        self.df_email_distro= pd.DataFrame()
        
    ## Default Values
    def __set_default_values(self):
        self.rule_col_rename_list = {
            'profile_date': 'data_dt',
            'pageName': 'value',
            'pageflow': 'value' ,
            'value': 'count_curr',
            'insert_dt': 'rule_run_ts',
        }
        
        self.final_col_rename_list = {
            'profile_date': 'data_dt',
            'value': 'count_curr',
            'dimension': 'grouped_columns',
            'rule_run_ts': 'prfl_run_ts',
        }
        
        self.custom_profile_report_columns = {
            "NUMERIC": [
                'avg_count_prev', 'variance_value', 'std_dev_value',
                'pct_change', 'count_curr','sigma_2_value','dq_score',
                'consistency_score',
            ],
            "INT64": ['profile_id', 'weekday', 'rpt_seq_num', ],
            "STRING": [
                'feature_name', 'grouped_columns', 'sigma_value', 'rpt_ref_key', 'data_dt',
                'profile_type', 'table_name', 'dq_pillar', 'rule_name', 'dq_status'
            ],
            "TIMESTAMP": ['prfl_run_ts'],
        }
        
        self.email_rename_cols = {
            'prfl_run_ts': 'Run Profile Date',
            'data_lob': 'LOB',
            # 'data_bus_elem': 'Domain Name',
            'database_name': 'DB Name',
            'table_name': 'Table Name',
            'feature_name': 'Feature Name',
            'grouped_columns': 'Dimensions',
            'dq_pillar': 'DQ Category',
            'weekday': 'Weekday',
            'count_curr': 'Value',
            'avg_count_prev': 'Avg Value',
            'pct_change': 'Percentage Change',
            'variance_value': 'Variance',
            'std_dev_value': "Std Deviation",
            'sigma_value': 'Sigma Value',
            'consistency_score': 'Consistency Score',
            'dq_score': 'DQ Score',
            'dq_status': 'Status',
        }
        
        ## Same order used in the Email
        self.summary_cols = [
            'prfl_run_ts', 'data_lob',  'database_name', 'table_name',
            'feature_name', 'grouped_columns', 'dq_pillar',
            'weekday', 'count_curr', 'avg_count_prev', 'pct_change',
            'variance_value', 'std_dev_value', 'sigma_value',
             'dq_score', 'dq_status'
        ]
        
        ## Columns used to round off the float values in Email, 
        ## Applicable Columns :- Should be available in self.summary_cols
        self.email_float_cols = [
           'count_curr', 'avg_count_prev', 'pct_change',
            'variance_value', 'std_dev_value',
             'dq_score',
        ]
    
    ## Historical Queries 
    @staticmethod
    def historical_queries(comparison_type: str='WEEKDAYS'):
        query = {
            
            "WEEKDAYS": r"""
            with top_records as 
            (
            select *,row_number() over (partition by profile_id,feature_name,grouped_columns order by data_dt desc) as row_num 
            from $rpt_table join 
            unnest(GENERATE_DATE_ARRAY(date_sub($RUN_DT,INTERVAL 3 MONTH),$RUN_DT, INTERVAL 1 day)) as interval_Date
            on data_dt = interval_Date
            where profile_id IN ($prfl_id_list)
            and data_dt != $RUN_DT
            order by profile_id,feature_name, grouped_columns,data_dt desc
            ) 
            select profile_id,feature_name,grouped_columns,WEEKDAY,
            round(sum(ifnull(count_curr, 0)), 2) as sum_count_prev,
            round(avg(ifnull(count_curr, 0)), 2) as avg_count_prev,
            round(var_pop(ifnull(count_curr, 0)), 2) as variance_value,
            round(stddev_pop(ifnull(count_curr, 0)), 2) as std_dev_value,
            round(stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as sigma_2_value, 
            round(avg(ifnull(count_curr, 0)) - stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as min_thresh_value, 
            round(avg(ifnull(count_curr, 0)) + stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as max_thresh_value
            from top_records 
            group by profile_id,feature_name,grouped_columns,WEEKDAY
            order by profile_id,feature_name,grouped_columns,WEEKDAY;
            """,
            
            "DTRAN_MONTHLY": r"""
            with top_records as 
            (select *,row_number() over (partition by profile_id,feature_name,grouped_columns order by data_dt desc) as row_num 
            from $rpt_table join 
            unnest(GENERATE_DATE_ARRAY(date_trunc(date_sub($RUN_DT,INTERVAL 3 MONTH), MONTH),$RUN_DT, INTERVAL 1 MONTH)) as interval_Date
            on data_dt = interval_Date
            where profile_id IN ($prfl_id_list)
            and data_dt != date_trunc(date($RUN_DT), MONTH)
            order by profile_id,feature_name, grouped_columns,data_dt desc) 
            select profile_id,feature_name,grouped_columns,
            round(sum(ifnull(count_curr, 0)), 2) as sum_count_prev,
            round(avg(ifnull(count_curr, 0)), 2) as avg_count_prev,
            round(var_pop(ifnull(count_curr, 0)), 2) as variance_value,
            round(stddev_pop(ifnull(count_curr, 0)), 2) as std_dev_value,
            round(stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as sigma_2_value, 
            round(avg(ifnull(count_curr, 0)) - stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as min_thresh_value, 
            round(avg(ifnull(count_curr, 0)) + stddev_pop(ifnull(count_curr, 0)) * 2.0, 2) as max_thresh_value
            from top_records 
            group by profile_id,feature_name,grouped_columns
            order by profile_id,feature_name,grouped_columns;
            """,
        }
        if comparison_type is None:
            return query['WEEKDAYS']
        if comparison_type.upper() in query:
            return query[comparison_type.upper()]
        return query['WEEKDAYS']
              
    ## Historical Results 
    def get_historical_details(self, prfl_id_list: list, run_date: str, comparison_type: str='WEEKDAYS'):
        hist_record_query = self.historical_queries(comparison_type)
        
        placeholders = {
            "$rpt_table": config.dqaas_profile_rpt,
            "$prfl_id_list": prfl_id_list,
            "$RUN_DT": run_date,
        }
        # hist_record_query = hist_record_query.format(placeholders)
        hist_record_query = reduce(lambda sql, replace_str: sql.replace(*replace_str), [hist_record_query, *list(placeholders.items())])
        df_tbl_hist_rec = self.utils.run_bq_sql(
            bq_auth=config.dq_gcp_auth_payload,
            select_query=hist_record_query
        )
        df_tbl_hist_rec = df_tbl_hist_rec.rename(columns={col: str(col).lower() for col in df_tbl_hist_rec.columns.tolist()})
        self.logger.info(f'Length of the historical groupby list : {len(df_tbl_hist_rec)}')
        self.logger.info(f'\n{df_tbl_hist_rec.columns}\n{df_tbl_hist_rec.head()}')

        if len(df_tbl_hist_rec) == 0:
            self.logger.warning("Historical Data Not found for Identifying the variance")
        
        return df_tbl_hist_rec
        
    ## Comparing Hist and Current Records
    def compare_historical_latest_dimensions(self, df_tbl_hist_rec, df_tbl_latest_rec: pd.DataFrame, comparison_type="WEEKDAYS", isHourly="N"):
        
        ## Historical Records
        self.logger.info('------------------------------------------------------------------')
        df_tbl_hist_rec = df_tbl_hist_rec.rename(columns={col: str(col).lower() for col in df_tbl_hist_rec.columns.tolist()})
        df_tbl_hist_rec['grouped_columns'] = df_tbl_hist_rec['grouped_columns'].astype(str).replace('<NA>',np.nan).replace('nan',np.nan).replace('None',np.nan)
        self.logger.info(f'Length of the historical Records : {len(df_tbl_hist_rec)}')
        self.logger.info(f'\n{df_tbl_hist_rec.columns}\n{df_tbl_hist_rec.head()}')
        
        ## Latest Records
        self.logger.info('------------------------------------------------------------------')
        df_tbl_latest_rec = df_tbl_latest_rec.rename(columns={col: str(col).lower() for col in df_tbl_latest_rec.columns.tolist()})
        df_tbl_latest_rec['grouped_columns'] = df_tbl_latest_rec['grouped_columns'].astype(str).replace('<NA>',np.nan).replace('nan',np.nan)
        self.logger.info(f'Length of the latest Records : {len(df_tbl_latest_rec)}')
        self.logger.info(f'\n{df_tbl_latest_rec.columns}\n{df_tbl_latest_rec.head()}')

        ## Merging the Historical and Latest Records
        self.logger.info('------------------------------------------------------------------')
        
        join_list = ['profile_id','feature_name','grouped_columns','weekday']
        
        if comparison_type == 'DTRAN_MONTHLY':
            join_list.remove('weekday')
                             
        df_merge_rec= pd.merge(
            df_tbl_hist_rec,
            df_tbl_latest_rec,
            on=join_list,
            how='right'
        )
        
        self.logger.info(f'Length of the Merged Records : {len(df_merge_rec)}')
        self.logger.info(f'\n{df_merge_rec.columns}\n{df_merge_rec.head()}')
        self.logger.info('------------------------------------------------------------------')
        return df_merge_rec
         
    ## Email distros
    def email_distro(self, sub_domain:str): 
        try:
            self.df_email_distro: pd.DataFrame = self.utils.get_email_distros_from_table(data_sub_dmn_list=[sub_domain])
            self.logger.info(f"Email Dataframe: \n{self.df_email_distro}")
            
            receipents_mail_group: list = self.utils.get_mail_distro(
                df_val=self.df_email_distro,
                persona='PERSONA_2',
                sub_dmn=sub_domain
            )
            self.logger.info(f"Email Group: {receipents_mail_group}")
            # receipents_mail_group.append(config.dqaas_default_email_distro)
            receipents_mail_group += config.dqaas_default_email_distro
            
            self.logger.info(f"Final Email Receipents: {receipents_mail_group}")  
            return receipents_mail_group
        except Exception as err:
            self.logger.error(f"Error in retrieving Email Distro. Assigning default email group. Error: {err}")
        return config.dqaas_default_email_distro
           
    ## Summary Report Generation and Sending part
    def send_email_report(self, reference_key: str, sub_domain: str):
        try:
            
            report_query = f"""
                select rpt.*, 
                mtd.data_lob ,mtd.database_name ,mtd.table_name ,mtd.dq_pillar 
                from {config.dqaas_auto_prfl_mtd} mtd 
                inner join 
                (select * from {config.dqaas_profile_rpt} 
                where rpt_ref_key = '{reference_key}' ) rpt
                on mtd.profile_id = rpt.profile_id
                and upper(mtd.profile_type) = 'RULE_CUSTOM'
            """
            self.logger.info(f"Mail Query : {report_query}")

            report_df = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=report_query
            )
            
            if len(report_df) == 0:
                raise Exception("No Records found for Email Summary")

            
            receipents_mail_group = self.email_distro(sub_domain=sub_domain)
            
            # report_df = report_df[['prfl_run_ts', 'feature_name',
            #     'grouped_columns', 'weekday', 'count_curr', 'avg_count_prev', 'pct_change',
            #     'variance_value', 'std_dev_value', 'sigma_value']]

            # col = [
            #     'count_curr', 'avg_count_prev', 'pct_change',
            #     'variance_value', 'std_dev_value'
            # ]
            
            report_df = report_df[self.summary_cols]
            col = self.email_float_cols

            for c in col:
                report_df[c] = report_df[c].fillna(np.nan).astype('float64').map(self.utils.round_off)
            
            report_df['prfl_run_ts'] = pd.to_datetime(report_df['prfl_run_ts']).dt.date
            report_df = report_df.rename(columns=self.email_rename_cols).reset_index(drop=True)
            
            style_format_dict = {
                'Weekday': '{:,.0f}',
                'Value': '{:,.2f}',
                'Avg Value': '{:,.2f}',
                'Percentage Change': '{:,.2f}',
                'Variance': '{:,.2f}',
                'Std Deviation': '{:,.2f}'
            }

            style_details_dict = {
                'outlier': '{background-color:#BF3131; color:#FFF; font-weight:bold;}'
            }
            validate_column = 'Sigma Value'
            

            email = SendEmail(
                smtp=config.SMTP_SERVER_NAME,
                mail_from=config.SENDER_EMAIL_ID,
                loggerObj=self.logger
            )
            current_date = datetime.strftime(datetime.now() - timedelta(days=1), '%Y-%m-%d')
            email.send_summary_with_highlights(
                email_template_filepath=os.path.join(config.TEMPLATE_DIR, "dq_summary_report_template_with_highlights.html"),
                mail_subject=f"Custom profile summary for {sub_domain} - {current_date}",
                message='',
                df_val=report_df,
                receipents_email_id=receipents_mail_group,
                style_format=style_format_dict,
                style_details_dict=style_details_dict,
                validate_column=validate_column
            )
        except Exception as err:
            self.logger.error(f"Error in Report Summary Email Block. Error:{err}")
    
    ## Loading Results to Report Table
    def load_to_report_results(self, df_report_result: pd.DataFrame, reference_key: str):
        try:
            ## BigQuery Client Connection
            dbclient, db_creds = self.utils.bigquery_client(
                auth=config.dq_gcp_auth_payload
            )
        
            df_report_result["rpt_ref_key"] = reference_key
            
            df_report_result = df_report_result.rename(columns={col: str(col).lower() for col in df_report_result.columns.tolist()})
            
            ## Loading Table Level Report
            self.utils.load_result_to_bq_report_table(
                dq_bq_client=dbclient,
                dq_credentials=db_creds,
                dq_report_table_name=config.dqaas_profile_rpt,
                df_load_data=df_report_result,
                seq_name='rpt_seq_num',
                column_details=self.custom_profile_report_columns
            )
            
        except Exception as err:
            self.logger.error(f" {err}")
    
    ## Consistency Score, DQ Score, Variation Percentage
    @staticmethod
    def calculate_percentage_change(df):
        print("calculate_percentage_change")
        df['count_curr'] = df['count_curr'].fillna(0).astype(float)
        df['avg_count_prev'] = df['avg_count_prev'].astype(float)
        df['threshold_limit'] = df['threshold_limit'].fillna(config.CUST_MIN_THRESHOLD).astype(float)
        df['max_threshold_limit'] = df['max_threshold_limit'].fillna(config.CUST_MAX_THRESHOLD).astype(float)
        # df['pct_change'] = (((df['count_curr'] - df['avg_count_prev']) / df['count_curr']) * 100).round(2)
        df['pct_change'] = np.where( 
            df['count_curr'].fillna(0) == 0, 0,
            (((df['count_curr'] - df['avg_count_prev']) / df['count_curr']) * 100).round(2)
        )
        # df['consistency_score'] = ((df['count_curr'] / df['avg_count_prev']) * 100).round(2)
        df['consistency_score'] = np.where( 
            df['avg_count_prev'].fillna(0).astype(float) == 0, 0,
            ((df['count_curr'] / df['avg_count_prev']) * 100).round(2)
        )
        df['dq_score'] = np.where(
            (df['threshold_limit'] <= df['consistency_score']) & (df['consistency_score'] <= df['max_threshold_limit']),
            100 , 0
        )

    ## 3 Sigma Values
    @staticmethod
    def calculate_sigma_value( df):
        print("calculate_sigma_value")
        df['std_dev_value'] = df['std_dev_value'].astype(float)
        df['count_curr'] = df['count_curr'].fillna(0).astype(float)
        df['avg_count_prev'] = df['avg_count_prev'].astype(float)    
        conditions_null_dim = [
            (df['count_curr'] >= df['avg_count_prev'].fillna(0).astype(float) - 1 * df['std_dev_value'].fillna(0).astype(float)) & (df['count_curr'] <= df['avg_count_prev'].fillna(0).astype(float) + 1 * df['std_dev_value'].fillna(0).astype(float)),
            (df['count_curr'] >= df['avg_count_prev'].fillna(0).astype(float) - 2 * df['std_dev_value'].fillna(0).astype(float)) & (df['count_curr'] <= df['avg_count_prev'].fillna(0).astype(float) + 2 * df['std_dev_value'].fillna(0).astype(float)),
            (df['count_curr'] >= df['avg_count_prev'].fillna(0).astype(float) - 3 * df['std_dev_value'].fillna(0).astype(float)) & (df['count_curr'] <= df['avg_count_prev'].fillna(0).astype(float) + 3 * df['std_dev_value'].fillna(0).astype(float)),
            pd.isna(df['grouped_columns'])
        ]
        choices = [1, 2, 3, 'outlier']
        df['sigma_value'] = np.select(conditions_null_dim, choices, default='outlier')
       
    ## DQ Status  
    @staticmethod
    def calculate_dq_status(df):
        print("calculate_dq_status")
        df['std_dev_value'] = df['std_dev_value'].astype(float)
        df['count_curr'] = df['count_curr'].fillna(0).astype(float)
        df['avg_count_prev'] = df['avg_count_prev'].astype(float)
        df['consistency_score'] = df['consistency_score'].fillna(0).astype(float)
        df['min_thresh_value'] = df['min_thresh_value'].fillna(0).astype(float)
        df['max_thresh_value'] = df['max_thresh_value'].fillna(0).astype(float)
        df['threshold_limit'] = df['threshold_limit'].fillna(config.CUST_MIN_THRESHOLD).astype(float)
        df['max_threshold_limit'] = df['max_threshold_limit'].fillna(config.CUST_MAX_THRESHOLD).astype(float)

        main_condition = (
            (df['count_curr'].fillna(0) != 0) & 
            (df['max_thresh_value'] >= df['count_curr']) &
            (df['min_thresh_value'] <= df['count_curr']) | 
            (
                (df['count_curr'] == df['avg_count_prev'].fillna(0).astype(float)) &
                (df['std_dev_value'].fillna(0).astype(float) == df['avg_count_prev'].fillna(0).astype(float))
            )
        )
        
        dq_status_condition = [
            main_condition & (( df['threshold_limit'] <= df['consistency_score'] ) & ( df['consistency_score'] <= df['max_threshold_limit'])), 
            
            ( main_condition &  (df['consistency_score'] < df['threshold_limit'] ) |
            ( (df['count_curr'] != 0 & (df['count_curr'] < df['min_thresh_value'])) &
            (df['consistency_score'] < df['threshold_limit'] ) | 
            (( df['threshold_limit']  <= df['consistency_score'] ) & ( df['consistency_score'] <= df['max_threshold_limit'])) ) ),
            
            (  main_condition & (df['consistency_score'] > df['max_threshold_limit']) |
            ( (df['count_curr'] != 0 & (df['count_curr'] > df['max_thresh_value'])) &
            (df['consistency_score'] > df['max_threshold_limit']) | (( df['threshold_limit']  <= df['consistency_score'] ) & ( df['consistency_score'] <= df['max_threshold_limit'])) ) )
        ]
        choices = ["PASS", "LOW", "HIGH"]
        df['dq_status'] = np.select(dq_status_condition, choices, default='NA')

    ## Not in Use
    def delete_current_null_records(self, prfl_id_list, business_date):
        try:
            delete_dup_record_query=f'''
                delete from {config.dqaas_profile_rpt}
                where date(rule_profile_dt)={business_date} 
                and profile_id in ({prfl_id_list}) 
                and weekday = extract(dayofweek from {business_date})  
                and avg_count_prev is null
                and pct_change is null 
                and variance_avg_count_prev is null 
                and std_dev_value is null 
                and sigma_value is null;
            '''
            
            num_recs_deleted = self.utils.run_bq_dml_sql(
                bq_auth=config.dq_gcp_auth_payload,
                dml_query=delete_dup_record_query
            )
            
            self.logger.info(f'Number of latest records deleted :  {num_recs_deleted}')
        except Exception as err:
            self.logger.error(f"Error in Deleting historical records. Error:{err}")
        
    ## Profile Engine 
    def profile_engine(self, df_rules: pd.DataFrame, param_to_replace: dict = None, comparison_type="WEEKDAYS"):
        
        ## Profiling Rules
        # df_result = pd.DataFrame()
        df_result = []
        for i in df_rules.index:
            self.logger.info('------------------------------------------------------------------')

            try:
                result = pd.DataFrame()
                
                profile_id = df_rules.loc[i, "profile_id"]
                rule_sql = df_rules.loc[i, "rule_sql"]
                data_src = df_rules.loc[i, "data_src"]
                
                # self.logger.info(f"Index: {i} : Rule ID:: {profile_id}, \nquery: {query}\n")
                self.logger.info(f"Index:: {i}, Rule ID:: {profile_id}")
                
                rule_sql = reduce(lambda sql, replace_str: sql.replace(*replace_str), [rule_sql, *list(param_to_replace.items())])
            
                result = self.utils.get_query_data(
                    data_src=data_src,
                    dbname=df_rules.loc[i, "database_name"],
                    select_query=rule_sql
                )
                
                result = result \
                .rename(columns={col: str(col).lower() for col in result.columns.tolist()}) \
                .rename(columns=self.rule_col_rename_list)
                
                
                if len(result) == 0:
                    self.logger.info('No Records Found in Rules')
                    # date_val = self.utils.run_bq_sql(
                    #     bq_auth=config.dq_gcp_auth_payload,
                    #     select_query=f'''select date({param_to_replace["RUN_DT"]}) as run_dt, extract(dayofweek from date({param_to_replace["RUN_DT"]})) as weekday;'''
                    # )
                    self.logger.info(f'Date Results :: \n{self.date_val}')
                    result = pd.DataFrame.from_records([{
                        "data_dt" : self.date_val.loc[0, "run_dt"],
                        "feature_name" : df_rules.loc[i, "feature_name"],
                        "dimension" : np.nan,
                        "count_curr" : 0,
                        "rule_run_ts" : datetime.now(),
                        "weekday" : self.date_val.loc[0, "weekday"],
                    }])

                    if comparison_type == 'DTRAN_MONTHLY':
                        result = result.drop(columns=["weekday"], errors='ignore', axis=1)
                    
                    # result["data_dt"] = date_val.loc[0, "run_dt"]
                    # result["feature_name"] = df_rules.loc[i, "feature_name"]
                    # result["dimension"] = np.nan
                    # result["count_curr"] = 0
                    # result["rule_run_ts"] = datetime.now()
                    # result["weekday"] = date_val.loc[0, "weekday"]
    
                result["profile_id"] = profile_id
                
                self.logger.info(f'\n{result}')
                # df_result = df_result.append(result)
                res_val = []
                res_val = result.to_dict("records")
                df_result.extend(res_val)
            except Exception as err:
                self.logger.error(f"Error While Executing Rules: Error{err}")
            
            self.logger.info('------------------------------------------------------------------')
            
        dfEndRes = pd.DataFrame(df_result)
        self.logger.info(f'Length of the Rules Result : {len(dfEndRes)}')
        self.logger.info(f'\n{dfEndRes.columns}\n{dfEndRes.head()}')
           
        if len(dfEndRes) == 0:
            raise ValueError("No Records Found in Custom Rule Profiling.")
        dfEndRes.to_csv("/apps/opt/application/dev_smartdq/dev/dqaas/dqaas/dfEndRes1.csv")
        # dfEndRes = dfEndRes.rename(columns=self.final_col_rename_list).reset_index(drop=True)
        dfEndRes = dfEndRes.rename(columns=self.final_col_rename_list).reset_index(drop=True)
        dfEndRes.to_csv("/apps/opt/application/dev_smartdq/dev/dqaas/dqaas/dfEndRes2.csv")
        return dfEndRes
    
    @staticmethod
    def int_df_to_list(df):
        int_list: list = [i for i in df['profile_id']]
        return list(set(int_list))
    
    ## Level 2 - Execution Layer : Run Rules, Get Hist Records, Compare the Results, and Find metrics
    def run_metrics_engine(self, df_mtd: pd.DataFrame, start_date, end_date, val_replace_params:dict = None):
        try:

            df_mtd = df_mtd.reset_index(drop=True)
            df_mtd["comparison_type"] = df_mtd["comparison_type"].fillna("WEEKDAYS")
            df_mtd["run_frequency"] = df_mtd["run_frequency"].fillna("N")
            comparisonType = df_mtd.loc[0,"comparison_type"]
            isHourly = df_mtd.loc[0, 'run_frequency']
            self.logger.info(f"Comparison:: {comparisonType}, Hourly:: {isHourly}")
            ## Creating Profile ID list for historical Load
            # rule_id_list: list = []
            # for i in df_mtd['prfl_id']:
            #     rule_id_list.append(i)
            
            print(df_mtd['profile_id'])
            prfl_id_list = [int(i) for i in df_mtd['profile_id']]
            prfl_id_list = list(set(prfl_id_list))
            prfl_id_list = f"{prfl_id_list}".replace('[', '').replace(']' ,'')
            self.logger.info(f'Profile ID List : {prfl_id_list}')
        
            ## For null results, weekday and date will be included
            self.date_val = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=f'''
                select date({val_replace_params["RUN_DT"]}) as run_dt,
                extract(dayofweek from date({val_replace_params["RUN_DT"]})) as weekday;
                '''
            )
            
            df_latest = self.profile_engine(
                df_rules=df_mtd,
                param_to_replace=val_replace_params,
            )
            # df_latest = pd.DataFrame(df_latest)
            # df_latest = df_latest.rename(columns=self.final_col_rename_list).reset_index(drop=True)
            df_latest["prfl_run_ts"] = datetime.now().strftime(config.DTM_FMT)
            if isHourly == 'Y':
                df_latest["hour"] = df_latest["grouped_columns"]
                
            # df_latest.to_csv("/apps/opt/application/dev_smartdq/dev/dqaas/dqaas/hour.csv")
            
            df_historical = self.get_historical_details(
                run_date=start_date,
                prfl_id_list=prfl_id_list,
                comparison_type=comparisonType
            )
            

            df_merged_dimensions = self.compare_historical_latest_dimensions(
                df_tbl_hist_rec=df_historical,
                df_tbl_latest_rec=df_latest,
                comparison_type=comparisonType,
                isHourly=isHourly,
            )
            
            # df_merged_dimensions.to_csv(os.path.join(os.path.dirname(__file__), "merged_results.txt"))
            mtdCols = ['profile_id', 'profile_type', 'table_name', 'dq_pillar', 'rule_name', 'threshold_limit', 'max_threshold_limit', 'comparison_type', 'run_frequency']
            df_final_result = pd.merge(
                df_merged_dimensions,
                df_mtd[mtdCols],
                on=['profile_id'],
                how='inner'
            )
            
            
            self.calculate_percentage_change(df_final_result)
            self.calculate_sigma_value(df_final_result)
            self.calculate_dq_status(df_final_result)
            
            # df_merged_dimensions.to_csv(os.path.join(os.path.dirname(__file__), "merged_results_final.txt"))

            self.logger.info('------------------------------------------------------------------')
            self.logger.info(f'Length of the End Results : {len(df_final_result)}')
            self.logger.info(f'\n{df_final_result.columns}\n{df_final_result.head()}')
            self.logger.info('------------------------------------------------------------------')
            # df_final_result.to_csv(os.path.join(os.path.dirname(__file__), "cm_res_3.txt"))
           
            # self.delete_historical_records(hist_date=hist_date, prfl_id_list=prfl_id_list)
            return df_final_result
                
        except Exception as err:
            self.logger.error(f"Error in Run Metrics Main Block. Error {err}")
    
    ## Level 1 - Metrics Execution initiation - entry point for Time based and Engine Initiation - Main Block
    def main_metrics_execution(self, df_mtd: pd.DataFrame, sub_domain: str, start_date: str, end_date: str):
        
        df_mtd = df_mtd.rename(columns={col: str(col).lower() for col in df_mtd.columns.tolist()})
        
        ## Reference Key -> For every new request one ID will be created
        reference_key = datetime.now().strftime('%Y%m%d%H%M%S%f')
        self.logger.info(f"\nRequest Reference:: {reference_key}\nSub Domain:: {sub_domain}")
  
        ## Placeholders for replacing the values in query during execution
        replace_params_for_rules = {"RUN_DT": start_date, "$START_DATE": start_date, "$END_DATE":end_date,}
        
        ## Metrics Initiation
        df_end_result = self.run_metrics_engine(
            df_mtd=df_mtd,
            start_date=start_date,
            end_date=end_date,
            val_replace_params=replace_params_for_rules
        )
        df_end_result.to_csv("/apps/opt/application/dev_smartdq/dev/dqaas/dqaas/df_end_result.csv")
        if len(df_end_result) == 0 :
            self.logger.error("No Records Found for Loading Custom Metrics")
            return 
        
        ## Loading Results to Report
        self.load_to_report_results(
            df_report_result=df_end_result,
            reference_key=reference_key
        )
        
        ## Send Email
        self.send_email_report(
            reference_key=reference_key,
            sub_domain=sub_domain
        )

    ## Metadata Retrival    
    def get_metadata(self, add_condition: str = None) -> pd.DataFrame:
            if add_condition in config.EMPTY_STR_LIST:
                add_condition = ""
            
            metadata_query = f"""
                SELECT * FROM {config.dqaas_profile_mtd}
                WHERE profile_type = 'RULE_CUSTOM'    
                and IS_ACTIVE_FLG = 'Y'    
                {add_condition}                     
                ORDER BY PROFILE_ID;
            """
            
            df_val = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=metadata_query
            )
            
            self.logger.info(f"Metadata Length: {len(df_val)}")
            
            if len(df_val) == 0:
                raise ValueError("No Records Found for Custom Metrics")
            
            df_val = df_val.rename(columns={col: str(col).lower() for col in df_val.columns.tolist()})
            return df_val


    def laod_historical_report(self):

        source_data = ''

        try:
            self.logger.info(f"Inside laod_historical_report")

            fetch_sql = f""" select distinct mtd.profile_id, mtd.profile_type, mtd.dq_pillar, mtd.src_tbl, mtd.meas_name, mtd.feature_name, meas_rule_sql from {config.dqaas_profile_mtd} mtd
            left join {config.dqaas_profile_rpt} rpt
            on mtd.profile_id = rpt.profile_id
            where rpt.profile_id is null
            and upper(mtd.profile_type) = 'CUSTOM_RULES'"""

            self.logger.info(f"Fetch SQL executing: {fetch_sql}")

            source_data = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=fetch_sql)

            self.logger.info(f"Fetched new onboarded tables")

        except Exception as err:
                self.logger.error(f"Error While Executing fetch_sql: Error{err}")

        try:
            for index, row in source_data.iterrows():
                profile_id = row[0]
                profile_type = row[1]
                dq_pillar = row[2]
                src_tbl = row[3]
                meas_name = row[4].replace(" ","_")
                meas_rule_sql = row[6]

                self.logger.info(f"Orginal meas_rule_sql : {meas_rule_sql}")

                modified_sql = meas_rule_sql.replace("= RUN_DT","between current_date() -1 and current_date() - 92")
                modified_sql = re.sub(r"(?i)null.* as.* dimension","'null' AS dimension", modified_sql)
                modified_sql = re.sub(r"(?i)^SELECT\s",f"""Select 999999 as rpt_seq_num, {profile_id} as profile_id, '{profile_type}' as profile_type, '{dq_pillar}' as dq_pillar, '{src_tbl}' as src_tbl, '{meas_name}' as meas_name, """, modified_sql)
                modified_sql = re.sub(r"(?i)date\s*\(\s*current_timestamp\s*\)","current_timestamp", modified_sql)
                modified_sql = re.sub(r"(?i)extract.*\(.*date.*current_timestamp\s*\(\s*\)\s*\)","current_timestamp()", modified_sql)
                modified_sql = re.sub(r"(?i)group\s* by\s*.*","group by 1,2,3,4,5,6,7,8,9,11,12;", modified_sql)

                insert_sql = f"""insert into {config.dqaas_profile_rpt} (rpt_seq_num, profile_id, profile_type, dq_pillar, src_tbl, meas_name, data_dt,feature_name,grouped_columns,count_curr,prfl_run_ts,weekday) """ + modified_sql

                self.logger.info(f"History SQL for {src_tbl} : {insert_sql}")

                #job = self.utils.client.query(insert_sql)

                insert_status = self.utils.run_bq_dml_sql(
                    bq_auth=config.dq_gcp_auth_payload,
                    dml_query=insert_sql
                )

                self.logger.info('insert_status')
                self.logger.info(insert_status)

        except Exception as err:
                self.logger.error(f"Error While Executing insert_sql: Error{err}")


    def main(self):
        try:
            self.laod_historical_report()
            df_mtd = self.get_metadata()
            
            sub_domain_list = df_mtd['data_sub_dmn'].unique().tolist()
            start_date = "current_date-1"
            end_date = "current_date-1"
            
            for dmn in sub_domain_list:
                self.main_metrics_execution(
                    df_mtd=df_mtd,
                    sub_domain=dmn,
                    start_date=start_date,
                    end_date=end_date
                )
            
        except ValueError as err:
            self.logger.error(err)
        except Exception as err:
            self.logger.error(f"Error in Main Block. Error {err}")
            
            


# CustomeMetrics().main()
=====================================================================================================================================================
------------------------------------------------------------------------------------------------------------------------------------------------
# importing modules
# from multiprocessing import Pool, set_start_method
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import psutil
import os
import shutil
import logging
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

## Added on 2024-03-26 for Connection issues
import argparse
import sys

## Importing User Defined Modules
sys.path.insert(1,os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))
import scripts.config_params as config
from scripts.dqr import DQR as DQR
import scripts.pattern_identifiers as pattern_identifiers

## Commenting Integrity for computational issue. Search the code (INT-2024-07-11) for the related changes 
# import integrity_score_identifier as integrity_score 
sys.path.insert(1,os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))
from utils.send_email import SendEmail
from scripts.common_handlers import CommonUtils, set_logger, get_args_parser
from scripts.dqaas_opsgenie import Alert
from scripts.dqaas_jira import Jira_ticket


class AutoProfileEngine(object):
    def __init__(self, data_src: str=None, log_filename: str=None):
        #self.__set_proxies()
        self.log_filename = log_filename
        self.data_src = data_src
        if self.data_src not in config.APPL_DATA_SRC:
            raise Exception(f"Data Source not Provided. Error: Data Source Value is {data_src}")
            
        self.logger = self.set_auto_profile_logger(
            process_name="AP-Main",
            data_src=data_src,
            log_filename=log_filename
        )
        self.logger.info(f"Logger Filename: {self.log_filename}")
        
        self.utils = CommonUtils(logObj=self.logger)
        
        self.email = SendEmail(
            loggerObj=self.logger,
            mail_from=config.SENDER_EMAIL_ID,
            smtp=config.SMTP_SERVER_NAME
        )

    ## Defining Proxy Parameters
    @staticmethod
    def __set_proxies():
        os.environ["http_proxy"] = config.GCP_HTTP_PROXY_URL
        os.environ["https_proxy"] = config.GCP_HTTPS_PROXY_URL
        os.environ["no_proxy"] = config.GCP_NO_PROXY_URLS
        
    ## Set Logger
    # @staticmethod
    def set_auto_profile_logger(self, process_name:str, data_src: str=None, log_filename: str=None):
        
        timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
        filename = f'autoprofile_{timestamp}'
        
        if data_src is not None:
            filename = f'{data_src}_autoprofile_{timestamp}'
            
        if log_filename is not None:
            filename = log_filename
        
        self.log_filename = filename
        
        print(f"Printing Logger Filename for AP: {self.log_filename} ")
        process_id = os.getpid()
        log: logging = set_logger(
            logger_path=config.AUTO_PROFILE_LOGS,
            log_filename=filename,
            process_name=f'{process_name}-{process_id}',
            no_date_yn="Y",
        )
        
        return log

    ## Convert Mail distro into list - Retrieve Persona Mails from the Dataframe
    def get_mail_distro(self, df_val:pd.DataFrame, sub_dmn:str, persona: str, default_mail_distro: str) -> list:
        try:
            self.logger.info(f"Sub Domain: {sub_dmn}, Persona: {persona}, Mail Distro Dataframe: \n{df_val}")
            mail_distro_str = df_val.query(f"DATA_SUB_DMN == '{sub_dmn}'")[persona].tolist()
            
            self.logger.info(f"Mail Distro: {mail_distro_str}")
            if len(mail_distro_str) > 0:
                return config.convert_str_to_list(mail_distro_str[0]) 
            
        except Exception as err:
            self.logger.error(f"Error Occured while getting the Mail Distro. Error:{err}")
            
        # return convert_str_to_list(default_mail_distro) 
        return [] 
            
    ## Frames Incremental Where Clause 
    def get_incremental_condition_where_clause(self, inc_dt_col: list, inc_dt_cond: int, date_range_val: str, day_limit: int=0):
        try:
            self.logger.info(f"""
            -------------------------------------------------------------------
            Incremental Column: {inc_dt_col}, Condition: {inc_dt_cond}
            Date Range: {date_range_val}, Days Limit: {day_limit}
            -------------------------------------------------------------------
            """)

            if len(inc_dt_col) > 0 :
                inc_dt_cond =  int(inc_dt_cond) if inc_dt_cond not in config.EMPTY_STR_LIST else 0
                # business_date_range = f"(date({date_range_val}) - {day_limit} - {inc_dt_cond})" 
                business_date_range = f"(cast({date_range_val} as date) - {inc_dt_cond})" 
                # where_clause = f" where cast({inc_dt_col[0]} as date format 'YYYY-MM-DD') >= {business_date_range}"
                where_clause = f" where cast({inc_dt_col[0]} as date) >= {business_date_range}"
                return where_clause, "INCR"
            
            return "", "FULL"
        except Exception as e: 
            self.logger.error(f"Error Occurred While Framing Incremental Where condition. Error: {e}")
        
        return "", "ERROR"
        
    ## Function to Check table data availability
    def check_data_availability(self, data_src: str, db_name: str, count_query: str):
        """
            Count Check:
            * This is a Count check Block, which will be used to get the Data Avalibility Count
            using count function in SQL statement.
            * Returns Table Record Count and Execution Status
            * If the count is greater than Zero then the Profiling will be initiated else the process will be skipped or ended
            * Status: ERROR - Failures, SUCCESS - For Successfully Execution
        """
        
        try:
            self.logger.info("Data Availability Check Initiated")
            
            df_count = self.utils.get_query_data(
                data_src=data_src,
                dbname=db_name,
                select_query=count_query
            )

            self.logger.info(f"Count Result: \n{df_count}")
            
            if len(df_count) > 0:
                total_record_count = df_count.iloc[0, 0]
                self.logger.info(f"Table Record Count: {total_record_count}")
                return total_record_count, 'SUCCESS'
            
            return 0, "WARN"
        except Exception as e:
            self.logger.info(f"Error While Executing the Table Data Availability. Error: {e}")
            
        return 0, "ERROR"

    ## Pillars: 2024-05-15. Pattern Identification for Conformity and Validity   
    def get_average_history_count(self, auto_prfl_tbl_rpt: str, src_table: str, table_id: int):
        try:
            avg_count = f"""
                select ifnull(avg(ifnull(TBL_ROW_COUNTS, 0)), 0) as avg_count from {auto_prfl_tbl_rpt} as main
                where PROFILE_ID = {table_id}
                and upper(TABLE_NAME) = upper('{src_table}')
                and date(prfl_run_dt) between (current_date - 30) and (current_date)
                and main.rpt_ref_key in (select max(max_rpt.rpt_ref_key) 
                                        from {auto_prfl_tbl_rpt} as max_rpt
                                        where date(max_rpt.prfl_run_dt) = date(main.prfl_run_dt) and max_rpt.PROFILE_ID=main.PROFILE_ID)
            """
            self.logger.info(f"Average Historical Query: {avg_count}")
            df_count = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=avg_count
                )
            self.logger.info(f"Count Result: {df_count}")
            
            if len(df_count) > 0:
                avg_hist_count = df_count.iloc[0, 0]
                self.logger.info(f"Average Hist Count: {avg_hist_count}")
                return avg_hist_count
        
            return 0
        except Exception as err:
            raise Exception(f"Error occurred while Identifying Patterns and Abnormal Data ")

    ## Pillars: 2024-05-15. Pattern Identification for Conformity and Validity   
    def get_data_patterns(self, df_val: pd.DataFrame):
        try:
            self.logger.info("Identifying Unique lengths and Patterns in Column Level")
            # identify length of each value in the cells
            df_length_pattern = df_val.astype(str).applymap(len)
            # identify pattern for each value in the cell 
            df_char_label_data = df_val.applymap(pattern_identifiers.charactor_labeling)
            df_string_pattern = df_char_label_data.applymap(pattern_identifiers.number_labeling)
            
            return df_length_pattern, df_string_pattern
        
        except Exception as err:
            raise Exception(f"Error occurred while Identifying Patterns and Abnormal Data ")

    ## Pillars: 2024-05-15. Numeric 
    def get_numeric_report_data(self, data_src: str, dqr_df: pd.DataFrame, src_dbname, src_tablename, incr_where_clause):
        try:
            df_numeric =  pd.DataFrame(dqr_df.describe().T)
            self.logger.info(f"Continuous dataframe Length: {len(df_numeric)} \n {df_numeric}")
            df_numeric['COL_MISSING_CNT']=((len(dqr_df)-df_numeric['count'])/len(dqr_df))*100
            df_numeric['COL_CARDINALITY']=np.nan
            df_numeric.reset_index(inplace=True)
            self.logger.info(f"Continuous dataframe: {df_numeric.columns}")
            
            df_numeric = df_numeric.rename(columns={
                'index': 'COL_NAME',
                'count': "COL_TOT_CNT",
                'missing': "COL_MISSING_CNT", 
                'cardinality': "COL_CARDINALITY",
                'mean': "COL_MEAN",
                '25%': "COL_1ST_QRT_VAL",
                '50%': "COL_MEDIAN",
                '75%': "COL_3RD_QRT_VAL", 
                'std': "COL_STD_DEV",
            })
            
            self.logger.info(f"Numeric table columns : {df_numeric.columns}")
            
            df_numeric = df_numeric.drop(
                columns=["data_type", "index", "DATATYPE", "LENGTH_CHECK_PERC", "PATTERN_CHECK_PERC", "min", "max"],
                axis=1,
                errors="ignore"
            )
            
            if len(df_numeric) > 0:
                col_min_max_dtls_list = self.get_min_max_for_numeric_columns(
                    data_src=data_src,
                    db_name=src_dbname,
                    source_tablename=src_tablename,
                    inc_dt_cond_where_clause=incr_where_clause,
                    df_numeric=df_numeric
                )
                df_col_min_max_dtls_list = pd.DataFrame.from_dict(col_min_max_dtls_list)
                df_numeric = df_numeric.merge(df_col_min_max_dtls_list, how="inner", on=["COL_NAME"])
                df_numeric = df_numeric.reset_index(inplace=True)
                
            return df_numeric
            
        except Exception as err:
            self.logger.error(f"Error Occurred while Identifying the Numeric Data. \nError: {err}")

    ## Pillars: 2024-05-15. Min Max
    def get_min_max_for_numeric_columns(self, data_src: str, df_numeric: pd.DataFrame, db_name, source_tablename, inc_dt_cond_where_clause) -> list:
        try:
            self.logger.info("Continuous Data - Identifying Min and Max Value")
            if len(df_numeric) == 0:
                self.logger.info('Numeric Dataframe Not Found')
                return pd.DataFrame()
            
            numeric_cols_list = df_numeric["COL_NAME"].unique().tolist()
            # forms aggregate query for numeric columns 
            agg_cols = [ f"{agg}({num}) as {agg}_{num}" for num in numeric_cols_list for agg in ["min", "max"]  ]
            agg_cols = ", ".join(agg_cols)
        
            agg_query = f"SELECT {agg_cols} FROM {source_tablename} {inc_dt_cond_where_clause};"
            self.logger.info(f"Aggregated query: {agg_query}")
            
            agg_result = self.utils.get_query_data(
                data_src=data_src,
                dbname=db_name,
                select_query=agg_query
            )

            if len(agg_result) > 0:
                self.logger.debug(f"Aggregated columns: {agg_result.columns}")
                
                column_agg_list = []
                for num_col in numeric_cols_list:
                    min_val = agg_result[f"min_{num_col}"].iloc[0]
                    max_val = agg_result[f"max_{num_col}"].iloc[0]
                    self.logger.debug(f"Numeric col : {num_col}, Min Value:{min_val}, Max Value:{max_val}")
                    column_agg_list.append({
                        "COL_NAME": num_col,
                        "COL_MIN_VAL": min_val,
                        "COL_MAX_VAL": max_val
                    })
                    
                self.logger.info(f"New List for Min Max: {column_agg_list}")

                return column_agg_list
            
        except Exception as err:
            self.logger.error(f"Error Occurred while Identifying Min and Max for the Numeric Data. \nError: {err}")
            
        return pd.DataFrame()

    ## Pillars: 2024-05-15. Categorical  
    def get_categorical_report_data(self, dqr_obj: DQR) -> pd.DataFrame:
        try:
            self.logger.info("Categorical Data Process started")
            df_catg = dqr_obj.get_categorical_df()
            
            self.logger.info(f"Categorical data frame: {df_catg.columns}")
            
            df_catg = df_catg.rename(columns={
                'index': 'COL_NAME',
                'count': "COL_TOT_CNT", 
                'missing': "COL_MISSING_CNT",
                'cardinality': "COL_CARDINALITY", 
                'mode': "COL_MODE",
                'mode_freq': "COL_MODE_FREQ", 
                'mode_percent': "COL_MODE_PERCENT",
                'mode_2': "COL_MODE_2", 
                'mode_2_freq': "COL_MODE_2_FREQ",
                'mode_2_perc': "COL_MODE_2_PERC",
            })
            
            self.logger.info(f"Categorical data frame renamed: {df_catg.columns}")

            df_catg = df_catg.drop(
                columns=["data_type", "index", "DATATYPE", "LENGTH_CHECK_PERC", "PATTERN_CHECK_PERC"],
                axis=1,
                errors="ignore"
            )
            df_catg = df_catg.reset_index(inplace=True)
            
            return df_catg
        except Exception as err:
            self.logger.error(f"Error Occurred while Identifying the Catgorical Data. \nError: {err}")
            
        return pd.DataFrame()

    ## Pillars: 2024-05-15. UniquePattern
    def get_unique_pattern_count(self, pattern_data: pd.DataFrame, column: str):
        # df_pattern_list = pd.DataFrame.from_dict(pattern_data[col].value_counts().reset_index().rename(columns={col: "value", 0: "count"}).to_dict())
        df_pattern_list = pd.DataFrame.from_dict(pattern_data[column].value_counts().reset_index().rename(columns={"index": "value", 0: "count"}).to_dict())
        unique_count = len(df_pattern_list['value'].unique().tolist())
        return unique_count
    
    def get_prod_details(self,data_lob,data_sub_dmn):
        # query = f''' select distinct product_type,product_area,product_name,business_program 
        #             from {config.dqaas_auto_prfl_mtd} mtd
        #             JOIN {config.dqaas_auto_rule_prod_mtd} prd 
        #             ON 
        #             mtd.data_lob = prd.data_lob
        #             --AND mtd.data_bus_elem = prd.data_bus_elem
        #             AND mtd.data_dmn = prd.data_dmn
        #             AND mtd.data_sub_dmn = prd.data_sub_dmn
        #             where
        #             mtd.data_lob = '{data_lob}'
        #             --AND mtd.data_bus_elem = '{data_bus_elem}'
        #             AND mtd.data_dmn = '{data_dmn}'
        #             AND mtd.data_sub_dmn = '{data_sub_dmn}'
        #             ;
        #         '''    
        query = f"""select distinct txnmy.product_type,txnmy.product_area,txnmy.product_name from `{config.dqaas_mtd}` mtd
                     JOIN {config.dqaas_taxonomy} txnmy
                     ON 
                     mtd.data_lob = txnmy.lob
                     AND mtd.data_sub_dmn = txnmy.l2_label
                     where
                     mtd.data_lob = '{data_lob}'
                     AND mtd.data_sub_dmn = '{data_sub_dmn}'"""
        try:
            df_val = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=query
            )
            return df_val
        except Exception as err:
            raise Exception(f'Error Occured While Executing the Query to fetch prod details. Error: {err} ')

    ## Auto Profile Engine
    def auto_profile(self, input_dict: dict):
        
        logger = self.logger
        process_id = os.getpid()  
        report_reference_key = datetime.now().strftime("%Y%m%d%H%M%S%f")
        overall_reference_key = datetime.now().strftime("%f")
        logger.info(f"******** Process Started. Process ID: {process_id}")
        start_mem = psutil.Process(process_id).memory_full_info().uss
        start_time = datetime.now()
        opsgenie_alert_info = {}
        filename: str = ""
        source_tablename: str =""
        failed_tables: dict = {}
        table_level_score: dict = {}
        try:
            logger.info(f"Input: {input_dict}")
            # Connect to particular table, calculate uniqueness, completeness
            # call reporting function
            # execution_date = auto_profile_Date
            execution_date = datetime.now()
            table = input_dict.get('TABLE_NAME','')
            domain = input_dict.get('DATA_DMN','')
            project_name = input_dict.get('PROJECT_NAME','')
            td_dbname = input_dict.get('DATABASE_NAME','')
            source_tablename = f"{project_name}.{td_dbname}.{table}"
            
            lob = input_dict.get('DATA_LOB','')
            data_sub_dmn = input_dict.get('DATA_SUB_DMN','')
            PROFILE_ID = input_dict.get('PROFILE_ID','')
            # table_cols = input_dict.get('TBL_CDE','')
            inc_dt_cond = input_dict.get('INCR_DATE_COND',0)
            data_src = input_dict['DATA_SRC']
            logger.info("Working -----------------------")
            ## Converting below Parameters to List, which will be used is retriving the data
            table_cde_columns = config.convert_str_to_list(str(input_dict.get('TBL_CDE','')).upper())
            unique_index_columns = config.convert_str_to_list(str(input_dict.get('UNIQUE_INDEX_COLS')).upper())
            incremental_date_column = config.convert_str_to_list(str(input_dict.get('INCR_DATE_COL','')).upper())
            logger.info("Working2 -----------------------")

            inc_dt_cond = config.convert_str_to_list(input_dict.get('INCR_DATE_COND',0))
            incremental_date_duration = 0
            if len(inc_dt_cond) > 0:
                incremental_date_duration = inc_dt_cond[0] if str(inc_dt_cond[0]) not in config.EMPTY_STR_LIST else 0

            logger.info(f"inc_dt_cond: {inc_dt_cond}, {incremental_date_duration}")
            
            
            # date_range = 'current_date'
            process_date = f"'{datetime.now().date() - timedelta(days=config.AP_N_DAYS_LIMIT)}'"
            
            
            print("source_tablename1", source_tablename)
            
            # required_rpt_cols = {
            #     'PRFL_RUN_DT' : execution_date,
            #     'TABLE_NAME' : table,
            #     'PROFILE_ID' : PROFILE_ID,
            #     'RPT_REF_KEY' : report_reference_key,
            # }
            
            """
            ## If Table CDE is not provide then all the columns will be considered While retriving the data
            ## If Table CDE is provided, then concating Table CDE, Unique Index and Incremental columns as list
            ## and removing the duplicate columns ins the list
            """
            profile_columns = "*" 
            if len(table_cde_columns) > 0:
                profile_columns = ", ".join(list(set(table_cde_columns + unique_index_columns + incremental_date_column)))
            
            logger.info(f"""
            ------------------------------------------------------------------
            Profille ID                 : {PROFILE_ID}
            Execution Date              : {execution_date}
            Table                       : {source_tablename}
            Sub Domain                  : {data_sub_dmn}
            Table CDE                   : {table_cde_columns}
            Unique Index Column         : {unique_index_columns} 
            Incremental Date Column     : {incremental_date_column}
            Incremental Date Condition  : {incremental_date_duration}
            Process Date                : {process_date}
            Columns For Profiling       : {profile_columns}
            Reference Key               : {report_reference_key}
            Log Thread ID               : {overall_reference_key}
            Process ID                  : {process_id}
            Start at                    : {start_time}
            ------------------------------------------------------------------
            """)
            
            
            """
            ## where clause is framed if Incremental Column is given, else empty str will be returned for Load
            ## To Differentiate the Load type status is returned
            ## INCR - Incremental Load, FULL - full Load, ERROR - Internal Errored
            """
            where_clause, incremental_load_status= self.get_incremental_condition_where_clause(
                inc_dt_col=incremental_date_column,
                inc_dt_cond=incremental_date_duration,
                date_range_val=process_date,
                # day_limit=n_days_interval
            )
            
            ## Any failures while framing Incremental where clause, Error is raised
            if incremental_load_status == "ERROR":
                failed_tables = {'Table': source_tablename, 'Query': ""}
                raise Exception("Error Occured While Framing Incremental Where Clause")

            total_record_count = 0
            count_query = f"select count(1) from {source_tablename}"
            
            """
            ## Checking Data Availability for the Given Count Query and where Clause
            ## Returns Table Record Count and Execution Status 
            ## Status: ERROR - Failures, SUCCESS - For Successfully Execution
            """
            incr_count_query = f"{count_query} {where_clause}"
            total_record_count, query_exec_status = self.check_data_availability(
                data_src=data_src,
                db_name=td_dbname,
                count_query=incr_count_query
            )
            logger.info(f"Incremental Load. Records Count:{total_record_count}, Status:{query_exec_status}")
            
            ## If the First Try is Full Load and Error is Occurred then Mail Alert and Exception is Raised
            # if incremental_load_status=="FULL" and query_exec_status=="ERROR":
            #     email_alert(f'Table({source_tablename}) not Found')
            #     raise Exception(f'Table not found - Full Load. Table Name:{source_tablename}')
            
            ## If the Total Record Count for Incremental Load is Zero, Then Full load is idenfied
            ## If the First Try is Full Load and Error is Occurred then Ignored
            # if total_record_count == 0 and not(incremental_load_status=="FULL" and query_exec_status=="ERROR"):
            
            """
            ## Commented on 2024-04-24
            if total_record_count == 0 and incremental_load_status != "FULL" and data_src == 'TD':
                total_record_count, query_exec_status = check_data_availability(
                    data_src=data_src,
                    db_name=td_dbname,
                    count_query=count_query,
                )
                where_clause = ""
                logger.info(f"Full Load. Records Count:{total_record_count}, Status:{query_exec_status}")
            """    
                
            ## If the Failure occurred on  the second try or Full Load then Mail Alert and Exception is Raised
            if query_exec_status == "ERROR":
                failed_tables = {'Table': source_tablename, 'Query': incr_count_query}
                self.email_alert(f'Table({source_tablename}) not Found')
                raise Exception(f'Table not found. Table Name:{source_tablename}')
            
            ## If there is no data found then Mail Alert and Exception is Raised       
            if total_record_count == 0:
                failed_tables = {'Table': source_tablename, 'Query': incr_count_query}
                self.email_alert(f'No Data Found in the Table({source_tablename})')
                raise Exception(f"No Data Found in the Table({source_tablename}")
            
            TD_data_limit_clause = BQ_data_limit_clause = ""
            if total_record_count > config.AP_DATA_LIMIT:
                TD_data_limit_clause = f"TOP {config.AP_DATA_LIMIT}" if data_src == 'TD' else ""
                BQ_data_limit_clause = f"LIMIT {config.AP_DATA_LIMIT}" if data_src in ('GCP', 'BQ') else ""
            
            data_query = f"select {TD_data_limit_clause} {profile_columns} from {source_tablename} {where_clause} {BQ_data_limit_clause}"
            logger.info(f"Query For Data Retreival: {data_query}")

            
            ## Data Retreival for profiling 
            data = pd.DataFrame()
            try:
                
                data = self.utils.get_query_data(
                    data_src=data_src,
                    dbname=td_dbname,
                    select_query=data_query
                )
                
            except Exception as err:
                message = f"""
                Error Occurred While Executing the Final table Query. Kindly Check the Table CDE / Unique Index Columns 
                Table: {source_tablename}
                Query: {data_query}
                Error: \n{err}
                """
                failed_tables = {'Table': source_tablename, 'Query': data_query}
                # email_alert(message)
                raise Exception(message)

            if len(data) > 0:
                data = data.rename(columns={col: str(col).upper() for col in data.columns.tolist()})
                
            logger.info(f"""
            Data Retreival Length: {len(data)}
            Columns: {data.columns}
            """)
            ## If the Total Count is greater than data Limit, Sampling the records 
            if total_record_count > config.AP_DATA_LIMIT:
                logger.info("Table Eligible for Sampling, Random Sampling in progress.........")
                data=data.sample(frac=0.25, replace=False, ignore_index=False)
            
            ## If the CDE's are not provided, then entire columns are considered as CDE
            if len(table_cde_columns) == 0:
                data = data.rename(columns={col: str(col).upper() for col in data.columns.tolist()})
                table_cde_columns = data.columns.tolist()
            
            ACTUAL_RECORD_COUNT = total_record_count
            SAMPLE_RECORD_COUNT =len(data)
            ## Patterns, Numeric and Categorical
            df_length_pattern, df_string_pattern = self.get_data_patterns(df_val=data)
            
            logger.info("Anomalies - Identifying Column level Conformity and Validity.....")
            df_abnormal_dtls = pattern_identifiers.anomalies_identifier(df_length_pattern, df_string_pattern)
            df_abnormal_dtls['COL_NAME'] = df_abnormal_dtls['COL_NAME'].map(lambda x: str(x).upper())
            df_abnormal_dtls = df_abnormal_dtls.rename(columns={
                'COL_NAME': 'COL_NAME',
                'LENGTH_CHECK_PERC':"COL_VALIDITY",
                'PATTERN_CHECK_PERC':"COL_CONFORMITY",
            })
            
            logger.info(f"Abnormal Details: {df_abnormal_dtls.columns}")
            
            ## Converting Dataframe to CSV for DQR report Purpose
            filename = f"TD_{source_tablename}_{report_reference_key}"
            temp_csv_filename = self.utils.write_df_to_csv(
                filepath=config.TEMP_DIR,
                filename=filename,
                df_val=data
            )
            
            dqr_df = pd.DataFrame()
            if os.path.exists(temp_csv_filename):
                dqr_df = pd.read_csv(temp_csv_filename)
                
            unnamed_col_list = [col  for col in dqr_df.columns if str(col).upper().__contains__("UNNAMED")]
            dqr_df = dqr_df.drop(columns=unnamed_col_list, axis=1, errors="ignore")
            
            ## Removing the CSV Files
            self.utils.remove_temp_csv(
                filepath=config.TEMP_DIR,
                filename=filename
            )
            
            ## Removing Categorical and Numerical 
            """
            ## Numeric Report
            df_numeric_rpt_dtls = get_numeric_report_data(
                dqr_df=dqr_df,
                src_dbname=db,
                src_tablename=source_tablename,
                incr_where_clause=where_clause
            )

            ## Categorical Report
            dqr_obj = DQR(df=dqr_df)
            df_catg_rpt_dtls = get_categorical_report_data(dqr_obj=dqr_obj)
            """
            ## Identifying Column Level Score    
            logger.info(f'Columns for Identifying the score - {table_cde_columns}')
            logger.info('Column Level Score - Validation Initiated')
            
            ## Commenting Integrity for computational issue. Search the code (INT-2024-07-11) for the related changes 
            """
            integrity_dict  = integrity_score.check_integrity(dqr_df,dict(zip(dqr_df.columns,dqr_df.dtypes)))
            logger.info(f"Integrity Score: {integrity_dict}")
            """
            
            column_level_score: list = []
            for col in table_cde_columns:
                logger.debug(f'Column: {col}')
                col_completeness = (100 - (data[col].isna().sum() / data[col].size * 100)).round(2)
                
                # col_unique_pattern_cnt = get_unique_pattern_count(pattern_data=df_string_pattern, column=col)
                # col_unique_length_cnt = get_unique_pattern_count(pattern_data=df_length_pattern, column=col)
                
                ## Commenting Integrity for computational issue and Assigned Zero. Search the code (INT-2024-07-11) for the related changes 
                # col_integrity = integrity_dict.get(col, np.nan)
                col_integrity = 0
                
                logger.debug(f'Completeness Score: {col_completeness}')
                column_level_score.append({
                    'COL_NAME': col,
                    'COL_COMPLETENESS': col_completeness,
                    'COL_INTEGRITY': col_integrity,
                    # 'COL_UNIQUENESS': round(len(data[col].unique()) / len(data[col]) * 100, 2),
                    # 'COL_CONFORMITY': 0,
                    # 'COL_VALIDITY': 0
                    'PRFL_RUN_DT' : execution_date,
                    'TABLE_NAME' : table,
                    'PROFILE_ID' : PROFILE_ID,
                    'RPT_REF_KEY' : report_reference_key,
                })
                
            logger.info(f'Column Level Score: {column_level_score}') 
            df_column_details = pd.DataFrame.from_records(column_level_score)
            df_column_details = df_column_details.merge(df_abnormal_dtls, how="left", on=["COL_NAME"])
            logger.info(f"df_column_details:\n {df_column_details}")
            # integrity_dict  = integrity_score.check_integrity(dqr_df,dict(zip(dqr_df.columns,dqr_df.dtypes)))
            # logger.info(f"Integrity Score: {integrity_dict}")
            # df_column_details["COL_INTEGRITY"]=integrity_dict.values()
            df_column_details = df_column_details.reset_index(drop=True)
            logger.info(f'Column Level Score - Validation Completed. Records Found: {len(df_column_details)} \nColumns:{df_column_details.columns}')
        
            # Timeliness
            #timeliness = timeliness_score(dfval=data, incr_dt_col=date_col, incr_dt_cond=chk_date_interval)
            
            TIMELINESS = 100 if len(data) > 0 else 0
            logger.info(f'Timeliness: {TIMELINESS}')
            
            ## Identifying Table Level Score
            logger.info(f"Table Level Score - Validation Initiated. Source Data Length: {len(data)}")
            
            temp_df = pd.DataFrame()
            try:
                logger.info(f"Unique Index Columns List :{unique_index_columns}")
                logger.info(f"Table Columns :{data.columns}")
                if len(unique_index_columns) > 0:
                    temp_df = data[unique_index_columns]
            except Exception as e:
                logger.error(f'Index Column Not Found. Error Info:{e}')
                temp_df = pd.DataFrame()

            if len(temp_df) == 0:
                temp_df = data
                
            ## Table Level Scores
            de_duplicate_len = len(temp_df.drop_duplicates()) #len(temp_df.duplicated())
            logger.info(f'Length of De-duplicates: {de_duplicate_len}, Source Data: {len(temp_df)}')
            COMPLETENESS = self.utils.round_off(df_column_details['COL_COMPLETENESS'].mean())
            # uniqueness = df_column_details['COL_UNIQUENESS'].mean().round(2)
            UNIQUENESS = self.utils.round_off(((de_duplicate_len/len(temp_df))*100))
            VALIDITY = self.utils.round_off(df_column_details['COL_VALIDITY'].mean())
            CONFORMITY = self.utils.round_off(df_column_details['COL_CONFORMITY'].mean())
            INTEGRITY = self.utils.round_off(df_column_details["COL_INTEGRITY"].mean())

            avg_hist_count = self.get_average_history_count(
                auto_prfl_tbl_rpt=config.dqaas_auto_prfl_tbl_rpt,
                src_table=table,
                table_id=PROFILE_ID
            )
            
            variation_perc = 0  
            if avg_hist_count > 0 :
                variation_perc = ((ACTUAL_RECORD_COUNT - avg_hist_count) / avg_hist_count) * 100
                
            CONSISTENCY = 100 if variation_perc < config.CONSISTENCY_VARIATION_PERC else 0
            
            total_pillars = 6 ## Assigned the Vairable value from 7 to 6 for intergrity issue . Search the code (INT-2024-07-11) for the related changes 
            overall_score  = self.utils.zero_if_null(UNIQUENESS) + self.utils.zero_if_null(COMPLETENESS)
            overall_score += self.utils.zero_if_null(VALIDITY) + self.utils.zero_if_null(CONFORMITY) # + self.utils.zero_if_null(INTEGRITY) ## Commenting Integrity for computational issue and Assigned Zero. Search the code (INT-2024-07-11) for the related changes 
            overall_score += self.utils.zero_if_null(TIMELINESS)  +  self.utils.zero_if_null(CONSISTENCY)
            overall_score = self.utils.zero_if_null(overall_score / total_pillars)
            
            
            logger.info(f"""
            ************************************************************
            Reference Key               : {report_reference_key}
            Process ID                  : {process_id}
            Completeness                : {COMPLETENESS}
            Uniqueness                  : {UNIQUENESS}
            Validity                    : {VALIDITY}
            Conformity                  : {CONFORMITY}
            Integrity                   : {INTEGRITY}
            Timeliness                  : {TIMELINESS}
            Consistency                 : {CONSISTENCY}
            Total Actual Count          : {ACTUAL_RECORD_COUNT}
            Sample Count                : {SAMPLE_RECORD_COUNT}
            Average Variation           : {avg_hist_count}
            Variation Percentage        : {variation_perc}
            DQ Score (Overall Score)    : {overall_score}
            ************************************************************
            """)
            
            # logger.info('***************************************')
            # score = (((completeness + uniqueness + timeliness) / 300) * 100).round(2)
            # logger.info('Overall DQ Score :' + str(score) )
            # logger.info('***************************************')
            
            table_level_score: dict = {
                'PRFL_RUN_DT' : execution_date,
                'PROFILE_ID' : PROFILE_ID,
                'RPT_REF_KEY' : report_reference_key,
                'DATA_LOB': lob,
                'DATA_DMN': domain,
                'DATA_SUB_DMN': data_sub_dmn,
                'DATA_SRC': input_dict.get('DATA_SRC',''),
                'DATABASE_NAME': td_dbname,
                'TABLE_NAME' : table,
                'TBL_COMPLETENESS': COMPLETENESS,
                'TBL_UNIQUENESS': UNIQUENESS,
                'TBL_TIMELINESS': TIMELINESS,
                'TBL_CONFORMITY': CONFORMITY,
                'TBL_VALIDITY': VALIDITY,
                'TBL_INTEGRITY': INTEGRITY,
                'TBL_CONSISTENCY': CONSISTENCY,
                'TBL_DQ_SCORE': overall_score,
                'TBL_AVG_COUNT': avg_hist_count,
                'TBL_VARIATION_PERC': variation_perc,
                'TBL_COL_COUNTS' : len(data.columns),
                'TBL_ROW_COUNTS' : ACTUAL_RECORD_COUNT,
                'TBL_TOTAL_CELLS' : data.size,
                'TBL_SAMPLE_ROW_COUNT': SAMPLE_RECORD_COUNT,
                'SCORE_MAX_THRESHOLD': input_dict.get('SCORE_MAX_THRESHOLD', np.nan),
                'SCORE_MIN_THRESHOLD': input_dict.get('SCORE_MIN_THRESHOLD', np.nan),
                # 'TBL_DUPLICATES': round((de_duplicate_len/len(temp_df))*100, 2),
                
            }
            print("table_level_score",table_level_score)
            
            try:
                message = ''
                request_id = ''
                if input_dict.get('IS_OPSGENIE_FLG') == 'Y':
                    self.logger.info(f"overall_score: {overall_score}")
                    self.logger.info(f"Auto_MAX_THRSD: {table_level_score['SCORE_MAX_THRESHOLD']}")
                    if table_level_score['SCORE_MAX_THRESHOLD'] not in config.EMPTY_STR_LIST:
                        table_level_score['SCORE_MAX_THRESHOLD'] = config.AP_SCORE_CHECK_PCT
                    if overall_score < table_level_score['SCORE_MAX_THRESHOLD']:                        
                        message = f"Table: {table_level_score['TABLE_NAME']} score is below threshold"
                        priority = "p3"                        
                        description = f"Run date: {table_level_score['PRFL_RUN_DT']}, Table: {table_level_score['TABLE_NAME']}, Score: {table_level_score['TBL_DQ_SCORE']}, Threshold: {table_level_score['SCORE_MAX_THRESHOLD']}"                        
                        details= {'Message': f"Table: {table_level_score['TABLE_NAME']} score is below threshold", 'Sub domain': table_level_score['DATA_SUB_DMN'], 'Data Src': table_level_score['DATA_SRC'] }
                        api_key = input_dict.get('OPSGENIE_API_KEY', np.nan)
                        if api_key == np.nan:
                            # Opsgenie Api Key
                            api_key = config.get_config_values('opsgenie', 'api_key')
                        opsgenie_client = Alert(api_key=api_key)
                        response,request_id = opsgenie_client.create_alert(message, description, details, priority)

                        logger.info(f"Opsgenie response code: {response}")
                        logger.info('Opsgenie alert sent successfully')

                if input_dict.get('JIRA_ASSIGNEE') is not None:
                    self.logger.info(f"Calling Jira Module for: {table_level_score['TABLE_NAME']}")
                    self.logger.info(f"overall_score: {overall_score}")
                    self.logger.info(f"Auto_MAX_THRSD: {table_level_score['SCORE_MAX_THRESHOLD']}")
                    if overall_score < table_level_score['SCORE_MAX_THRESHOLD']:                        
                        summary = f"ViewX|DQ Failure|Table: {table_level_score['TABLE_NAME']} score is below threshold"
                        description = f"DQ has failed for Table : {table_level_score['TABLE_NAME']} on Run date : {table_level_score['PRFL_RUN_DT']}. The Score is: {table_level_score['TBL_DQ_SCORE']} which is below set Threshold: {table_level_score['SCORE_MAX_THRESHOLD']}"
                        jira_client = Jira_ticket()
                        ticket_id=jira_client.create_jira_ticket(summary, description)

                try:
                    prod_info = self.get_prod_details(lob,data_sub_dmn)
                except Exception as err:
                    self.logger.error(f"Error occured while fetching product details {err}")
                
                opsgenie_alert_info: dict = {                    
                    'profile_id' : PROFILE_ID,                    
                    'data_sub_dmn': data_sub_dmn,
                    'data_src': input_dict.get('DATA_SRC',''),
                    'db_name': td_dbname,
                    'table_name' : table,
                    'dq_score': overall_score,
                    'threshold': input_dict.get('SCORE_MAX_THRESHOLD', np.nan),
                    'request_id': request_id,
                    'alert_message': message,
                    'product_type':prod_info.loc[0,'product_type'],
                    'product_area':prod_info.loc[0,'product_area'],
                    'product_name':prod_info.loc[0,'product_name']
                    # 'rule_id' : 'NA',
                    # 'src_col' : 'NA',
                    # 'meas_rule' : 'NA'
                }

            except Exception as err:
                self.logger.error(f"Error occured while creating opsgenie alert during auto profile {err}")
            
            df_table_details = pd.DataFrame.from_records([table_level_score])
            df_table_details = df_table_details.reset_index(drop=True)
            logger.info(f"Table Level Score - Validation Completed. \nRecord Count: {len(df_table_details)} \nColumns: {df_table_details.columns}")
            opsgenie_table_info = pd.DataFrame.from_records([opsgenie_alert_info])
            opsgenie_table_info = opsgenie_table_info.reset_index(drop=True)
            ## Load Auto Profile Results to Respective Report Tables
            self.load_auto_profile_results(
                table_report=df_table_details,
                column_report=df_column_details,
                opsgenie_report=opsgenie_table_info
            )
            
            
            end_mem = psutil.Process(process_id).memory_full_info().uss
            end_time = datetime.now()
            
            logger.info(f"""
            ======================================================================================
            Execution Details:
                Reference Key           :   {report_reference_key}
                Process ID              :   {process_id}
                Started at              :   {start_time}
                Ended by                :   {end_time}
                Total Time Taken        :   {(end_time-start_time)}
                Memory consumed (in MB) :   {(end_mem - start_mem) / (1024 * 1024 * 1024)} (in MB)
            ======================================================================================
            ******** Execution Completed 
            ======================================================================================
            """)
            
            return {"error_list": failed_tables, "end_result": table_level_score }
        except Exception as e :
            logger.error(f"Error Occured While Profiling the Table({source_tablename})\nError info: {e}")
            
            ## Removing CSV File
            self.utils.remove_temp_csv(
                filepath=config.TEMP_DIR,
                filename=filename
            )
            
        end_mem = psutil.Process(process_id).memory_full_info().uss
        end_time = datetime.now()
        
        logger.info(f"""
        ======================================================================================
        Execution Details:
            Reference Key           :   {report_reference_key}
            Process ID              :   {process_id}
            Started at              :   {start_time}
            Ended by                :   {end_time}
            Total Time Taken        :   {(end_time-start_time)}
            Memory consumed (in MB) :   {(end_mem - start_mem) / (1024 * 1024 * 1024)} (in MB)
        ======================================================================================
        ******** Execution Completed 
        ======================================================================================
        """)
        
        return {"error_list": failed_tables, "end_result": {} }
        # break

    ## Load Auto Profile Results to BQ Report Tables 
    def load_auto_profile_results(self, table_report: pd.DataFrame, column_report: pd.DataFrame,opsgenie_report: pd.DataFrame):
        try:
            ## BigQuery Client Connection
            self.logger.info("Loading to Resutls BQ")
            dbclient, db_creds = self.utils.bigquery_client(
                auth=config.dq_gcp_auth_payload
            )
            
            table_report = table_report.rename(columns={col: str(col).lower() for col in table_report.columns.tolist()})
            column_report = column_report.rename(columns={col: str(col).lower() for col in column_report.columns.tolist()})
            
            
            ## Loading Table Level Report
            self.utils.load_result_to_bq_report_table(
                dq_bq_client=dbclient,
                dq_credentials=db_creds,
                dq_report_table_name=config.dqaas_auto_prfl_tbl_rpt,
                df_load_data=table_report,
                seq_name='auto_prfl_num',
                column_details=config.AUTO_PRFL_TBL_RPT
            )
            
            ## Loading Column Level Report
            self.utils.load_result_to_bq_report_table(
                dq_bq_client=dbclient,
                dq_credentials=db_creds,
                dq_report_table_name=config.dqaas_auto_prfl_col_rpt,
                df_load_data=column_report,
                seq_name='auto_prfl_col_num',
                column_details=config.AUTO_PRFL_COL_RPT
            )

            ## Loading Opsgenie alert details to Opsgenie rpt table
            if len(opsgenie_report) > 0:
                self.utils.load_result_to_bq_report_table(
                dq_bq_client=dbclient,
                dq_credentials=db_creds,
                dq_report_table_name=config.dqaas_opsgenie_alert_rpt,
                df_load_data=opsgenie_report,
                seq_name='opsgenie_alert_num',
                column_details=config.OPSGENIE_ALERT_INFO_UI
            )

        except Exception as err:
            raise f"Error While Loading the results to BigQuery Report Table. Error: {err}"

    ## Email Alert               
    def email_alert(self, message: str, df=pd.DataFrame(), subject: str='', receipents_email_dtls: list=None) -> None:
        try:
            self.logger.info('Auto Profile e-mail Initiated')
            self.logger.info(f"Mail Group: {receipents_email_dtls}")
            if len(subject) == 0:
                subject = r'DQaaS 2.0 Auto Profile Report'
            
            if receipents_email_dtls is None:
                receipents_email_dtls = [] #dbconnection.get_mail_id(persona='PERSONA_3')
            
            ## If mail distro not found in mail distro table, then default Mail Distro is assigned
            if len(receipents_email_dtls) == 0:
                self.logger.info(f"Email ID not Found in the Mail Distro Table. Assigning Default mail distro({config.AP_DEFAULT_MAIL_GROUP})")
                receipents_email_dtls = config.AP_DEFAULT_MAIL_GROUP ##receipents_email_id #dbconnection.get_mail_id(persona='PERSONA_3')
                
            self.email.send_common_message(
                email_template_filepath=config.common_email_template,
                mail_subject=subject,
                message=message,
                df_val=df,
                receipents_email_id=receipents_email_dtls
            )
            
        except Exception as e:
            self.logger.error('Error while triggering email.\n {}'.format(e))

    ## Color Fonting Email Alert - Not in Use - Will be Enabled if required
    def email_summary_end_report(self, data_sub_dmn='', df_val=pd.DataFrame()):
        mail_subject = 'Auto Profile Summary Report'
        # summary_email_template  = os.path.join(os.path.abspath(config.get('dir','template_dir')),r'dq_auto_profile_report_summary_report.html')
        validation_col = 'Score% (Out of 100)'
        validation_score = 60 
        message = 'Please find the below Auto Summary Report for the table(s).'
        receipents_email_id = receipents_email_id #dbconnection.get_mail_id(persona='PERSONA_3')
        style_format_dict = {'Auto Profile Date':'{:%m-%d-%y}','Completeness':'{:,.2f}',
                            'Uniqueness':'{:,.2f}','Timeliness':'{:.2f}', r'Score% (Out of 100)':'{:.2f}'}
        
        self.email.send_auto_profile_email_message(
            mail_subject=mail_subject,
            email_template_filepath=config.ap_summary_email_template,
            df_val=df_val,
            report_header_name='',
            receipents_email_id=receipents_email_id,
            message=message,
            validation_col=validation_col,
            validation_score=validation_score,
            style_format_dict=style_format_dict
        )

    ## Perona Email - DevOps - Email Summary Alert
    def devops_email_summary_alert(self, sub_domain: str, df_summary_dtls: pd.DataFrame, df_email_distro: pd.DataFrame):
        try:
            ## Replacing NoN with NA for the float columns
            for col in config.FLOAT_COLS:
                if col in (df_summary_dtls.columns).to_list():
                    df_summary_dtls[col] = df_summary_dtls[col].fillna(np.nan).astype(float)
                    df_summary_dtls[col] = np.where(df_summary_dtls[col] == np.nan, 'NA', round(df_summary_dtls[col], 2))

            ## Filtering required columns for Summary Mail - Renaming Columns - Parameters are defined in config_params module
            df_summary_dtls  = df_summary_dtls[config.DEV_OPS_SUMMARY_COLS].rename(columns=config.AUTO_PROFILE_EMAIL_COLUMN_RENAME_LIST)
            df_summary_dtls  = df_summary_dtls.reset_index(drop=True)
            
            ## Getting Persona 3 Email Distro - For Dev-ops Summary Mail  
            persona_group_3_email = self.get_mail_distro(
                df_val=df_email_distro,
                sub_dmn=sub_domain,
                persona='PERSONA_3',
                default_mail_distro=config.AP_DEFAULT_MAIL_GROUP
            )
            
            self.logger.info(f""" Dev-Ops Email Details
            <======================= Persona 3: =======================>
            Sub Domain: {sub_domain}
            Mail Distro: {persona_group_3_email}
            Summary Length: {len(df_summary_dtls)}
            columns list: {df_summary_dtls.columns.to_list()}
            DataFrame: \n{df_summary_dtls.head(5)}
            <===========================================================>
            """)
            
            ## Overall Summary Mail For Dev-ops Team - Persona 3 Mail Groups
            summary_run_date = datetime.now().strftime("%m-%d-%y")
            summary_run_time = datetime.now().strftime("X%I:%M %p").replace('X0','').replace('X','').lower()
            email_message = f'Auto Profiling completed successfully for {summary_run_date} at {summary_run_time}. Kindly find the below list.\n Table List:'
                                                                                                                                
            self.email_alert(
                message=email_message, 
                df=df_summary_dtls,
                subject=f'DEV-OPS - Auto Profile Report for {sub_domain}',
                receipents_email_dtls=persona_group_3_email
            )
        except Exception as e:
            self.logger.error(f'Error Occured in DevOps Mail Summary Alert. Error Info: {e}')

    ## Persona Email - Data Steward - Overall summary and Threshold based summary
    def data_steward_email_summary_alert(self, sub_domain:str, df_completed_table_dtls: pd.DataFrame, df_mail_distros: pd.DataFrame):
        try:
            self.logger.info(f"""
            Data Steward / Persona 2 - Email Summary Alert - Initiated ........
            <=============== Persona Email ===============>
            Sub Domain: {sub_domain}
            Arrived Length: {len(df_completed_table_dtls)}
            Columns List: {df_completed_table_dtls.columns.tolist()}
            """)
            
            ## Identifying the Average Score using the 
            persona_2 = df_completed_table_dtls.groupby(by=['DATA_DMN','TABLE_NAME']).agg({'TBL_DQ_SCORE' : ['sum','count']})
            persona_2 = persona_2.fillna(0).astype(float)

            persona_2.columns = ['actual_sum','actual_count']
            persona_2 = persona_2.reset_index(level=['DATA_DMN','TABLE_NAME'])
            self.logger.debug(f"Persona 2 Agg Results:\n{persona_2}")

            persona_2['AVG_SCORE'] = np.where ( persona_2['actual_count']==0, 0, round(persona_2['actual_sum'] / persona_2['actual_count'],2))

            persona_2 = persona_2.drop(columns=['actual_sum','actual_count'], axis=1, errors='ignore')
            
            # df_completed_table_dtls = df_completed_table_dtls.loc[:, ~df_completed_table_dtls.columns.isin(['TBL_DQ_SCORE'])]
            persona_2 = df_completed_table_dtls.merge(persona_2, on=['DATA_DMN','TABLE_NAME'], how='left')
            
            ## Summary for Data Steward - Score less than defined Thresold
            # persona_2_score_based = persona_2[persona_2['AVG_SCORE'] < config.AP_SCORE_CHECK_PCT]
            persona_2_score_based = persona_2[persona_2['AVG_SCORE'] < persona_2['SCORE_MAX_THRESHOLD']]
            
            ## Replace NaN with NA for the float cols
            for col in config.FLOAT_COLS:
                if col in (persona_2.columns).to_list(): 
                    # persona_2[col] = persona_2[col].fillna(np.nan).astype(float)
                    persona_2[col] = np.where(persona_2[col] == np.nan, 'NA', round(persona_2[col], 2))
                
            ## Overall Summary for Data Steward    
            df_persona_2 = persona_2[config.DATA_STEWARD_SUMMARY_COLS].rename(columns=config.AUTO_PROFILE_EMAIL_COLUMN_RENAME_LIST)
            df_persona_2 = df_persona_2.reset_index(drop=True)
            
            ## Summary for Data Steward - Score less than defined Thresold
            # persona_2_score_based = persona_2[persona_2['AVG_SCORE'] < config.AP_SCORE_CHECK_PCT]
            persona_2_score_based = persona_2_score_based[config.DATA_STEWARD_SUMMARY_COLS].rename(columns=config.AUTO_PROFILE_EMAIL_COLUMN_RENAME_LIST)
            persona_2_score_based = persona_2_score_based.reset_index(drop=True)
            
            self.logger.info(f"""         
                Initiating Summary e-Mail for Data Steward - {sub_domain}
                
                Data Steward Email Details - Sub Domain:{sub_domain}
                <======================= Persona 2: =======================>
                Summary Length: {len(df_persona_2)}
                columns list: {df_persona_2.columns.to_list()}
                DataFrame: \n{df_persona_2.head(5)}

                <==================== Threshold Based: ====================>
                Summary Length: {len(persona_2_score_based)}
                columns list: {persona_2_score_based.columns.to_list()}
                DataFrame: \n{persona_2_score_based.head(5)}
                <===========================================================>
            """)
            
            persona_group_2_email = self.get_mail_distro(
                df_val=df_mail_distros,
                sub_dmn=sub_domain,
                persona='PERSONA_2',
                default_mail_distro=config.AP_DEFAULT_MAIL_GROUP
            )
            
            self.logger.info(f'Persona 2 (Data Steward) Email Group: {persona_group_2_email}')
            
            ## Send Email - Message Preparation and Email Request
            summary_run_date = datetime.now().strftime("%m-%d-%y")
            summary_run_time = datetime.now().strftime("X%I:%M %p").replace('X0','').replace('X', '').lower()
            email_message = f'Auto Profiling completed successfully for {summary_run_date} at {summary_run_time}. Kindly find the below list.<br>Table List:'

            ## Sending Email - Overall Summary for Data Steward
            if len(df_persona_2) == 0:
                email_message = f'Auto Profiling completed successfully for {summary_run_date} at {summary_run_time}. <p><b>Note: All the tables are above {config.AP_SCORE_CHECK_PCT}% </b></p>'
            
            email_subject = f'Data Steward - Auto Profile Report for {sub_domain}'
            self.email_alert(
                message=email_message,  df=df_persona_2,
                subject=email_subject, receipents_email_dtls=persona_group_2_email
            )
            
            ## Sending Email - Summary for Data Steward - Score less than defined Thresold
            if len(persona_2_score_based) == 0:
                email_message = f'Auto Profiling completed successfully for {summary_run_date} at {summary_run_time}. <p><b>Note: All the tables are above {config.AP_SCORE_CHECK_PCT}% </b></p>'
                
            email_subject = f'Data Steward - Auto Profile Report for {sub_domain} having score less the {config.AP_SCORE_CHECK_PCT}%'
            self.email_alert(
                message=email_message,  df=persona_2_score_based,
                subject=email_subject, receipents_email_dtls=persona_group_2_email
            )
                
        except Exception as e:
            self.logger.error(f'Error Occured in Data Steward Mail Summary. Error Info: {e}')

    ## Previous Execution Score Details - Data Retrieved from the Table
    def prev_day_score(self, df=pd.DataFrame()):
        try:
            if len(df) > 0:
                src_tab_list = df['TABLE_NAME'].to_list()
                rpt_ref_key_list = df['RPT_REF_KEY'].to_list()
                table_details = "', '".join(src_tab_list) if len(src_tab_list) > 0 else ''
                reference_keys = "', '".join(rpt_ref_key_list) if len(rpt_ref_key_list) > 0 else ''
                    
                    
                self.logger.info(f'\nTable name List: {src_tab_list} \nReference Keys: {rpt_ref_key_list}')
                if len(src_tab_list) > 0:   
                    previous_score_query = f"""
                        SELECT 
                        A.PROFILE_ID                 AS ACTUAL_TABLE_ID,
                        A.AUTO_PRFL_NUM               AS PREV_PRFL_NUM,
                        A.RPT_REF_KEY                 AS PREV_RPT_REF_KEY,
                        A.TABLE_NAME                     AS TABLE_NAME,
                        A.PRFL_RUN_DT            AS PREV_DATE,
                        ifnull(TBL_COMPLETENESS, 0)   AS COMPLETENESS_PREV_SCORE,   
                        ifnull(TBL_UNIQUENESS, 0)     AS UNIQUENESS_PREV_SCORE,
                        ifnull(TBL_TIMELINESS, 0)     AS TIMELINESS_PREV_SCORE,
                        ifnull(TBL_CONFORMITY, 0)     AS CONFORMITY_PREV_SCORE,   
                        ifnull(TBL_VALIDITY, 0)       AS VALIDITY_PREV_SCORE,
                        ifnull(TBL_INTEGRITY, 0)      AS INTEGRITY_PREV_SCORE, 
                        ifnull(TBL_CONSISTENCY, 0)    AS CONSISTENCY_PREV_SCORE, 
                        (CASE 
                        WHEN IFNULL(TBL_DQ_SCORE, 0) = 0 THEN 
                            ROUND((ifnull(TBL_COMPLETENESS, 0)  + ifnull(TBL_UNIQUENESS, 0)  +
                            ifnull(TBL_TIMELINESS, 0)  + ifnull(TBL_CONFORMITY, 0)  +
                            ifnull(TBL_VALIDITY, 0)  + ifnull(TBL_INTEGRITY, 0)  +
                            ifnull(TBL_CONSISTENCY, 0)) / 7, 3)
                        ELSE 
                            IFNULL(TBL_DQ_SCORE, 0)
                        END)                          AS PREV_SCORE,

                        ROUND((((ifnull(TBL_COMPLETENESS, 0) + ifnull(TBL_UNIQUENESS, 0) +
                        ifnull(TBL_TIMELINESS, 0)) / 300) * 100), 3) AS  PREV_3_PILLAR_SCORE    
                        FROM   {config.dqaas_auto_prfl_tbl_rpt}   A
                        WHERE  A.TABLE_NAME IN ('{table_details}')
                        AND A.PRFL_RUN_DT = ( SELECT MAX(B.PRFL_RUN_DT)    
                                                        FROM {config.dqaas_auto_prfl_tbl_rpt}   B    
                                                        WHERE A.TABLE_NAME = B.TABLE_NAME    
                                                        AND A.PROFILE_ID = B.PROFILE_ID
                                                        AND B.RPT_REF_KEY NOT IN ('{reference_keys}'))
                        AND EXISTS (SELECT 1 FROM {config.dqaas_auto_prfl_tbl_rpt}   B
                                    WHERE A.TABLE_NAME = B.TABLE_NAME    
                                    AND A.PROFILE_ID = B.PROFILE_ID
                                    AND B.RPT_REF_KEY IN ('{reference_keys}'))
                        ORDER BY A.TABLE_NAME;                
                    """

                    df_prev_day_rpt = self.utils.run_bq_sql(
                        bq_auth=config.dq_gcp_auth_payload,
                        select_query=previous_score_query
                    )
                    
                    self.logger.info(f'''
                    ============================================================================================
                    ## Previous Day Score Details
                    ## Dataframe Length: {len(df_prev_day_rpt)}
                    ## Columns: {df_prev_day_rpt.columns}
                    ## Results: \n{df_prev_day_rpt}
                    ============================================================================================
                    ''')
                        
                    df_comp_summary = df.merge(df_prev_day_rpt, how="left", on=["TABLE_NAME"])
                    
                    self.logger.info(f'''
                    ============================================================================================
                    ## Merged Results
                    ## Dataframe Length: {len(df_comp_summary)}
                    ## Columns: {df_comp_summary.columns}
                    ## Results: \n{df_comp_summary}
                    ============================================================================================
                    ''')
                    
                return df_comp_summary
            else:
                return df
        except Exception as e:
            self.logger.error(f'Error While finding Previous Day Score. Error info: {e}')
            return df 

    ## Email Summary - Main Block
    def populate_end_summary (self, results: list):
        try:
            self.logger.info(f'Records Arrived for Mail summary: {len(results)}')
            
            ## Complete Summary 
            df_completed_table_dtls = pd.DataFrame.from_records(results)
            df_completed_table_dtls['PRFL_RUN_DT'] = pd.to_datetime(df_completed_table_dtls['PRFL_RUN_DT']).dt.date
            df_completed_table_dtls = df_completed_table_dtls.reset_index(drop=True)
            
            ## Sub Domain List for EmaiL
            data_sub_dmn_list = df_completed_table_dtls['DATA_SUB_DMN'].unique().tolist()
            data_sub_dmn = data_sub_dmn_list[0]
            
            ## Merged result of Current and Previous Execution - Score Details 
            # df_completed_table_dtls = prev_day_score(auto_profile_Date, df_completed_table_dtls)
            df_completed_table_dtls = self.prev_day_score(df=df_completed_table_dtls)
            
            ## If Previous Score not available then the missing Previous Score Columns will be added and assigned with NaN - Requried Columns
            for col in config.PREVIOUS_SUMMARY_REQD_COLUMNS:
                if col not in (df_completed_table_dtls.columns).to_list():
                    df_completed_table_dtls[col] = np.nan
            
            for col in config.FLOAT_COLS:
                if col in (df_completed_table_dtls.columns).to_list():
                    df_completed_table_dtls[col] = df_completed_table_dtls[col].fillna(np.nan).astype(float)
                    df_completed_table_dtls[col] = np.where(df_completed_table_dtls[col]>=0, round(df_completed_table_dtls[col], 2), np.nan)

            ## Assigning NaN with default Score Percentage defined in Config File
            df_completed_table_dtls[col] = df_completed_table_dtls[col].fillna(config.AP_SCORE_CHECK_PCT).astype(float)            
                
            ## Getting EmaiL Group Distros for Table
            self.df_mail_distros = self.utils.get_email_distros_from_table(
                data_sub_dmn_list=data_sub_dmn_list
            )
            
            ## Data Steward Email Summary Alert - Persona 3
            self.data_steward_email_summary_alert(
                sub_domain=data_sub_dmn,
                df_completed_table_dtls=df_completed_table_dtls,
                df_mail_distros=self.df_mail_distros
            )
            
            ## DevOps Summary Alert - Persona 3
            self.devops_email_summary_alert(
                sub_domain=data_sub_dmn, 
                df_summary_dtls=df_completed_table_dtls,
                df_email_distro=self.df_mail_distros
            )
            
            ## Commenting below mail process, will be enabled based on confirmation
            ## End summary for postive and failure
            # email_summary_end_report(df_val=df_completed_table_dtls)
            
        except Exception as e:
            self.logger.error(f'Error Occured in Summary for mail. Error Info: {e}')


    #Summary and Other Mails
    def send_email_alert(
        self, sub_domain_name: str = '', message: str = None, subject: str = None,
        df_val=pd.DataFrame(), df_error_val=pd.DataFrame(), receipents_email_group: list = None
    
):
        self.logger.info('-------------------------------------------------------------------------')
        self.logger.info('Email Initiated')
        self.logger.info('-------------------------------------------------------------------------')
        try:

           
            addl_info = f'for {sub_domain_name}' if len(sub_domain_name) > 0 else ''
            subject = f'DQaaS 2.0 Auto Profile Summary {addl_info}' if subject in config.EMPTY_STR_LIST else subject
            message = f'Please find the below Auto Profile Summary {addl_info}' if message in config.EMPTY_STR_LIST else message

            if len(df_error_val) > 0:
                addl_msg = f'<br><b>Auto Profile Error List:</b>{df_error_val.to_html()}'
                message += addl_msg

            if len(df_val) > 0:
                message += '<br><b>Table List:</b><br>'

            receipents_email_addr_list: list = None
            receipents_email_addr_list: list = receipents_email_group if receipents_email_group not in config.EMPTY_STR_LIST else  []
            
            if len(receipents_email_addr_list) == 0:
                self.logger.info(f"Email ID not Found in the Mail Distro Table. Assigning Default mail distro({config.AP_DEFAULT_MAIL_GROUP})")

            receipents_email_addr_list = receipents_email_addr_list + config.AP_DEFAULT_MAIL_GROUP

            self.logger.info(f"Receipents e-Mail Group:{receipents_email_addr_list}")
            
            self.email.send_common_message(
                email_template_filepath=config.common_email_template,
                mail_subject=subject,
                message=message,
                df_val=df_val,
                receipents_email_id=receipents_email_addr_list
            )

            self.logger.info('Email Send Successfully')
        except:
            self.logger.error('Error Occured in email trigger')

    """
    ## Multiprocess - Pool Handler        
    def pool_handler(self, tables_autoprofile=pd.DataFrame()):
        ## pooling_size = 3
        pooling_size = config.AP_POOLING_SIZE
        # If the length of Dataframe is greate than configured pool size, then configured pool size is taken
        pool_size = pooling_size if len(tables_autoprofile) > pooling_size else len(tables_autoprofile)
        pool = Pool(pool_size)
        input = tables_autoprofile.to_dict('records')
        
        result: list = []
        for records in input:
            try:
                self.logger.info(f"Input: {records}")
                get_end_result = pool.apply_async(self.auto_profile, args=(records,))
                self.logger.info(f"Result: {get_end_result}")
                result.append(get_end_result.get())
            except Exception as e:
                self.logger.error('Error Occured. Proceeding with next Iteration.\n Error Info:'+str(e))
                continue
    
        self.logger.info(f'''
        ---------------------------------------
        End Result Length: {len(result)}
        Result : {result}
        ---------------------------------------
        ''')

        pool.close()
        pool.terminate()
        pool.join()
        return result
    """
    
    ## Calls Auto profile Engine
    def profile_engine (self, df_mtd: pd.DataFrame):
        try:
            input_val = df_mtd.to_dict('records')
            profile_result = [self.auto_profile(input_dict=val) for val in input_val]
            self.logger.info(f"Profiled Result: \n {profile_result}")
            return profile_result
        except Exception as err:
            self.logger.error(f'Error Occured. Proceeding with next Iteration.\n Error Info: {err}')
        
        return pd.DataFrame()
     
    def retieve_resulte_from_dict(self, results: list):
        try:
            if len(results) > 0:
                failed_list  = [ val["error_list"] for val in results if (len(val["error_list"]) > 0 and val["error_list"] not in config.EMPTY_STR_LIST)]
                success_list = [ val["end_result"] for val in results if (len(val["end_result"]) > 0 and val["error_list"] not in config.EMPTY_STR_LIST)]
                return failed_list, success_list

        except Exception as err:
            self.logger.error(f'Error While Retrieving the results list .\n Error Info: {err}')
        
        return [], []

    def send_overall_summary_email(self, failure_tbl_list: list, end_results: list, sub_dmn_list: list):
        try:
            if len(failure_tbl_list) > 0 or len(end_results) > 0:
                ## Mail Distro Details
                self.df_mail_distros = self.utils.get_email_distros_from_table(
                    data_sub_dmn_list=sub_dmn_list
                )
                
                ## Sub Domain List
                sub_domain = sub_dmn_list[0] if len(sub_dmn_list) > 0 else ''
                
                ## Receptients Emaill Address - Persona 2
                receptients_mail_addrs = self.get_mail_distro(
                    df_val=self.df_mail_distros,
                    sub_dmn=sub_domain,
                    persona='EMAIL_DISTRO',
                    default_mail_distro=config.AP_DEFAULT_MAIL_GROUP
                )
                
                ## Failed Table List for Email Summary
                df_failure = pd.DataFrame()
                if len(failure_tbl_list) > 0:
                    df_failure = pd.DataFrame.from_records(failure_tbl_list)
                    df_failure = df_failure.reset_index(drop=True)
                    
                ## Profiled Details for Email Summary
                df_completed = pd.DataFrame()
                if len(end_results) > 0:
                    df_completed = pd.DataFrame.from_records(end_results)
                    # df_completed = df_completed.loc[:, df_completed.columns.isin(config.AP_OVERALL_SUMMARY_COLS)]
                    reqd_column_list = [col for col in config.AP_OVERALL_SUMMARY_COLS if col in df_completed.columns.tolist()]
                    df_completed = df_completed[reqd_column_list]
                    
                    for col in config.FLOAT_COLS:
                        if col in (df_completed.columns).to_list():
                            df_completed[col] = df_completed[col].fillna(np.nan).astype(float)
                            df_completed[col] = np.where(df_completed[col]>=0, round(df_completed[col], 2), np.nan)

                    self.logger.info(f"Mail Columns: {df_completed.columns.tolist()}")
                    df_completed = df_completed.rename(columns=config.AUTO_PROFILE_EMAIL_COLUMN_RENAME_LIST)
                    df_completed = df_completed.reset_index(drop=True)
                    
                ## Send Email Summary
                self.send_email_alert(
                    df_error_val=df_failure,    
                    df_val=df_completed,
                    sub_domain_name=sub_domain,
                    receipents_email_group=receptients_mail_addrs
                )

        except Exception as e:
            self.logger.error(f'Error While Sending Overall Email. Error: {e}')
        
    ## Auto Profile - Initiation Block - Pre-Requisite 
    def initiate_auto_profile(self, tables_csv_autoprofile=pd.DataFrame(), data_sub_dmn=[]):
        try:
            tables_csv_autoprofile = tables_csv_autoprofile.drop(columns=['Unnamed: 0'], axis=1, errors='ignore')
            tables_csv_autoprofile = tables_csv_autoprofile.reset_index(drop=True)
            self.logger.info(f'No of Active Tables for Auto Profile is {len(tables_csv_autoprofile)}')

            tables = tables_csv_autoprofile['TABLE_NAME'].to_list()
            no_of_tables = len(tables)
            if no_of_tables <= 0:
                self.logger.info('Tables Not found for auto profiling')
                self.email_alert(message='Tables Not found for auto profiling')
            else:
                self.logger.info('No of tables required for auto profiling is {}'.format(len(tables_csv_autoprofile)))
                
                ## Initiating Auto Profile Engine with multiprocessing
                ## Below Spawning Need to be placed outside Pool Handler
                # set_start_method("spawn",force=True)
                # get_results_list = self.pool_handler(tables_csv_autoprofile)
                get_results_list = self.profile_engine(df_mtd=tables_csv_autoprofile)
                
                """                        
                results_list = []
                if len(get_results_list) > 0:
                    # results_list = [val for val in get_results_list if val not in EMPTY_STR_LIST if len(val) > 0]
                    for val in get_results_list:
                        # if val != None :
                        if val not in config.EMPTY_STR_LIST :
                            if len(val) > 0:
                                results_list.append(val)
                """
                
                failure_list, success_list = self.retieve_resulte_from_dict(results=get_results_list)  
                self.logger.info(f"Failure Table List Count: {len(failure_list)}, Compelete Summary Count: {len(success_list)}")      
                
                if len(success_list) > 0:
                    # self.populate_end_summary(success_list)
                    ## Above Line is commented - Depreciating Devops and Data Steward Mail
                    pass
                else:
                    self.logger.warn("No Records Found for Email Summary")

                ## Send Overall Summary with Failure List and Complete Summary List 
                self.send_overall_summary_email(
                    failure_tbl_list=failure_list,
                    end_results=success_list,
                    sub_dmn_list=data_sub_dmn
                )

        except Exception as e:
            self.logger.error(f'Tables Not found for auto profiling.\n Error Info:{e}')
            self.email_alert(message='Tables Not found for auto profiling')

    ## Converting Runtime Arguments into where clause for metadata  
    def get_addl_condition_using_parser(self, args=None):
        add_condition: str = ''
        data_src: str = ''
        if args:
            
            if args.data_dmn:
                add_condition += f" AND upper(DATA_DMN) in ('{str(args.data_dmn).upper()}') "

            if args.data_sub_dmn:
                add_condition += f" AND upper(DATA_SUB_DMN) in ('{str(args.data_sub_dmn).upper()}') "

            if args.data_bus_elem:
                add_condition += f" AND upper(DATA_BUS_ELEM) in ('{str(args.data_bus_elem).upper()}') "
            
            if args.data_lob:
                add_condition += f" AND upper(DATA_LOB) in ('{str(args.data_lob).upper()}') "
            
            if args.data_src:
                data_src = str(args.data_src).upper()
                add_condition += f" AND upper(DATA_SRC) in ('{data_src}') "
                
            if args.data_src is None and self.data_src is not None:
                data_src = str(self.data_src).upper()	
                add_condition += f" AND upper(DATA_SRC) in ('{data_src}') "
                
            if args.profile_id:
                add_condition += f" AND profile_id in ({args.profile_id}) "
            
            if args.critical_flag_value in ('Y','N'):
                add_condition += f" AND IS_CRITICAL_FLG = '{str(args.critical_flag_value).upper()}' "
        
        return data_src, add_condition

    ## Converting Keyword Arguments into where clause for metadata
    def get_addl_condition_dict(self, kwargs):
        add_condition: str = ''
        data_src: str = ''
        
        kwargs = { k.upper(): v for k, v in kwargs.items()}
        if "DATA_SRC" not in kwargs.keys() and self.data_src is not None:
            kwargs["DATA_SRC"] = self.data_src
            
        for key, value in kwargs.items():
            column_value = ""
            if key in ['DATA_DMN', 'DATA_SUB_DMN', 'DATA_BUS_ELEM', 'DATA_LOB', 'TABLE_NAME', 'DB_NAME', 'DATA_SRC']:
                column_value = "', '".join(config.convert_str_to_list(str(value).upper()))
                add_condition += f" AND upper({key}) IN ('{column_value}') "
                
            if key in ['PROFILE_ID']:
                column_value = ", ".join(config.convert_str_to_list(value))
                add_condition += f""" AND {key} in ({column_value}) """

            if key == 'DATA_SRC':
                data_src = str(value).upper()
        
        return data_src, add_condition     
    
    ## Get the Metadata from table - DataFrame
    def get_metadata(self, add_condition: str) -> pd.DataFrame:
        try:
            metadata_query = f"""
                SELECT * FROM {config.dqaas_auto_prfl_mtd}
                WHERE IS_ACTIVE_FLG = 'Y'    
                {add_condition}
                ORDER BY PROFILE_ID;
            """
            ## AND DATA_SRC = 'TD'
            df_val = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=metadata_query
            )
            df_val = df_val.rename(columns={col: str(col).upper() for col in df_val.columns.tolist()})
            return df_val
        except Exception as err:
            self.logger.error(f'Error Occured While Executing the Metadata Query. Error: {err} ')
        return pd.DataFrame()

    ## Call the Auto Profile Engine - Used in Main Block and Scheduler
    def call_auto_profile_engine(self, df_input: pd.DataFrame):
        """Called inside Table Watcher for Initiating Auto Profile Engine"""
        
        process_id = os.getpid()
        start_mem = psutil.Process(process_id).memory_full_info().uss
        start_time = datetime.now()
        self.logger.info(f'''
        ==================================================================
        #### Initiating Auto Profile Engine
        Process ID      :   {process_id}
        Requested Time  :   {start_time}
        ==================================================================
        ''')
        
        try:
            self.logger.info(f'Total Records Found Profiling: {len(df_input)}')
            if len(df_input) > 0 :
                self.logger.info('Requesting for Auto Profiling............')
                
                summary_run_date = datetime.now().strftime("%m-%d-%y")
                summary_run_time = datetime.now().strftime("X%I:%M %p").replace('X0','').replace('X', '').lower()
                self.email_alert(message=f'Auto Profiling started for {summary_run_date} at {summary_run_time}.')
                
                data_sub_dmn = df_input['DATA_SUB_DMN'].unique().tolist()

                self.initiate_auto_profile(
                    tables_csv_autoprofile=df_input,
                    data_sub_dmn=data_sub_dmn
                )
            else:
                self.logger.error('Tables Not found for auto profiling at the scheduled hour')
                self.email_alert(message='Tables Not found for Auto Profiling at the scheduled hour')

        except Exception as e:
            self.logger.error('Failied to Initiate Auto Profile Engine.\n Error Info: '+ str(e))
            self.email_alert(message='File not Found in the server')
            
        end_mem = psutil.Process(process_id).memory_full_info().uss
        end_time = datetime.now()
        
        self.logger.info(f"""
        ======================================================================================
        Job Process Details:
            Process ID              :   {process_id}
            Started at              :   {start_time}
            Ended by                :   {end_time}
            Total Time Taken        :   {(end_time-start_time)}
            Memory consumed (in MB) :   {(end_mem - start_mem) / (1024 * 1024 * 1024)}
        ======================================================================================
        """)
    
    ## Main Block
    def main(self, parse_val=None, **kwargs):
        try: 
            self.critical_flag_value = None
            self.csv_flag = None
            self.data_sub_dmn = None
            self.input_src_filepath = None
            additional_cond_str = None
            
            ## Runtime Arguments
            val = sys.argv[:1]
            if len(val) == 0 and parse_val is not None:
                val = parse_val
            
            if len(val) > 0:
                parse_args = get_args_parser(parse_val=val)
                self.critical_flag_value = parse_args.critical_flag_value
                self.data_sub_dmn = parse_args.data_sub_dmn
                self.input_src_filepath = parse_args.input_filepath
                self.csv_flag = parse_args.csv_flag
                data_src, additional_cond_str = self.get_addl_condition_using_parser(args=parse_args)
            
            ## Keyword Arguments
            if len(kwargs) > 0:
                self.logger.info(f"Kwargs: {kwargs}")
                data_src, additional_cond_str = self.get_addl_condition_dict(kwargs)
                    
            self.logger.info(f"""\nCSV Flag: {self.csv_flag} \nCritical Flag: {self.critical_flag_value} 
                \nFile Path: {self.input_src_filepath} \nAdditional Condition: {additional_cond_str}
                \nSub Domain: {self.data_sub_dmn}
                """)
        
            if self.csv_flag == 'Y':
                rules = pd.read_csv(self.input_src_filepath)
                
                try:
                    shutil.move(src=self.input_src_filepath, dst=config.ARCHIVE_DIR)
                    print('CSV file moved to Archival path')
                except Exception as err:
                    print(f'CSV file not Archived. Error: {err}')
                        
            else:
                rules = self.get_metadata(add_condition=additional_cond_str)

            self.call_auto_profile_engine(df_input=rules)
            
        except Exception as err:
            self.logger.error(f"Error in Main Block Execution. Error: {err}")
            self.email_alert(message=f'''<p>Error Occured While Auto Profiling.<br><b>Error:</b>{err}</p>''')
