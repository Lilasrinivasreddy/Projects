What is the endpoint, embedding endpoint, cross endpoint? (0:06) Right, right. (0:34) We have this OS.endron (0:40) Sorry, I lost for a while. (1:14) Right. (1:20) Right. (1:28) In the test we have. (1:47) We can refer that one here. (1:50) No need this JSON (1:51) and calling that key and all. (1:54) We don't require this. (1:55) Okay, okay. (1:57) You can refer the previous (1:59) config.py and I will create (2:01) I will modify this (2:03) Elasticsearch one. (2:05) We have this IEMP (2:07) threat manager. (2:10) Let me (2:11) Yeah. (2:37) So what Ashish said, (2:39) but how is the responses? (2:40) Is it too bad than previous one (2:43) or it is like similar? (2:46) No, it's like okay. (2:47) But still couple of questions (2:49) whatever is testing is not getting (2:51) response. I mean wrong (2:53) answer or something. (2:55) But we are getting the (2:58) responses in our (2:59) current. Yeah, yeah. (3:01) With the current we are getting, yes. (3:04) I mean not with the (3:05) current one, with the old (3:07) and not with the (3:08) you are saying it's with the rug, right? (3:10) No, no, not like that. (3:12) So I mean (3:14) see what (3:16) Ashish sometimes he tests (3:18) in different way like which (3:22) so configuration (3:23) for with connectivity (3:25) how to configure like how (3:26) or something he is not giving sometimes. (3:28) So I think that is also sometimes (3:30) causing. (3:32) So that we have to (3:34) test it (3:36) like (3:39) not in a proper (3:41) meaning he is (3:42) giving sometimes. (3:44) Okay. (3:48) But for now he said it is almost (3:51) okay. But for (3:53) few things still we need (3:55) improvement. (3:57) Okay. (4:42) Here I have provided (4:44) three keys but I should avoid this. (4:50) This is for UAT, right? (4:53) Yeah, but I don't need this (4:54) dev API key, UAT API key (4:56) because we already have a secret (4:58) for this API key. (5:00) UAT API key we have already (5:02) secret created here. (5:04) So we don't need that (5:05) these two. I will (5:08) just add only this secret. (5:19) Don't add this directly. (5:22) Okay, I will update this (5:23) and maybe you can (5:25) just use the secret name (5:30) Leela. (5:31) I think for (5:34) Okay. (5:34) Directly we can use the secret name. (5:38) Use the secret name for Elasticsearch (5:41) so os.environ. (5:43) So like that you can use this for (5:44) Elasticsearch. We don't need (5:46) to take like (5:49) key value pair. We don't need to load this (5:50) JSON and take the key value pair. (5:52) Directly I am going to provide this key only (5:54) here. So you can take that (5:57) for Elasticsearch. So (5:58) you do the modifications in the code. (6:00) I will update this here as well. (6:02) Okay, so you will be adding whatever (6:05) Saesh has shared, right? (6:06) That also? (6:08) No, no, no. That is for (6:11) prod, right? (6:13) Prod I cannot add it. (6:14) But I can keep it in the deployment instructions. (6:17) Tomorrow Vegas team will create it (6:19) in the prod. Yeah, I don't have (6:21) access to do that. (6:22) Yeah, yeah. I understand. (6:23) Senthil will be, not Senthil. (6:26) Selvakumar. Selvar team. (6:27) Yeah, yeah. I understand. (6:30) Sure. I will just change. (6:32) Because why I am saying this, (6:34) we have to add it because (6:35) we are going to push the same code to the (6:37) main branch. And based on (6:39) the environment key in the (6:41) prod environment, it will fetch the values (6:44) so that our code should be up to (6:45) date. It should work in both prod (6:47) and the test environment based on (6:49) the environment variable coming in. (6:52) So when the (6:53) ENV is prod, we should have (6:55) that URL should be in the place in the (6:57) dpf.config.py (6:59) and whatever the (7:01) secrets we are going to create in the (7:03) prod, that value (7:05) should be used in the code, right? (7:08) So that's what I am saying. (7:10) Whatever config we kept it for (7:11) the UID and we have to update (7:14) it for prod as well. (7:15) Based on the (7:17) key and everything (7:18) based on that environment. (7:21) You just update it and let (7:22) me know. So I will (7:24) update this instead of JSON (7:26) file upload. I will just directly (7:28) add the key to the secret. (7:31) Sure. I will update it. (7:33) So let me...



import os
import time
import warnings
# from ray import serve
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.encoders import jsonable_encoder
from pydantic import BaseModel
from typing import List, Union, Literal
from config import *
from langchain.retrievers import  EnsembleRetriever
from langchain_community.vectorstores import FAISS

from typing import Any, Dict, List, Optional
import requests
import json

from pydantic.v1 import BaseModel as langchainBaseModel, Extra, Field
from langchain.schema.embeddings import Embeddings
import uvicorn
import numpy as np
from config import EMAS_URL,apikey,url,env
import pickle
from langchain.retrievers import EnsembleRetriever

# Import DPF configuration
from dpf_config import (
    DPF_ELASTICSEARCH_CONFIG, 
    DPF_ENDPOINTS, 
    DPF_API_KEY,
    DPF_UAT_API_KEY, 
    DPF_SEARCH_CONFIG,
    DPF_CROSS_ENCODER_CONFIG,
    DPF_LLM_CONFIG,
    DPF_DOCUMENT_CHUNKING_CONFIG,
    DPF_ADVANCED_SEARCH_CONFIG
)

warnings.filterwarnings("ignore")

app = FastAPI()#docs_url=None, redoc_url=None)

origins = ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class EMAS(langchainBaseModel, Embeddings):

    def __init__(self, **kwargs: Any):
        """Initialize the sentence_transformer."""
        super().__init__(**kwargs)
        # self.EMAS_URL = EMAS_URL

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid

    def get_embed(self, text: str) -> List[float]:
        if env != "prod":
            payload = json.dumps({
                "region": "us-east4",
                "project_id": "688379114786",
                "endpoint_id": "7786939260002631680",
                "usecase_name": "llm-embeddings",
                "model_type": "llm-embeddings",
                    "input_request": {
                        "instances": [
                            {"text": f"{text}"}]
                }
            })

            headers = {
                'Content-Type': 'application/json',
                'X-apikey':apikey
            }
            try:
                response = requests.request(
                    "POST", EMAS_URL, headers=headers, data=payload, verify=False).json()
                result = response[0][0]['embedding']
            except Exception as e:
                print(f"Failure from embedding API. Encountered exception: {e}")
                print(text,response)
                raise
        else:
            payload = json.dumps({
                "region": "us-east4",
                "project_id": "783334890793",
                "endpoint_id": "7414266390837723136",
                "usecase_name": "llm-embeddings",
                "model_type": "llm-embeddings",
                    "input_request": {
                        "instances": [
                            {"text": f"{text}"}]
                }
            })

            headers = {
                'Content-Type': 'application/json',
                'X-apikey':apikey
            }
            try:
                response = requests.request(
                    "POST", EMAS_URL, headers=headers, data=payload, verify=False).json()
                result = response[0][0]['embedding']
            except Exception as e:
                print(f"Failure from embedding API. Encountered exception: {e}")
                print(text,response)
                raise
        return result

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Compute doc embeddings using a HuggingFace transformer model.

        Args:
            texts: The list of texts to embed.

        Returns:
            List of embeddings, one for each text.
        """
        res = []
        for text in texts:
            res.append(self.get_embed(text))

        return res

    def embed_query(self, text: str) -> List[float]:
        """Compute query embeddings using a HuggingFace transformer model.

        Args:
            text: The text to embed.

        Returns:
            Embeddings for the text.
        """
        return self.get_embed(text)


class LLMQuery(BaseModel):
    query: str
    index: Literal["gcp", "bi_tools", "dgs",
                   "ml_platform", "teradata", "all", "hadoop","informatica","data_indus","data_discovery","edw_modernization","ai_indus","ai_workmate"] = "all"

class DPFQuery(BaseModel):
    query: str
    use_case: str # Add default use_case
    index_name: str = DPF_ELASTICSEARCH_CONFIG["index_name"]
    k: int = DPF_SEARCH_CONFIG["default_k"]
    # include_llm_response: bool = True
    # document_types: List[str] = None  # Filter by specific document types
    # enable_hierarchical: bool = True  # Enable document-aware chunking


st = time.time()
faiss_embeddings=EMAS()
# print(type(faiss_embeddings))



db_dict = {
    'all': {'faiss': FAISS.load_local(FAISS_ALL_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "gcp": {'faiss': FAISS.load_local(FAISS_GCP_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "bi_tools": {'faiss': FAISS.load_local(FAISS_BI_TOOLS_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "dgs": {'faiss': FAISS.load_local(FAISS_DGS_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "ml_platform": {'faiss': FAISS.load_local(FAISS_ML_PLATFORM_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "teradata": {'faiss': FAISS.load_local(FAISS_TERADATA_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "hadoop": {'faiss': FAISS.load_local(FAISS_HADOOP_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "informatica": {'faiss': FAISS.load_local(FAISS_INFORMATICA_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "data_indus": {'faiss': FAISS.load_local(FAISS_DATA_INDUS_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "data_discovery": {'faiss': FAISS.load_local(FAISS_DATA_DISCOVERY_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "edw_modernization": {'faiss': FAISS.load_local(FAISS_EDW_MODERNIZATION_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "ai_indus": {'faiss': FAISS.load_local(FAISS_AI_INDUS_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "ai_workmate": {'faiss': FAISS.load_local(FAISS_AI_WORKMATE_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)}
}

et = time.time() - st
# print(f'Loading database took {et} seconds.')

async def reranker(reranklist):
    try:
        if env!= "prod":
            headers={
                'Content-Type': 'application/json',
                'X-apikey':apikey
            }
            payload = json.dumps({
            "region": "us-east4",
            "project_id": "688379114786",
            "endpoint_id": "4179014998757998592",
            "usecase_name": "cross-encoder",
            "model_type": "cross-encoder",
            "input_request": {
                "instances": reranklist
            }
        })
        else:
            headers={
                'Content-Type': 'application/json',
                'X-apikey':apikey
            }
            payload = json.dumps({
            "region": "us-east4",
            "project_id": "783334890793",
            "endpoint_id": "6356118390498656256",
            "usecase_name": "cross-encoder",
            "model_type": "cross-encoder",
            "input_request": {
                "instances": reranklist
            }
        })
        response = requests.request(
                "POST", url, headers=headers, data=payload, verify=False).json()
        
        return response
    except Exception as e:
        print(f"Failure from reranker API. Encountered exception: {e}")
        raise



async def faiss_search(faiss_db, request):
    st = time.time()
    bm25_retriever=[]
    #retriever = faiss_db.as_retriever()
    #faiss_results = retriever.invoke(request.query,search_kwargs={"k": 5})
    bm25path=f'indexes/{request.index}/{request.index}_bm25_retriever.pkl'
    with open(bm25path,'rb') as file:
        bm25_retriever = pickle.load(file)
        bm25_retriever.k=5
    

    faiss_retriever = faiss_db.as_retriever(search_kwargs={"k":5})
    ensemble_retriever=EnsembleRetriever(retrievers=[bm25_retriever,faiss_retriever],
                                       weights=[0.5,0.5])
    #faiss_results=faiss_db.similarity_search(request.query)
    faiss_results=ensemble_retriever.invoke(request.query)
    # print(faiss_results)
    et = time.time() - st
    print(f'retrieval time {et} seconds.')
    st = time.time()
    reranker_list=[]
    source_documents=[]
    cross_encoder_updated=[]
    if isinstance(faiss_results, list) and len(faiss_results) > 0:
        for j in faiss_results:
            templist=[]
            templist.append(request.query)
            templist.append(j.page_content)
            reranker_list.append(templist)

            
        rerank_response=await reranker(reranker_list)
        #print("rerank_response: ",rerank_response)
        rerank_score =np.array(rerank_response[0][0]["prediction_scores"])
        K=4
        rerank_score_sorted = np.argsort(-rerank_score)[:K]
        et = time.time() - st
        print(f'cross encoder time {et} seconds.')
        print("rerank_score_sorted: ",rerank_score_sorted)
        cross_encoder_updated = [faiss_results[i] for i in rerank_score_sorted]
        for i in cross_encoder_updated:
            if i.metadata["source"] not in source_documents:
                source_documents.append(i.metadata["source"])

    return {"llm_context":cross_encoder_updated,"source_document":source_documents}

# @app.post("/vegas/apps/aider-retriever/api")
# async def search( request: LLMQuery):
#     try:
#         print("--------------------------------------------------------")
#         print(f"Index:{request.index} and query:{request.query}")
#         print("env: ",env)
#         print("EMAS_URL: ",EMAS_URL)
#         default_db = db_dict.get(request.index)
#         # print(default_db)
#         #search_type = request.search

#         final_result = {"faiss": []}
#         # faiss
#         faiss_db = default_db['faiss']
#         # print(faiss_db.index.ntotal)
#         final_result['faiss'] = await faiss_search(faiss_db, request)

#         return final_result
#     except Exception as e:
#         print(e)
#         return {"error": str(e)}

@app.post("/vegas/apps/aider-retriever/dpf/api")
async def dpf_search_endpoint(request: DPFQuery):
    """
    DPF search endpoint using Elasticsearch with guaranteed consistent responses
    """
    try:
        start_time = time.time()
        result = await dpf_search(request)
        total_time = time.time() - start_time
        
        result["search_metadata"]["total_pipeline_time"] = total_time
        return result
    except Exception as e:
        return {"error": "Unable to process your request at this time. Please try again."}

@app.get("/vegas/apps/aider-retriever/dpf/health")
async def dpf_health_check():
    """
    Health check endpoint for DPF service
    """
    try:
        # Test Elasticsearch connectivity
        test_query = {
            "query": {"match_all": {}},
            "size": 1
        }
        
        es_result = await search_elasticsearch(
            DPF_ELASTICSEARCH_CONFIG["api_url"], 
            DPF_ELASTICSEARCH_CONFIG["api_key"], 
            DPF_ELASTICSEARCH_CONFIG["index_name"], 
            test_query
        )
        
        return {
            "status": "healthy",
            "elasticsearch": "connected",
            "index_name": DPF_ELASTICSEARCH_CONFIG["index_name"],
            "total_documents": es_result.get("hits", {}).get("total", {}).get("value", 0),
            "timestamp": time.time()
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": time.time()
        }

async def search_elasticsearch(api_url: str, api_key: str, index_name: str, query: dict):
    """
    Retrieve chunks from an Elasticsearch index based on a query.
    """
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"ApiKey {api_key}"
    }

    search_url = f"{api_url}/{index_name}/_search"
    
    try:
        response = requests.post(search_url, headers=headers, data=json.dumps(query))
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"Error querying Elasticsearch: {e}")
        raise HTTPException(status_code=500, detail="Unable to search documents at this time")

async def generate_dpf_embedding(text: str):
    """
    Generate embedding for DPF using the embedding endpoint
    """
    headers = {
        'Content-Type': 'application/json',
        'X-apikey': DPF_UAT_API_KEY  # Use DEV/UAT API key 
    }
    
    # # Use the payload format 
    payload = {
        "region": "us-east4",
        "project_id": "688379114786",
        "endpoint_id": "7786939260002631680",
        "usecase_name": "llm-embeddings",
        "model_type": "llm-embeddings",
        "input_request": {
            "instances": [
                {"text": text}
            ]
        }
    }
    
    try:
        response = requests.post(
            DPF_ENDPOINTS["embedding_endpoint"], 
            headers=headers, 
            data=json.dumps(payload),
            verify=False  # Added 
        )
        print(f"Embedding API Response Text: {response.text}")
        response.raise_for_status()
        result = response.json()
        
        # For the simple endpoint, the response is directly a list of floats
        if isinstance(result, list) and all(isinstance(x, (int, float)) for x in result):
            return result
        
        # Fallback extraction methods for complex response formats
        if isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], list) and len(result[0]) > 0:
                if isinstance(result[0][0], dict) and 'embedding' in result[0][0]:
                    return result[0][0]['embedding']
        
        if isinstance(result, dict) and 'embedding' in result:
            return result['embedding']
        elif isinstance(result, list) and len(result) > 0:
            # If it's a list of embeddings, take the first one
            if isinstance(result[0], list):
                return result[0]
            elif isinstance(result[0], dict) and 'embedding' in result[0]:
                return result[0]['embedding']
            else:
                return result
        elif isinstance(result, list):
            return result
        else:
            print(f"Unexpected embedding response format: {result}")
            raise HTTPException(status_code=500, detail="Invalid embedding response format")
            
    except requests.exceptions.RequestException as e:
        print(f"Error generating embedding: {e}")
        raise HTTPException(status_code=500, detail="Unable to process your query at this time")

async def dpf_reranker(sentence_pairs: List[List[str]]):
    """
    Rerank results using cross-encoder for DPF
    """
    headers = {
        'Content-Type': 'application/json', 
        'X-apikey': DPF_UAT_API_KEY 
    }
    
    payload = {
        "region": DPF_CROSS_ENCODER_CONFIG["region"],
        "project_id": DPF_CROSS_ENCODER_CONFIG["project_id"],
        "endpoint_id": DPF_CROSS_ENCODER_CONFIG["endpoint_id"],
        "usecase_name": DPF_CROSS_ENCODER_CONFIG["usecase_name"],
        "model_type": DPF_CROSS_ENCODER_CONFIG["model_type"],
        "input_request": {
            "instances": sentence_pairs
        }
    }
    print("reranker")
    try:
        response = requests.post(
            DPF_ENDPOINTS["cross_encoder_endpoint"], 
            headers=headers, 
            data=json.dumps(payload)
        )
        response.raise_for_status()
        result = response.json()
        
        # Handle different response formats
        if isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], list) and len(result[0]) > 0:
                if isinstance(result[0][0], dict) and "prediction_scores" in result[0][0]:
                    return result
                else:
                    # If the structure is different, wrap it properly
                    return [[{"prediction_scores": result[0] if isinstance(result[0], list) else [result[0]]}]]
            else:
                return [[{"prediction_scores": [0.0] * len(sentence_pairs)}]]
        elif isinstance(result, dict) and 'predictions' in result:
            return result['predictions']
        else:
            print(f"Unexpected reranker response format: {result}")
            return [[{"prediction_scores": [0.0] * len(sentence_pairs)}]]
            
    except requests.exceptions.RequestException as e:
        print(f"Error in reranking: {e}")
        # Return default scores on error
        return [[{"prediction_scores": [0.0] * len(sentence_pairs)}]]

async def generate_llm_response(user_query: str, context, use_case: str):
    """
    Generate LLM response using with Gemini fallback 
    """
    # Check cache first for absolute consistency
    # cache_key = get_llm_cache_key(user_query, context)
    # if cache_key in RESPONSE_CACHE:
    #     return RESPONSE_CACHE[cache_key]

    primary_response = await _call_llm_api(user_query, context, use_case)
    print("primary",primary_response)
    return primary_response
    # Check if primary response is unhelpful and fallback is enabled
    # is_unhelpful = _is_unhelpful_response(primary_response)
    
#     if DPF_LLM_CONFIG["enable_gemini_fallback"] and is_unhelpful:
#         print(f"Primary LLM gave unhelpful response, trying Gemini fallback")
#         print(f"Primary response: {primary_response[:100]}")
#         # Create enhanced context for fallback ---use any prompts 
#         enhanced_context = f"""
# Based on the available information and general knowledge, please help answer this question: {user_query}

# Context from documents:
# {context}

# Instructions: If the exact answer isn't in the documents, provide a helpful general explanation based on common knowledge, industry practices, or similar concepts. Be helpful and informative even if the specific term isn't mentioned in the provided documents.
# """
        
#         gemini_response = await _call_llm_api_enhanced_with_different_usecase(
#             user_query, 
#             enhanced_context, 
#             DPF_LLM_CONFIG["gemini_fallback_usecase"],
#             DPF_LLM_CONFIG["gemini_fallback_context_id"]
#         )
        
#         if not _is_unhelpful_response(gemini_response):
#             print(f"Gemini fallback successful: {gemini_response[:100]}...")
#             # Cache and return successful Gemini response
#             RESPONSE_CACHE[cache_key] = gemini_response
#             save_response_cache()
#             return gemini_response
#         else:
#             print(f"Gemini also gave unhelpful response: {gemini_response[:100]}...")
            
#             # Try one more fallback with knowledge-based approach
#             print("Trying knowledge-based fallback...")
#             knowledge_response = await _generate_knowledge_based_response(user_query)
#             if knowledge_response and not _is_unhelpful_response(knowledge_response):
#                 print(f"Knowledge-based fallback successful: {knowledge_response[:100]}...")
#                 RESPONSE_CACHE[cache_key] = knowledge_response
#                 save_response_cache()
#                 return knowledge_response
    
#     # Cache and return primary response (whether helpful or not)
#     RESPONSE_CACHE[cache_key] = primary_response
#     save_response_cache()
#     print(f"Final LLM Response: {primary_response[:200]}...")
    

async def _call_llm_api(user_query: str, context: str, use_case: str):
    """
    Helper function to call LLM API with specified context ID
    """
    
    headers = {
        'Content-Type': 'application/json', 
        'X-apikey': DPF_UAT_API_KEY  # Use UAT API key for LLM
    }
    
    # usecases and contextID's
    if use_case == "aider":
        payload = {
            "useCase": "AIDER",
            "contextId": "aider_context",
            "preSeed_injection_map": {
                "{CHUNK_INPUT}": context,  # Used CHUNK_INPUT recent changes as discussed with team
                "{QUESTION}": user_query
            },
            "parameters": {
                "maxOutputTokens": DPF_LLM_CONFIG["max_output_tokens"]
            }
        }
    elif use_case == "dpf-agentic-copilot":
        payload = {
            "useCase": "dpf-agentic-copilot-prompt",
            "contextId": "copilot-rag",
            "preSeed_injection_map": {
                DPF_LLM_CONFIG["context_placeholder"]: context,
                DPF_LLM_CONFIG["question_placeholder"]: user_query
            },
            "parameters": {
                "maxOutputTokens": DPF_LLM_CONFIG["max_output_tokens"]
            }
        }
    elif use_case == "ai_workmate":
        payload = {
            "useCase": "AIDER",
            "contextId": "aiworkmate_context",
            "preSeed_injection_map": {
                "{CHUNK_INPUT}": context,  # Used CHUNK_INPUT recent changes as discussed with team
                "{QUESTION}": user_query
            },
            "parameters": {
                "maxOutputTokens": DPF_LLM_CONFIG["max_output_tokens"]
            }
        }
    else:
        # Default fallback
        payload = {
            "useCase": "AIDER",
            "contextId": "aider_context", 
            "preSeed_injection_map": {
                "{CHUNK_INPUT}": context,
                "{QUESTION}": user_query
            }
        }
    print("payload",payload)
    try:
        response = requests.post(
            DPF_ENDPOINTS["llm_endpoint"], 
            headers=headers, 
            data=json.dumps(payload)
        )
        response.raise_for_status()
        result = response.json()
        
        # Extract response from result - handle different response formats
        llm_response = None
        if isinstance(result, dict):
            if 'prediction' in result:
                llm_response = result['prediction']
            elif 'response' in result:
                llm_response = result['response']
            elif 'text' in result:
                llm_response = result['text']
            elif 'generated_text' in result:
                llm_response = result['generated_text']
        elif isinstance(result, str):
            llm_response = result
        elif isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], dict):
                llm_response = result[0].get('prediction', result[0].get('response', str(result[0])))
            else:
                llm_response = str(result[0])
        
        if llm_response is None:
            llm_response = "I apologize, but I encountered an issue generating a response. Please try again."
        
        # Cache the response immediately for consistency
        # RESPONSE_CACHE[cache_key] = llm_response
        # save_response_cache()
        print(f"LLM Response Generated: {llm_response[:200]}...")
        return llm_response
            
    except requests.exceptions.RequestException as e:
        error_response = f"I apologize, but I'm experiencing technical difficulties. Please try again in a moment."
        # # Cache error responses for consistency
        # RESPONSE_CACHE[cache_key] = error_response
        # save_response_cache()
        # print(f"LLM Error Response: {error_response}")
        print(f"LLM API Error with context {context_id}: {e}")
        print(f"Request URL: {DPF_ENDPOINTS['llm_endpoint']}")
        print(f"Request payload: {json.dumps(payload, indent=2)}")
        print(f"Response status code: {getattr(e.response, 'status_code', 'N/A') if hasattr(e, 'response') else 'N/A'}")
        if hasattr(e, 'response') and e.response:
            print(f"Response text: {e.response.text}")
        return error_response

async def _call_llm_api_enhanced(user_query: str, enhanced_context: str, context_id: str):
    """
    Enhanced LLM API call with modified prompt strategy for fallback
    """
    headers = {
        'Content-Type': 'application/json', 
        'X-apikey': DPF_UAT_API_KEY #update key
    }
    
    # Use a different prompt structure for enhanced fallback
    payload = {
        "useCase": DPF_LLM_CONFIG["usecase"],
        "contextId": context_id,
        "preSeed_injection_map": {
            DPF_LLM_CONFIG["context_placeholder"]: enhanced_context,
            DPF_LLM_CONFIG["question_placeholder"]: "Please provide a helpful response based on the context and instructions above."
        }
    }
    
    try:
        response = requests.post(
            DPF_ENDPOINTS["llm_endpoint"], 
            headers=headers, 
            data=json.dumps(payload)
        )
        response.raise_for_status()
        result = response.json()
        
        # Extract response from result - handle different response formats
        llm_response = None
        if isinstance(result, dict):
            if 'prediction' in result:
                llm_response = result['prediction']
            elif 'response' in result:
                llm_response = result['response']
            elif 'text' in result:
                llm_response = result['text']
            elif 'generated_text' in result:
                llm_response = result['generated_text']
        elif isinstance(result, str):
            llm_response = result
        elif isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], dict):
                llm_response = result[0].get('prediction', result[0].get('response', str(result[0])))
            else:
                llm_response = str(result[0])
        
        if llm_response is None:
            llm_response = "I apologize, but I encountered an issue generating a response. Please try again."
        
        return llm_response
            
    except requests.exceptions.RequestException as e:
        error_response = f"I apologize, but I'm experiencing technical difficulties. Please try again in a moment."
        print(f"Enhanced LLM API Error with context {context_id}: {e}")
        print(f"Request URL: {DPF_ENDPOINTS['llm_endpoint']}")
        print(f"Request payload: {json.dumps(payload, indent=2)}")
        print(f"Response status code: {getattr(e.response, 'status_code', 'N/A') if hasattr(e, 'response') else 'N/A'}")
        if hasattr(e, 'response') and e.response:
            print(f"Response text: {e.response.text}")
        return error_response

async def _call_llm_api_enhanced_with_different_usecase(user_query: str, enhanced_context: str, usecase: str, context_id: str):
    """
    Enhanced LLM API call with different usecase and context for Gemini fallback
    """
    headers = {
        'Content-Type': 'application/json', 
        'X-apikey': DPF_UAT_API_KEY
    }
    
    # Use different usecase and context for Gemini fallback
    payload = {
        "useCase": usecase,  # Use AIDER instead of COMMON_RAG
        "contextId": context_id,  # Use aider_gemini instead of GENERIC
        "preSeed_injection_map": {
            DPF_LLM_CONFIG["context_placeholder"]: enhanced_context,
            DPF_LLM_CONFIG["question_placeholder"]: "Please provide a helpful response based on the context and instructions above."
        }
    }
    
    try:
        response = requests.post(
            DPF_ENDPOINTS["llm_endpoint"], 
            headers=headers, 
            data=json.dumps(payload)
        )
        response.raise_for_status()
        result = response.json()
        
        # Extract response from result - handle different response formats
        llm_response = None
        if isinstance(result, dict):
            if 'prediction' in result:
                llm_response = result['prediction']
            elif 'response' in result:
                llm_response = result['response']
            elif 'text' in result:
                llm_response = result['text']
            elif 'generated_text' in result:
                llm_response = result['generated_text']
        elif isinstance(result, str):
            llm_response = result
        elif isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], dict):
                llm_response = result[0].get('prediction', result[0].get('response', str(result[0])))
            else:
                llm_response = str(result[0])
        
        if llm_response is None:
            llm_response = "I apologize, but I encountered an issue generating a response. Please try again."
        
        return llm_response
            
    except requests.exceptions.RequestException as e:
        error_response = f"I apologize, but I'm experiencing technical difficulties. Please try again in a moment."
        print(f"Gemini LLM API Error with usecase {usecase} and context {context_id}: {e}")
        print(f"Request URL: {DPF_ENDPOINTS['llm_endpoint']}")
        print(f"Request payload: {json.dumps(payload, indent=2)}")
        print(f"Response status code: {getattr(e.response, 'status_code', 'N/A') if hasattr(e, 'response') else 'N/A'}")
        if hasattr(e, 'response') and e.response:
            print(f"Response text: {e.response.text}")
        return error_response

def _is_unhelpful_response(response: str) -> bool:
    """
    Check if LLM response is unhelpful and should trigger fallback
    """
    if not response:
        return True
        
    # Use configured unhelpful phrases
    unhelpful_phrases = DPF_LLM_CONFIG["unhelpful_phrases"]
    print(f"Checking response against {len(unhelpful_phrases)} unhelpful phrases")
    
    response_lower = response.lower()
    for phrase in unhelpful_phrases:
        if phrase.lower() in response_lower:
            print(f"Found unhelpful phrase: '{phrase}'")
            return True
    
    # Also check if response is too short (likely unhelpful)
    if len(response.strip()) < DPF_LLM_CONFIG["min_response_length"]:
        print(f"Response too short: {len(response.strip())} < {DPF_LLM_CONFIG['min_response_length']}")
        return True
        
    print(f"Response seems helpful (length: {len(response.strip())})")
    return False

async def dpf_search(request: DPFQuery):
    """
    Complete DPF search pipeline with enhanced caching for 100% consistency
    """
    # Generate comprehensive cache key including all parameters that affect results
    # search_cache_key = get_query_cache_key(
    #     request.query, 
    #     request.index_name,
    #     request.use_case,
    #     request.k, 
    #     request.include_llm_response,
    #     request.enable_hierarchical
    # )
    # # print("search_cache_key",search_cache_key)
    # # Check cache first for absolute consistency
    # if search_cache_key in RESPONSE_CACHE:
    #     cached_result = RESPONSE_CACHE[search_cache_key].copy()
        
    #     # Ensure response format matches current request
    #     if not request.include_llm_response and 'llm_response' in cached_result:
    #         cached_result['llm_response'] = None
    #     elif request.include_llm_response and not cached_result.get('llm_response'):
    #         # Generate LLM response for cached results if needed
    #         if cached_result.get('reranked_results'):
    #             top_doc = cached_result['reranked_results'][0]
    #             # source_links = cached_result.get('source_links', [])
                
    #             base_llm_response = await generate_llm_response(request.query, top_doc['text'])
                
    #             # if source_links:
    #             #     llm_response = f"{base_llm_response}\n\n**Sources:**\n"
    #             #     for i, source_info in enumerate(source_links, 1):
    #             #         llm_response += f"[{i}] {source_info['url']}\n"
    #             # else:
    #             llm_response = base_llm_response
                
    #             cached_result['llm_response'] = llm_response
        
    #     return cached_result
    
    # 1. Generate embedding with error handling
    embedding_start = time.time()
    try:
        embedding = await generate_dpf_embedding(request.query)
    except Exception as e:
        error_result = {
            "retrieved_documents": [],
            "reranked_results": [],
            "llm_response": "Unable to process your query at this time. Please try again.",
            "source_links": [],
            "search_metadata": {
                "embedding_time": 0,
                "search_time": 0,
                "rerank_time": 0,
                "llm_time": 0,
                "total_results": 0,
                "reranked_count": 0,
                "sources_found": 0
            }
        }
        # RESPONSE_CACHE[search_cache_key] = error_result.copy()
        # save_response_cache()
        return error_result
        
    embedding_time = time.time() - embedding_start

    # Set up dynamic filter clause based on use case
    filter_clause = None
    if hasattr(request, 'use_case') and request.use_case == "aider":
        filter_clause = [{ "term": { "usecase_name": "aider" } }]
        print(f"Using aider filter: {filter_clause}")
    if hasattr(request, 'use_case') and request.use_case == "dpf-agentic-copilot":
        filter_clause = [
                { "term": { "usecase_name": "aider_csv" } },
                { "term": { "usecase_name": "aider_pdf" } }
            # "term": { "usecase_name": "aider_json" }
        ]
        print(f"Using copilot filter: {filter_clause}")
    if hasattr(request, 'use_case') and request.use_case == "ai_workmate":
        filter_clause = [
                { "term": { "usecase_name": "aider_ai_workmate" } }       
        ]
        print(f"Using ai_workmate filter: {filter_clause}")
    if filter_clause is None:
        # Default filter for aider
        filter_clause = [{ "term": { "usecase_name": "aider" } }]    
        print(f"Using default aider filter: {filter_clause}")
    
    # 2. Elasticsearch query 
    es_query = {
        "_source": [DPF_SEARCH_CONFIG["chunk_data_field"], DPF_SEARCH_CONFIG["custom_metadata_field"]],
        "query": {
            "bool": {
                "filter": [
                    {
                        "bool": {
                            "should": filter_clause,
                            "minimum_should_match": 1
                        }
                    }
                ],
                "should": [
                    {
                        "knn": {
                            "field": DPF_SEARCH_CONFIG["embedding_field"],
                            "query_vector": embedding,
                            "k": min(request.k, DPF_SEARCH_CONFIG["max_k"]),
                            "num_candidates": DPF_SEARCH_CONFIG["num_candidates"]
                        }
                    }
                ]
            }
        },
        "size": min(request.k, DPF_SEARCH_CONFIG["max_k"])
    }
    # print("es_query",es_query)
    search_start = time.time()
    try:
        es_results = await search_elasticsearch(
            DPF_ELASTICSEARCH_CONFIG["api_url"], 
            DPF_ELASTICSEARCH_CONFIG["api_key"], 
            request.index_name, 
            es_query
        )
    except Exception as e:
        error_result = {
            "retrieved_documents": [],
            "reranked_results": [],
            "llm_response": "Unable to search documents at this time. Please try again.",
            "source_links": [],
            "search_metadata": {
                "embedding_time": embedding_time,
                "search_time": 0,
                "rerank_time": 0,
                "llm_time": 0,
                "total_results": 0,
                "reranked_count": 0,
                "sources_found": 0
            }
        }
        RESPONSE_CACHE[search_cache_key] = error_result
        return error_result
        
    search_time = time.time() - search_start
    elastic_search_results = es_results['hits']['hits']
    elastic_search_results = sorted(elastic_search_results, 
                                   key=lambda x: (-x.get('_score', 0), x.get('_id', '')))
    print("elastic_search_results",elastic_search_results)
    # Use enhanced document
    # elastic_search_contexts = await get_document_context(elastic_search_results, request.query)
    elastic_search_contexts = [contexts.get('_source').get('original_content') for contexts in elastic_search_results]
    # if not elastic_search_contexts:
    #     no_results = {
    #         "retrieved_documents": [],
    #         "reranked_results": [],
    #         "llm_response": "No relevant documents found for your query.",
    #         "source_links": [],
    #         "search_metadata": {
    #             "embedding_time": embedding_time,
    #             "search_time": search_time,
    #             "rerank_time": 0,
    #             "llm_time": 0,
    #             "total_results": 0,
    #             "reranked_count": 0,
    #             "sources_found": 0
    #         }
    #     }
    #     RESPONSE_CACHE[search_cache_key] = no_results
    #     return no_results
    
    # 3. Rerank results with error handling
    rerank_start = time.time()
    sentence_pairs = [[request.query, context] for context in elastic_search_contexts]
    print("sentence_pairs",sentence_pairs)
    try:
        reranked_response = await dpf_reranker(sentence_pairs)
        print("reranked_response",reranked_response)
    except Exception as e:
        # Use original order if reranking fails
        reranked_response = [[{"prediction_scores": [0.5] * len(sentence_pairs)}]]
        
    rerank_time = time.time() - rerank_start
    cross_scores = reranked_response[0][0].get("prediction_scores", [])
    print("cross_scores",cross_scores)
    # Combine scores with contexts
    context_list = []
    for score, context in zip(cross_scores, elastic_search_results):
        reranked_dict = {}
        reranked_dict['cross_score'] = score
        reranked_dict['text'] = context
        context_list.append(reranked_dict)
        
    # Sort by cross-encoder 
    reranked_docs = sorted(context_list, key=lambda x: x["cross_score"], reverse=True)

    top_doc = top_doc = max(reranked_docs, key=lambda x: x['cross_score'])
    
    llm_response = None
    llm_time = 0
    source_links = []
    all_content = []
    all_content = [doc['text'] for doc in reranked_docs]
    print("all_content",all_content)
    base_llm_response = await generate_llm_response(request.query, all_content, request.use_case)
    llm_response = base_llm_response
    # if request.include_llm_response and top_doc:
    #     llm_start = time.time()
    #     print("entering _llm")
    #     # Extract source links from top documents
    #     # source_links = await extract_source_links(reranked_docs, top_k=1)
    #     # print(f"Source Links Extracted: {len(source_links)} links found")
    #     # for i, link in enumerate(source_links):
    #         # print(f"  Link {i+1}: {link['url']} (score: {link['score']:.4f})")
        
    #     # LLM response
    #     all_content = [doc['text'] for doc in reranked_docs]
    #     print("all_content",all_content)
    #     # if request.use_case == "dpf-agentic-copilot":
    #     base_llm_response = await generate_llm_response(request.query, all_content, request.use_case)
    #     llm_response = base_llm_response
    #     # else:
    #     #     # combined_context = "\n\n".join(all_content)
    #     #     # base_llm_response = await generate_llm_response(request.query, combined_context, request.use_case)
    #     #     # top_doc_context = top_doc['text']
    #     #     base_llm_response = await generate_llm_response(request.query, all_content, request.use_case)
    #     # llm_time = time.time() - llm_start
    print(f"Final LLM Response (source): {llm_response[:300]}...")
    # Prepare final result
    result = {
        "retrieved_documents": elastic_search_results,
        "reranked_results": reranked_docs,
        "top_document": top_doc,
        "llm_response": llm_response,
        # "source_links": source_links,
        "search_metadata": {
            "embedding_time": embedding_time,
            "search_time": search_time,
            "rerank_time": rerank_time,
            "llm_time": llm_time,
            "total_results": len(elastic_search_results),
            "reranked_count": len(reranked_docs),
            # "sources_found": len(source_links)
        }
    }
    
    # RESPONSE_CACHE[search_cache_key] = result
    return result

async def detect_document_type(es_result):
    """
    Detect document type from Elasticsearch result for chunking strategy
    """
    source = es_result.get('_source', {})
    
    # Check for document type indicators in metadata
    if 'file_extension' in source:
        ext = source['file_extension'].lower()
        if ext == 'pdf':
            return 'pdf'
    
    if 'source_system' in source:
        system = source['source_system'].lower()
        if 'confluence' in system:
            return 'confluence'
        elif 'jira' in system:
            return 'jira'
        elif 'sharepoint' in system:
            return 'sharepoint'
    
    if 'document_type' in source:
        return source['document_type'].lower()
    
    return 'default'

# async def get_document_context(es_results, query):
#     """
#     Enhanced document context retrieval with hierarchical chunking
#     """
#     if not DPF_ADVANCED_SEARCH_CONFIG["enable_document_context"]:
#         # Fallback to original simple extraction
#         return [result.get('_source', {}).get(DPF_SEARCH_CONFIG["chunk_data_field"], '') 
#                 for result in es_results]
    
#     enhanced_contexts = []
    
#     # Process each result directly since they're already individual chunks
#     for result in es_results:
#         source = result.get('_source', {})
        
#         # Get the main content using the configured field name
#         main_content = source.get(DPF_SEARCH_CONFIG["chunk_data_field"], '')
        
#         # If the main field is empty, try alternative field names
#         if not main_content:
#             main_content = (source.get('original_content', '') or 
#                            source.get('chunk_data', '') or 
#                            source.get('content', '') or 
#                            source.get('text', ''))
        
#         # Add the content (with minimal metadata if needed)
#         if main_content:
#             enhanced_contexts.append(main_content)
#         else:
#             # Fallback: combine all available text fields
#             all_text = []
#             for key, value in source.items():
#                 if isinstance(value, str) and len(value) > 10:  # Reasonable content length
#                     all_text.append(value)
#             enhanced_contexts.append(' '.join(all_text) if all_text else '')
    
#     return enhanced_contexts

# Source link extraction disabled by Lila
# async def extract_source_links(reranked_docs, top_k=1):
#     """
#     Extract source links from reranked documents
#     """
#     source_links = []
#     seen_links = set()
    
#     # Extract from top K documents to provide multiple sources
#     for doc in reranked_docs[:top_k]:
#         if 'original_result' in doc:
#             source_data = doc['original_result'].get('_source', {})
#             custom_metadata = source_data.get(DPF_SEARCH_CONFIG["custom_metadata_field"], {})
            
#             source_link = None
#             if isinstance(custom_metadata, dict):
#                 source_link = (custom_metadata.get('source_link') or
#                              custom_metadata.get('source_url') or 
#                              custom_metadata.get('url') or 
#                              custom_metadata.get('link') or 
#                              custom_metadata.get('source') or
#                              custom_metadata.get('document_url') or
#                              custom_metadata.get('page_url'))
#             elif isinstance(custom_metadata, str) and custom_metadata.strip():
#                 source_link = custom_metadata.strip()
            
#             # Check other potential source fields in the main document
#             if not source_link:
#                 source_link = (source_data.get('source_url') or 
#                              source_data.get('url') or 
#                              source_data.get('source') or
#                              source_data.get('document_url'))
            
#             # Add unique source links
#             if source_link and source_link not in seen_links:
#                 source_links.append({
#                     'url': source_link,
#                     'score': doc.get('cross_score', 0),
#                     'preview': doc.get('text', '')[:100] + '...' if doc.get('text') else ''
#                 })
#                 seen_links.add(source_link)
#                 print(f"Added source link: {source_link} (score: {doc.get('cross_score', 0):.4f})")
    
#     print(f"Total unique source links found: {len(source_links)}")
#     return source_links

import hashlib
from typing import Dict, Any

# Response cache 
# RESPONSE_CACHE: Dict[str, Any] = {}

def load_response_cache():
    """Load response cache - DISABLED for in-memory only"""
    global RESPONSE_CACHE
    RESPONSE_CACHE = {}

def save_response_cache():
    """Save response cache - DISABLED for in-memory only"""
    pass

def get_query_cache_key(query: str, index_name: str,use_case: str, k: int, include_llm: bool = True, enable_hierarchical: bool = True) -> str:
    """Generate a comprehensive cache key for a query to ensure consistency"""
    normalized_query = query.strip().lower().replace('\n', ' ').replace('\r', ' ')
    normalized_query = ' '.join(normalized_query.split())
    
    cache_data = f"{normalized_query}|{index_name}|{use_case}|{k}|{include_llm}|{enable_hierarchical}"
    return hashlib.md5(cache_data.encode('utf-8')).hexdigest()

def get_llm_cache_key(query: str, context) -> str:
    """Generate a consistent cache key for LLM responses"""
    normalized_query = query.strip().lower().replace('\n', ' ').replace('\r', ' ')
    normalized_query = ' '.join(normalized_query.split())

    if isinstance(context, list):
        # For array context
        normalized_context = "|".join([str(item).strip().replace('\n', ' ').replace('\r', ' ') for item in context])
    else:
        # For string
        normalized_context = str(context).strip().replace('\n', ' ').replace('\r', ' ')
    normalized_context = ' '.join(normalized_context.split())
    
    cache_data = f"{normalized_query}|{normalized_context}"
    return hashlib.md5(cache_data.encode('utf-8')).hexdigest()

# Load cache on startup
# load_response_cache()

async def _generate_knowledge_based_response(user_query: str) -> str:
    """
    Generate a knowledge-based response as a final fallback
    """
    try:
        #knowledge-based response
        return f"Based on your query about '{user_query}', I recommend checking the relevant documentation or contacting the appropriate team for more specific information."
    except Exception as e:
        print(f"Error in knowledge-based response: {e}")
        return "I apologize, but I'm unable to provide a helpful response at this time."

if __name__ == "__main__":
    # #, reload=True)#, debug=True, workers=1)
    uvicorn.run("main_serve:app", host='0.0.0.0',
                port=2000, reload=True)
=====================================
==================================
# DPF Elasticsearch Configuration
import os
import json

# path 
secret_file_path = "/etc/secrets/AIDER_DPF_API"

# Read and parse the secrets.json file
with open(secret_file_path, "r") as secret_file:
    secrets = json.load(secret_file)

if 'ENV' in os.environ and os.environ['ENV'] in ['dev', 'uat', 'prod']:
    env = os.environ['ENV'].lower()
else:
    env = "dev"

if env == 'prod':
    DPF_API_KEY = secrets["PROD_API_KEY"]
else:
    DPF_API_KEY = secrets["DEV_API_KEY "]
    DPF_UAT_API_KEY = secrets["UAT_API_KEY"]
    # DPF API Endpoints
    DPF_ENDPOINTS = {
    "embedding_endpoint": "https://oa-uat.ebiz.verizon.com/vegas/models/api",  # UAT endpoint
    # "embedding_endpoint": "https://oa-dev2.ebiz.verizon.com/vegas/models/llm-embeddings",  # Current DEV endpoint
    "cross_encoder_endpoint": "https://oa-uat.ebiz.verizon.com/vegas/models/api",
    "llm_endpoint": "https://oa-uat.ebiz.verizon.com/vegas/apps/prompt/LLMInsight"
    }
    DPF_ELASTICSEARCH_CONFIG = {
    "api_url": "https://vectordb-experimentation-np.ebiz.verizon.com",
    "api_key": secrets["ES_API_KEY"],
    "index_name": "jysv-dpf-dev1"
    }


# Validate that required environment variables are set
if not DPF_API_KEY:
    raise ValueError("DPF_API_KEY is required but not found")
if not DPF_UAT_API_KEY:
    raise ValueError("DPF_UAT_API_KEY is required but not found")

# DPF Search Configuration
DPF_SEARCH_CONFIG = {
    "default_k": 10,
    "max_k": 10,
    "num_candidates": 100,
    "embedding_model": 'all-distilroberta-v1',
    "usecase_filter_aider": 'aider',
    "usecase_filter_copilot": ["aider_json","aider_pdf","aider_csv"],
    "usecase_filter": 'aider',
    "embedding_field": 'text_embedding_768',
    "chunk_data_field": 'original_content',  # Use original_content field
    "custom_metadata_field": 'custom_metadata'
}

# DPF Cross-encoder Configuration
DPF_CROSS_ENCODER_CONFIG = {
    "region": "us-east4",
    "project_id": "688379114786", 
    "endpoint_id": "4179014998757998592",
    "usecase_name": "cross-encoder",
    "model_type": "cross-encoder"
}

# DPF LLM Configuration
DPF_LLM_CONFIG = {
    "usecase": 'AIDER',
    "context_id": 'AIDER_CONTEXT',
    "gemini_fallback_usecase": 'AIDER',  # For Gemini fallback
    "gemini_fallback_context_id": 'aider_gemini',  # Context for Gemini fallback
    "context_placeholder": "{CONTEXT}",
    "question_placeholder": "{QUESTION}",
    "enable_gemini_fallback": True,  # Enable Gemini fallback for better responses
    "min_response_length": 50,  # Minimum response length 
    "max_output_tokens": 20480,  # Added 
    "unhelpful_phrases": [  # Phrases that indicate unhelpful responses
        "I couldn't find an answer",
        "Please rephrase your question", 
        "refine your search",
        "create a Jira ticket",
        "I don't have enough information",
        "I cannot provide an answer",
        "No relevant information found",
        "I'm unable to answer",
        "I don't know",
        "I cannot find",
        "I am sorry, but this document does not contain",
        "this document does not contain information",
        "the document does not provide",
        "I apologize, but the provided",
        "the provided document does not",
        "does not contain information about",
        "I'm sorry, but I don't see",
        "the document doesn't mention",
        "no information about",
        "I'm experiencing technical difficulties",
        "experiencing technical difficulties",
        "technical difficulties",
        "I am sorry, but I cannot answer your question",
        "the context does not provide information",
        "cannot answer your question, as the context does not",
        "I cannot answer your question, as the context",
        "as the context does not provide",
        "the provided passages describe"
    ]
}

# Document-Specific Chunking Configuration
DPF_DOCUMENT_CHUNKING_CONFIG = {
    "pdf": {
        "chunk_fields": ["chunk_data", "page_content", "section_header"],
        "metadata_fields": ["document_title", "page_number", "section_type"],
        "chunk_size": 512,
        "overlap": 50,
        "use_hierarchical": True
    },
    "confluence": {
        "chunk_fields": ["chunk_data", "page_title", "breadcrumb"],
        "metadata_fields": ["space_key", "page_id", "parent_page"],
        "chunk_size": 256,
        "overlap": 30,
        "use_hierarchical": True
    },
    "jira": {
        "chunk_fields": ["chunk_data", "issue_summary", "description"],
        "metadata_fields": ["project_key", "issue_type", "priority"],
        "chunk_size": 128,
        "overlap": 20,
        "use_hierarchical": False
    },
    "sharepoint": {
        "chunk_fields": ["chunk_data", "file_name", "folder_path"],
        "metadata_fields": ["site_collection", "library_name", "file_type"],
        "chunk_size": 384,
        "overlap": 40,
        "use_hierarchical": True
    },
    "default": {
        "chunk_fields": ["chunk_data"],
        "metadata_fields": ["source", "document_type"],
        "chunk_size": 256,
        "overlap": 25,
        "use_hierarchical": False
    }
}

# Advanced Search Configuration
DPF_ADVANCED_SEARCH_CONFIG = {
    "enable_document_context": True,
    "max_chunks_per_document": 3,
    "document_boost_factor": 1.2,
    "enable_section_grouping": True,
    "enable_parent_chunk_retrieval": True,
    "context_window_expansion": 2  # Number of neighboring chunks to include
}



# apikey = "hHBs6qEhZHP1M0JBLoEZ2DGzq7MNAEjs"

