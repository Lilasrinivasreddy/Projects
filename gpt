import os
import time
import warnings
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Any, Optional
import requests
import json
import uvicorn

# Import DPF configuration
from dpf_config import (
    DPF_ELASTICSEARCH_CONFIG, 
    DPF_ENDPOINTS, 
    DPF_API_KEY, 
    DPF_SEARCH_CONFIG,
    DPF_CROSS_ENCODER_CONFIG,
    DPF_LLM_CONFIG
)

warnings.filterwarnings("ignore")

app = FastAPI()#docs_url=None, redoc_url=None)

origins = ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class DPFQuery(BaseModel):
    query: str
    use_case: str # Add default use_case
    index_name: str = DPF_ELASTICSEARCH_CONFIG["index_name"]
    k: int = DPF_SEARCH_CONFIG["default_k"]
    # include_llm_response: bool = True
    # document_types: List[str] = None  # Filter by specific document types
    # enable_hierarchical: bool = True  # Enable document-aware chunking

@app.post("/vegas/apps/aider-retriever/dpf/api")
async def dpf_search_endpoint(request: DPFQuery):
    """
    DPF search endpoint using Elasticsearch with guaranteed consistent responses
    """
    try:
        start_time = time.time()
        result = await dpf_search(request)
        total_time = time.time() - start_time
        
        result["search_metadata"]["total_pipeline_time"] = total_time
        return result
    except Exception as e:
        print("exception",str(e))
        return {"error": "Unable to process your request at this time. Please try again."}

async def search_elasticsearch(api_url: str, api_key: str, index_name: str, query: dict):
    """
    Retrieve chunks from an Elasticsearch index based on a query.
    """
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"ApiKey {api_key}"
    }

    search_url = f"{api_url}/{index_name}/_search"
    
    try:
        response = requests.post(search_url, headers=headers, data=json.dumps(query))
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"Error querying Elasticsearch: {e}")
        raise HTTPException(status_code=500, detail="Unable to search documents at this time")

async def generate_dpf_embedding(text: str):
    """
    Generate embedding for DPF using the embedding endpoint
    """
    headers = {
        'Content-Type': 'application/json',
        'X-apikey': DPF_API_KEY # Use API key 
    }
    
    # # Use the payload format 
    payload = {
        # "region": "us-east4",
        # "project_id": "688379114786",
        "text": text,
        "usecase_name": "aider",
        "model_name": "all-distilroberta-v1",
        "client_metadata": {
            "client_tx_id": "f0f0a0b5-2ac5-4f56",
            "client_application": "aider",
        }

    }
    print("payload_embed",payload)
    
    try:
        response = requests.post(
            DPF_ENDPOINTS["embedding_endpoint"], 
            headers=headers, 
            data=json.dumps(payload),
            verify=False  # Added 
        )
        print(f"Embedding API Response Text: {response.text}")
        response.raise_for_status()
        result = response.json()
        print("result_embed",result)
        # For the simple endpoint, the response is directly a list of floats
        if isinstance(result, list) and all(isinstance(x, (int, float)) for x in result):
            return result
        
        # Fallback extraction methods for complex response formats
        if isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], list) and len(result[0]) > 0:
                if isinstance(result[0][0], dict) and 'embedding' in result[0][0]:
                    return result[0][0]['embedding']
        
        if isinstance(result, dict) and 'embedding' in result:
            return result['embedding']
        elif isinstance(result, list) and len(result) > 0:
            # If it's a list of embeddings, take the first one
            if isinstance(result[0], list):
                return result[0]
            elif isinstance(result[0], dict) and 'embedding' in result[0]:
                return result[0]['embedding']
            else:
                return result
        elif isinstance(result, list):
            return result
        else:
            print(f"Unexpected embedding response format: {result}")
            raise HTTPException(status_code=500, detail="Invalid embedding response format")
            
    except requests.exceptions.RequestException as e:
        print(f"Error generating embedding: {e}")
        raise HTTPException(status_code=500, detail="Unable to process your query at this time")

async def dpf_reranker(sentence_pairs: List[List[str]]):
    """
    Rerank results using cross-encoder for DPF
    """
    headers = {
        'Content-Type': 'application/json', 
        'X-apikey': DPF_API_KEY 
    }
    
    payload = {
        # "region": DPF_CROSS_ENCODER_CONFIG["region"],
        # "project_id": DPF_CROSS_ENCODER_CONFIG["project_id"],
        "endpoint_id": DPF_CROSS_ENCODER_CONFIG["endpoint_id"],
        "usecase_name": DPF_CROSS_ENCODER_CONFIG["usecase_name"],
        "model_type": DPF_CROSS_ENCODER_CONFIG["model_type"],
        "input_request": {
            "instances": sentence_pairs
        }
    }
    print("reranker",payload)
    try:
        response = requests.post(
            DPF_ENDPOINTS["cross_encoder_endpoint"], 
            headers=headers, 
            data=json.dumps(payload)
        )
        response.raise_for_status()
        result = response.json()
        print("result",result)
        # Handle different response formats
        if isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], list) and len(result[0]) > 0:
                if isinstance(result[0][0], dict) and "prediction_scores" in result[0][0]:
                    return result
                else:
                    # If the structure is different, wrap it properly
                    return [[{"prediction_scores": result[0] if isinstance(result[0], list) else [result[0]]}]]
            else:
                return [[{"prediction_scores": [0.0] * len(sentence_pairs)}]]
        elif isinstance(result, dict) and 'predictions' in result:
            return result['predictions']
        else:
            print(f"Unexpected reranker response format: {result}")
            return [[{"prediction_scores": [0.0] * len(sentence_pairs)}]]
            
    except requests.exceptions.RequestException as e:
        print(f"Error in reranking: {e}")
        # Return default scores on error
        return [[{"prediction_scores": [0.0] * len(sentence_pairs)}]]

async def generate_llm_response(user_query: str, context, use_case: str):
    """
    Generate LLM response without Gemini fallback
    """
    # Get primary response
    primary_response = await _call_llm_api(user_query, context, use_case)
    print(f"Final LLM Response: {primary_response[:200]}...")
    return primary_response

async def _call_llm_api(user_query: str, context, use_case: str):
    """
    Helper function to call LLM API with specified context ID
    """
    
    headers = {
        'Content-Type': 'application/json', 
        'X-apikey': DPF_API_KEY   # Use API key 
    }
     # Ensure context is always a list of strings (extract content only)
    if isinstance(context, list):
        # If it's a list of strings, use directly
        if all(isinstance(item, str) for item in context):
            context_strings = context
        else:
            # If it's a list of objects with text_content, extract the strings
            context_strings = []
            for item in context:
                if isinstance(item, dict) and 'text_content' in item:
                    context_strings.append(item['text_content'])
                elif isinstance(item, str):
                    context_strings.append(item)
                else:
                    context_strings.append(str(item))
    else:
        # Single item, convert to list
        if isinstance(context, str):
            context_strings = [context]
        elif isinstance(context, dict) and 'text_content' in context:
            context_strings = [context['text_content']]
        else:
            context_strings = [str(context)]
    
    # usecases and contextID's
    if use_case == "aider":
        payload = {
            "useCase": "AIDER",
            "contextId": "aider_context",
            "preSeed_injection_map": {
                "{CHUNK_INPUT}": context_strings,  # Used CHUNK_INPUT recent changes as discussed with team
                "{QUESTION}": user_query
            },
            "parameters": {
                "maxOutputTokens": DPF_LLM_CONFIG["max_output_tokens"]
            }
        }
    elif use_case == "dpf-agentic-copilot":
        payload = {
            "useCase": "dpf-agentic-copilot-prompt",
            "contextId": "copilot-rag",
            "preSeed_injection_map": {
                DPF_LLM_CONFIG["context_placeholder"]: context_strings,
                DPF_LLM_CONFIG["question_placeholder"]: user_query
            },
            "parameters": {
                "maxOutputTokens": DPF_LLM_CONFIG["max_output_tokens"]
            }
        }
    elif use_case == "ai_workmate":
        payload = {
            "useCase": "AIDER",
            "contextId": "aiworkmate_context",
            "preSeed_injection_map": {
                "{CHUNK_INPUT}": context_strings,  # Used CHUNK_INPUT recent changes as discussed with team
                "{QUESTION}": user_query
            },
            "parameters": {
                "maxOutputTokens": DPF_LLM_CONFIG["max_output_tokens"]
            }
        }
    else:
        # Default fallback
        payload = {
            "useCase": "AIDER",
            "contextId": "aider_context", 
            "preSeed_injection_map": {
                "{CHUNK_INPUT}": context_strings,
                "{QUESTION}": user_query
            }
        }
    print("payload",payload)
    try:
        response = requests.post(
            DPF_ENDPOINTS["llm_endpoint"], 
            headers=headers, 
            data=json.dumps(payload)
        )
        response.raise_for_status()
        result = response.json()
        print("llm_result",result)
        # Extract response from result - handle different response formats
        llm_response = None
        if isinstance(result, dict):
            if 'prediction' in result:
                llm_response = result['prediction']
            elif 'response' in result:
                llm_response = result['response']
            elif 'text' in result:
                llm_response = result['text']
            elif 'generated_text' in result:
                llm_response = result['generated_text']
        elif isinstance(result, str):
            llm_response = result
        elif isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], dict):
                llm_response = result[0].get('prediction', result[0].get('response', str(result[0])))
            else:
                llm_response = str(result[0])
        
        if llm_response is None:
            llm_response = "I apologize, but I encountered an issue generating a response. Please try again."
        
        # Cache the response immediately for consistency
        # RESPONSE_CACHE[cache_key] = llm_response
        # save_response_cache()
        print(f"LLM Response Generated: {llm_response[:200]}...")
        return llm_response
            
    except requests.exceptions.RequestException as e:
        error_response = f"I apologize, but I'm experiencing technical difficulties. Please try again in a moment."
        print(f"Request URL: {DPF_ENDPOINTS['llm_endpoint']}")
        print(f"Request payload: {json.dumps(payload, indent=2)}")
        print(f"Response status code: {getattr(e.response, 'status_code', 'N/A') if hasattr(e, 'response') else 'N/A'}")
        if hasattr(e, 'response') and e.response:
            print(f"Response text: {e.response.text}")
        return error_response

async def _call_llm_api_with_aider_gemini(user_query: str, enhanced_context: str, usecase: str, context_id: str):
    """
    Enhanced LLM API call with different usecase and context for Gemini fallback
    """
    headers = {
        'Content-Type': 'application/json', 
        'X-apikey': DPF_API_KEY
    }
    
    # Use different usecase and context for Gemini fallback
    payload = {
        "useCase": usecase,  # Use AIDER instead of COMMON_RAG
        "contextId": context_id,  # Use aider_gemini instead of GENERIC
        "preSeed_injection_map": {
            DPF_LLM_CONFIG["context_placeholder"]: [enhanced_context] if isinstance(enhanced_context, str) else enhanced_context,
            DPF_LLM_CONFIG["question_placeholder"]: "Please provide a helpful response based on the context and instructions above."
        }
    }
    
    try:
        response = requests.post(
            DPF_ENDPOINTS["llm_endpoint"], 
            headers=headers, 
            data=json.dumps(payload)
        )
        response.raise_for_status()
        result = response.json()
        
        # Extract response from result - handle different response formats
        llm_response = None
        if isinstance(result, dict):
            if 'prediction' in result:
                llm_response = result['prediction']
            elif 'response' in result:
                llm_response = result['response']
            elif 'text' in result:
                llm_response = result['text']
            elif 'generated_text' in result:
                llm_response = result['generated_text']
        elif isinstance(result, str):
            llm_response = result
        elif isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], dict):
                llm_response = result[0].get('prediction', result[0].get('response', str(result[0])))
            else:
                llm_response = str(result[0])
        
        if llm_response is None:
            llm_response = "I apologize, but I encountered an issue generating a response. Please try again."
        
        return llm_response
            
    except requests.exceptions.RequestException as e:
        error_response = f"I apologize, but I'm experiencing technical difficulties. Please try again in a moment."
        print(f"Gemini LLM API Error with usecase {usecase} and context {context_id}: {e}")
        print(f"Request URL: {DPF_ENDPOINTS['llm_endpoint']}")
        print(f"Request payload: {json.dumps(payload, indent=2)}")
        print(f"Response status code: {getattr(e.response, 'status_code', 'N/A') if hasattr(e, 'response') else 'N/A'}")
        if hasattr(e, 'response') and e.response:
            print(f"Response text: {e.response.text}")
        return error_response

def _is_unhelpful_response(response: str) -> bool:
    """
    Check if LLM response is unhelpful and should trigger fallback
    """
    if not response:
        return True
        
    # Use configured unhelpful phrases
    unhelpful_phrases = DPF_LLM_CONFIG["unhelpful_phrases"]
    print(f"Checking response against {len(unhelpful_phrases)} unhelpful phrases")
    
    response_lower = response.lower()
    for phrase in unhelpful_phrases:
        if phrase.lower() in response_lower:
            print(f"Found unhelpful phrase: '{phrase}'")
            return True
    
    # Check if response is too short
    response_length = len(response.strip())
    min_length = DPF_LLM_CONFIG["min_response_length"]
    if response_length < min_length:
        print(f"DEBUG: Response too short: {response_length} < {min_length}")
        return True
        
    print(f"DEBUG: Response seems helpful (length: {response_length})")
    return False

async def dpf_search(request: DPFQuery):
    """
    Complete DPF search pipeline with enhanced caching for 100% consistency
    """
    # 1. Generate embedding with error handling
    embedding_start = time.time()
    try:
        embedding = await generate_dpf_embedding(request.query)
    except Exception as e:
        error_result = {
            "retrieved_documents": [],
            "reranked_results": [],
            "llm_response": "Unable to process your query at this time. Please try again.",
            "source_links": [],
            "search_metadata": {
                "embedding_time": 0,
                "search_time": 0,
                "rerank_time": 0,
                "llm_time": 0,
                "total_results": 0,
                "reranked_count": 0,
                "sources_found": 0
            }
        }
        # RESPONSE_CACHE[search_cache_key] = error_result.copy()
        # save_response_cache()
        return error_result
        
    embedding_time = time.time() - embedding_start

    # Set up dynamic filter clause based on use case
    filter_clause = None
    if hasattr(request, 'use_case') and request.use_case == "aider":
        filter_clause = [{ "term": { "usecase_name": "aider" } }]
        print(f"Using aider filter: {filter_clause}")
    if hasattr(request, 'use_case') and request.use_case == "dpf-agentic-copilot":
        filter_clause = [
                { "term": { "usecase_name": "aider_csv" } },
                { "term": { "usecase_name": "aider_pdf" } }
            # "term": { "usecase_name": "aider_json" }
        ]    
    if hasattr(request, 'use_case') and request.use_case == "ai_workmate":
        filter_clause = [
                { "term": { "usecase_name": "aider_ai_workmate" } }       
        ]     
    if filter_clause is None:
        # Default filter for aider
        filter_clause = [{ "term": { "usecase_name": "aider" } }]    
       
    
    # 2. Elasticsearch query 
    es_query = {
        "_source": [DPF_SEARCH_CONFIG["chunk_data_field"], DPF_SEARCH_CONFIG["custom_metadata_field"]],
        "query": {
            "bool": {
                "filter": [
                    {
                        "bool": {
                            "should": filter_clause,
                            "minimum_should_match": 1
                        }
                    }
                ],
                "should": [
                    {
                        "knn": {
                            "field": DPF_SEARCH_CONFIG["embedding_field"],
                            "query_vector": embedding,
                            "k": min(request.k, DPF_SEARCH_CONFIG["max_k"]),
                            "num_candidates": DPF_SEARCH_CONFIG["num_candidates"]
                        }
                    }
                ]
            }
        },
        "size": min(request.k, DPF_SEARCH_CONFIG["max_k"])
    }
    # print("es_query",es_query)
    search_start = time.time()
    try:
        es_results = await search_elasticsearch(
            DPF_ELASTICSEARCH_CONFIG["api_url"], 
            DPF_ELASTICSEARCH_CONFIG["api_key"], 
            request.index_name, 
            es_query
        )
    except Exception as e:
        error_result = {
            "retrieved_documents": [],
            "reranked_results": [],
            "llm_response": "Unable to search documents at this time. Please try again.",
            "source_links": [],
            "search_metadata": {
                "embedding_time": embedding_time,
                "search_time": 0,
                "rerank_time": 0,
                "llm_time": 0,
                "total_results": 0,
                "reranked_count": 0,
                "sources_found": 0
            }
        }
        # RESPONSE_CACHE[search_cache_key] = error_result
        return error_result
        
    search_time = time.time() - search_start
    elastic_search_results = es_results['hits']['hits']
    elastic_search_results = sorted(elastic_search_results, 
                                   key=lambda x: (-x.get('_score', 0), x.get('_id', '')))
    print("elastic_search_results",elastic_search_results)
    # Extract contexts using original_content field
    elastic_search_contexts = [contexts.get('_source').get('original_content', '') for contexts in elastic_search_results]
   
    # 3. Rerank results with error handling
    rerank_start = time.time()
    sentence_pairs = [[request.query, context] for context in elastic_search_contexts]
    print("sentence_pairs",sentence_pairs)
    try:
        reranked_response = await dpf_reranker(sentence_pairs)
        print("reranked_response",reranked_response)
    except Exception as e:
        # Use original order if reranking fails
        reranked_response = [[{"prediction_scores": [0.5] * len(sentence_pairs)}]]
        
    rerank_time = time.time() - rerank_start
    cross_scores = reranked_response[0][0].get("prediction_scores", [])
    print("cross_scores",cross_scores)
    # Combine scores with contexts 
    context_list = []
    for score, context in zip(cross_scores, elastic_search_results):
        reranked_dict = {}
        reranked_dict['cross_score'] = score
        reranked_dict['text'] = context
        reranked_dict['text_content'] = context.get('_source', {}).get('original_content', '')  
        context_list.append(reranked_dict)
        
    # Sort by cross-encoder 
    reranked_docs = sorted(context_list, key=lambda x: x["cross_score"], reverse=True)

    top_doc = max(reranked_docs, key=lambda x: x['cross_score'])
    
    llm_response = None
    llm_time = 0
    source_links = []
    all_content = []
    
    # Using aider_gemini directly if cross_encoder_score is 0 (or) response is unhelpful
    if top_doc['cross_score'] == 0.0:
        print(f"Cross score is 0 - query is not available in doc, calling aider_gemini")
        llm_start = time.time()
        llm_response = await _call_llm_api_with_aider_gemini(request.query, [], request.use_case)
        llm_time = time.time() - llm_start
        is_gemini_response = True
    else:
        # Query might be available in doc - try with context first
        all_content = [doc['text_content'] for doc in reranked_docs if doc['text_content']]
        print("all_content", all_content)
        llm_start = time.time()
        llm_response = await generate_llm_response(request.query, all_content, request.use_case)
        llm_time = time.time() - llm_start
        is_gemini_response = False
        
        # If response is unhelpful, query is not available in document - use aider_gemini
        if _is_unhelpful_response(llm_response):
            print(f"Query not available in document (unhelpful response), calling aider_gemini")
            print(f"Document response: {llm_response[:100]}")
            fallback_start = time.time()
            llm_response = await _call_llm_api_with_aider_gemini(request.query, [], request.use_case)
            additional_time = time.time() - fallback_start
            llm_time += additional_time
            is_gemini_response = True

    print(f"Final LLM Response (source): {llm_response[:300]}...")
    # Prepare final result
    # Ensure top_document has proper structure
    if is_gemini_response:
        # For aider_gemini responses, create a proper top_document structure
        top_doc = {
            'cross_score': 0.0,  # Keep original score 
            'text': {},  # Empty dict to avoid errors
            'text_content': 'Gemini Response - No context needed',
            'original_result': {
                '_source': {
                    'custom_metadata': {
                        'source_link': 'Gemini Response'
                    }
                }
            }
        }
    elif top_doc and 'text' in top_doc and isinstance(top_doc['text'], dict):
        # Check if the response came from normal search (ensure original_result structure)
        source_data = top_doc['text'].get('_source', {})
        custom_metadata = source_data.get('custom_metadata', {})
        
        # If no source_link found or structure is missing, add default
        if not custom_metadata or not custom_metadata.get('source_link'):
            # Create the expected structure
            top_doc_with_source = {
                'cross_score': top_doc.get('cross_score', 0),
                'text': top_doc['text'],
                'text_content': top_doc.get('text_content', ''),
                'original_result': {
                    '_source': {
                        'custom_metadata': {
                            'source_link': 'Document Source'
                        }
                    }
                }
            }
            top_doc = top_doc_with_source
        else:
            # Ensure original_result structure exists
            if 'original_result' not in top_doc:
                top_doc['original_result'] = top_doc['text']
    elif top_doc and 'original_result' not in top_doc:
        # Handle case where top_doc exists but doesn't have original_result
        top_doc['original_result'] = {
            '_source': {
                'custom_metadata': {
                    'source_link': 'Document Source'
                }
            }
        }
    elif not top_doc:
        # No top document found, create a default one
        top_doc = {
            'cross_score': 0.0,
            'text': {},
            'text_content': '',
            'original_result': {
                '_source': {
                    'custom_metadata': {
                        'source_link': 'No Source Available'
                    }
                }
            }
        }
    result = {
        "retrieved_documents": elastic_search_results,
        "reranked_results": reranked_docs,
        "top_document": top_doc,
        "llm_response": llm_response,
        # "source_links": source_links,
        "search_metadata": {
            "embedding_time": embedding_time,
            "search_time": search_time,
            "rerank_time": rerank_time,
            "llm_time": llm_time,
            "total_results": len(elastic_search_results),
            "reranked_count": len(reranked_docs),
            # "sources_found": len(source_links)
        }
    }
    
    # RESPONSE_CACHE[search_cache_key] = result
    return result

async def _call_llm_api_with_aider_gemini(user_query: str, context_list: list, use_case: str):
    """
    Call LLM API with aider_gemini context for fallback when primary response is unhelpful
    """
    headers = {
        'Content-Type': 'application/json', 
        'X-apikey': DPF_API_KEY
    }
    
    # Ensure context_list is always a list of strings
    if not isinstance(context_list, list):
        context_list = [str(context_list)]
    
    print(f"aider_gemini fallback context type: {type(context_list)}, length: {len(context_list)}")
    
    # Use aider_gemini context based on use_case
    if use_case == "aider":
        payload = {
            "useCase": "AIDER",
            "contextId": "aider_gemini",  # Use aider_gemini context for fallback
            "preSeed_injection_map": {
                "{QUESTION}": user_query
            },
            "parameters": {
                "maxOutputTokens": DPF_LLM_CONFIG["max_output_tokens"]
            }
        }
    elif use_case == "dpf-agentic-copilot":
        payload = {
            "useCase": "datax-agentic-copilot-prompt",
            "contextId": "aider_gemini",  # Use aider_gemini context for fallback
            "preSeed_injection_map": {
                DPF_LLM_CONFIG["question_placeholder"]: user_query
            },
            "parameters": {
                "maxOutputTokens": DPF_LLM_CONFIG["max_output_tokens"]
            }
        }
    elif use_case == "ai_workmate":
        payload = {
            "useCase": "AIDER",
            "contextId": "aider_gemini",  # Use aider_gemini context for fallback
            "preSeed_injection_map": {  # Send list of strings directly
                "{QUESTION}": user_query
            },
            "parameters": {
                "maxOutputTokens": DPF_LLM_CONFIG["max_output_tokens"]
            }
        }
    else:
        # Default fallback with aider_gemini context
        payload = {
            "useCase": "AIDER",
            "contextId": "aider_gemini",  # Use aider_gemini context for fallback
            "preSeed_injection_map": {  # Send list of strings directly
                "{QUESTION}": user_query
            },
            "parameters": {
                "maxOutputTokens": DPF_LLM_CONFIG["max_output_tokens"]
            }
        }
    
    print(f"aider_gemini fallback payload CHUNK_INPUT type: {type(payload['preSeed_injection_map'].get('{CHUNK_INPUT}', payload['preSeed_injection_map'].get(DPF_LLM_CONFIG['context_placeholder'], 'NOT_FOUND')))}")
    print(f"aider_gemini fallback payload: {payload}")
    
    try:
        response = requests.post(
            DPF_ENDPOINTS["llm_endpoint"], 
            headers=headers, 
            data=json.dumps(payload)
        )
        response.raise_for_status()
        result = response.json()
        
        # Extract response from result 
        llm_response = None
        if isinstance(result, dict):
            if 'prediction' in result:
                llm_response = result['prediction']
            elif 'response' in result:
                llm_response = result['response']
            elif 'text' in result:
                llm_response = result['text']
            elif 'generated_text' in result:
                llm_response = result['generated_text']
        elif isinstance(result, str):
            llm_response = result
        elif isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], dict):
                llm_response = result[0].get('prediction', result[0].get('response', str(result[0])))
            else:
                llm_response = str(result[0])
        
        if llm_response is None:
            llm_response = "I apologize, but I encountered an issue generating a response. Please try again."
        
        print(f"aider_gemini LLM Response Generated: {llm_response[:200]}...")
        return llm_response
            
    except requests.exceptions.RequestException as e:
        error_response = f"I apologize, but I'm experiencing technical difficulties with the fallback system. Please try again in a moment."
        print(f"aider_gemini LLM API Error: {e}")
        print(f"Request URL: {DPF_ENDPOINTS['llm_endpoint']}")
        print(f"Request payload: {json.dumps(payload, indent=2)}")
        print(f"Response status code: {getattr(e.response, 'status_code', 'N/A') if hasattr(e, 'response') else 'N/A'}")
        if hasattr(e, 'response') and e.response:
            print(f"Response text: {e.response.text}")
        return error_response

if __name__ == "__main__":
    # #, reload=True)#, debug=True, workers=1)
    uvicorn.run("main_serve:app", host='0.0.0.0',
                port=2000, reload=True)

==============================================================
import os
import time
import warnings
# from ray import serve
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.encoders import jsonable_encoder
from pydantic import BaseModel
from typing import List, Union, Literal
from config import *
from langchain.retrievers import  EnsembleRetriever
from langchain_community.vectorstores import FAISS

from typing import Any, Dict, List, Optional
import requests
import json

from pydantic.v1 import BaseModel as langchainBaseModel, Extra, Field
from langchain.schema.embeddings import Embeddings
import uvicorn
import numpy as np
from config import EMAS_URL,apikey,url,env
import pickle
from langchain.retrievers import EnsembleRetriever

# Import DPF configuration
from dpf_config import (
    DPF_ELASTICSEARCH_CONFIG, 
    DPF_ENDPOINTS, 
    DPF_API_KEY, 
    DPF_SEARCH_CONFIG,
    DPF_CROSS_ENCODER_CONFIG,
    DPF_LLM_CONFIG,
    DPF_DOCUMENT_CHUNKING_CONFIG,
    DPF_ADVANCED_SEARCH_CONFIG
)

warnings.filterwarnings("ignore")

app = FastAPI()#docs_url=None, redoc_url=None)

origins = ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class EMAS(langchainBaseModel, Embeddings):

    def __init__(self, **kwargs: Any):
        """Initialize the sentence_transformer."""
        super().__init__(**kwargs)
        # self.EMAS_URL = EMAS_URL

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid

    def get_embed(self, text: str) -> List[float]:
        if env != "prod":
            payload = json.dumps({
                "region": "us-east4",
                "project_id": "688379114786",
                "endpoint_id": "7786939260002631680",
                "usecase_name": "llm-embeddings",
                "model_type": "llm-embeddings",
                    "input_request": {
                        "instances": [
                            {"text": f"{text}"}]
                }
            })

            headers = {
                'Content-Type': 'application/json',
                'X-apikey':apikey
            }
            try:
                response = requests.request(
                    "POST", EMAS_URL, headers=headers, data=payload, verify=False).json()
                result = response[0][0]['embedding']
            except Exception as e:
                print(f"Failure from embedding API. Encountered exception: {e}")
                print(text,response)
                raise
        else:
            payload = json.dumps({
                "region": "us-east4",
                "project_id": "783334890793",
                "endpoint_id": "7414266390837723136",
                "usecase_name": "llm-embeddings",
                "model_type": "llm-embeddings",
                    "input_request": {
                        "instances": [
                            {"text": f"{text}"}]
                }
            })

            headers = {
                'Content-Type': 'application/json',
                'X-apikey':apikey
            }
            try:
                response = requests.request(
                    "POST", EMAS_URL, headers=headers, data=payload, verify=False).json()
                result = response[0][0]['embedding']
            except Exception as e:
                print(f"Failure from embedding API. Encountered exception: {e}")
                print(text,response)
                raise
        return result

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Compute doc embeddings using a HuggingFace transformer model.

        Args:
            texts: The list of texts to embed.

        Returns:
            List of embeddings, one for each text.
        """
        res = []
        for text in texts:
            res.append(self.get_embed(text))

        return res

    def embed_query(self, text: str) -> List[float]:
        """Compute query embeddings using a HuggingFace transformer model.

        Args:
            text: The text to embed.

        Returns:
            Embeddings for the text.
        """
        return self.get_embed(text)


class LLMQuery(BaseModel):
    query: str
    index: Literal["gcp", "bi_tools", "dgs",
                   "ml_platform", "teradata", "all", "hadoop","informatica","data_indus","data_discovery","edw_modernization","ai_indus","ai_workmate"] = "all"

class DPFQuery(BaseModel):
    query: str
    use_case: str # Add default use_case
    index_name: str = DPF_ELASTICSEARCH_CONFIG["index_name"]
    k: int = DPF_SEARCH_CONFIG["default_k"]
    # include_llm_response: bool = True
    # document_types: List[str] = None  # Filter by specific document types
    # enable_hierarchical: bool = True  # Enable document-aware chunking


st = time.time()
faiss_embeddings=EMAS()
# print(type(faiss_embeddings))



db_dict = {
    'all': {'faiss': FAISS.load_local(FAISS_ALL_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "gcp": {'faiss': FAISS.load_local(FAISS_GCP_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "bi_tools": {'faiss': FAISS.load_local(FAISS_BI_TOOLS_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "dgs": {'faiss': FAISS.load_local(FAISS_DGS_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "ml_platform": {'faiss': FAISS.load_local(FAISS_ML_PLATFORM_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "teradata": {'faiss': FAISS.load_local(FAISS_TERADATA_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "hadoop": {'faiss': FAISS.load_local(FAISS_HADOOP_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "informatica": {'faiss': FAISS.load_local(FAISS_INFORMATICA_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "data_indus": {'faiss': FAISS.load_local(FAISS_DATA_INDUS_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "data_discovery": {'faiss': FAISS.load_local(FAISS_DATA_DISCOVERY_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "edw_modernization": {'faiss': FAISS.load_local(FAISS_EDW_MODERNIZATION_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "ai_indus": {'faiss': FAISS.load_local(FAISS_AI_INDUS_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)},
    "ai_workmate": {'faiss': FAISS.load_local(FAISS_AI_WORKMATE_INDEX_PATH, faiss_embeddings,allow_dangerous_deserialization=True)}
}

et = time.time() - st
# print(f'Loading database took {et} seconds.')

async def reranker(reranklist):
    try:
        if env!= "prod":
            headers={
                'Content-Type': 'application/json',
                'X-apikey':apikey
            }
            payload = json.dumps({
            "region": "us-east4",
            "project_id": "688379114786",
            "endpoint_id": "4179014998757998592",
            "usecase_name": "cross-encoder",
            "model_type": "cross-encoder",
            "input_request": {
                "instances": reranklist
            }
        })
        else:
            headers={
                'Content-Type': 'application/json',
                'X-apikey':apikey
            }
            payload = json.dumps({
            "region": "us-east4",
            "project_id": "783334890793",
            "endpoint_id": "6356118390498656256",
            "usecase_name": "cross-encoder",
            "model_type": "cross-encoder",
            "input_request": {
                "instances": reranklist
            }
        })
        response = requests.request(
                "POST", url, headers=headers, data=payload, verify=False).json()
        
        return response
    except Exception as e:
        print(f"Failure from reranker API. Encountered exception: {e}")
        raise



async def faiss_search(faiss_db, request):
    st = time.time()
    bm25_retriever=[]
    #retriever = faiss_db.as_retriever()
    #faiss_results = retriever.invoke(request.query,search_kwargs={"k": 5})
    bm25path=f'indexes/{request.index}/{request.index}_bm25_retriever.pkl'
    with open(bm25path,'rb') as file:
        bm25_retriever = pickle.load(file)
        bm25_retriever.k=5
    

    faiss_retriever = faiss_db.as_retriever(search_kwargs={"k":5})
    ensemble_retriever=EnsembleRetriever(retrievers=[bm25_retriever,faiss_retriever],
                                       weights=[0.5,0.5])
    #faiss_results=faiss_db.similarity_search(request.query)
    faiss_results=ensemble_retriever.invoke(request.query)
    # print(faiss_results)
    et = time.time() - st
    print(f'retrieval time {et} seconds.')
    st = time.time()
    reranker_list=[]
    source_documents=[]
    cross_encoder_updated=[]
    if isinstance(faiss_results, list) and len(faiss_results) > 0:
        for j in faiss_results:
            templist=[]
            templist.append(request.query)
            templist.append(j.page_content)
            reranker_list.append(templist)

            
        rerank_response=await reranker(reranker_list)
        #print("rerank_response: ",rerank_response)
        rerank_score =np.array(rerank_response[0][0]["prediction_scores"])
        K=4
        rerank_score_sorted = np.argsort(-rerank_score)[:K]
        et = time.time() - st
        print(f'cross encoder time {et} seconds.')
        print("rerank_score_sorted: ",rerank_score_sorted)
        cross_encoder_updated = [faiss_results[i] for i in rerank_score_sorted]
        for i in cross_encoder_updated:
            if i.metadata["source"] not in source_documents:
                source_documents.append(i.metadata["source"])

    return {"llm_context":cross_encoder_updated,"source_document":source_documents}

@app.post("/vegas/apps/aider-retriever/api")
async def search( request: LLMQuery):
    try:
        print("--------------------------------------------------------")
        print(f"Index:{request.index} and query:{request.query}")
        print("env: ",env)
        print("EMAS_URL: ",EMAS_URL)
        default_db = db_dict.get(request.index)
        # print(default_db)
        #search_type = request.search

        final_result = {"faiss": []}
        # faiss
        faiss_db = default_db['faiss']
        # print(faiss_db.index.ntotal)
        final_result['faiss'] = await faiss_search(faiss_db, request)

        return final_result
    except Exception as e:
        print(e)
        return {"error": str(e)}

@app.post("/vegas/apps/aider-retriever/dpf/api")
async def dpf_search_endpoint(request: DPFQuery):
    """
    DPF search endpoint using Elasticsearch with guaranteed consistent responses
    """
    try:
        start_time = time.time()
        result = await dpf_search(request)
        total_time = time.time() - start_time
        
        result["search_metadata"]["total_pipeline_time"] = total_time
        return result
    except Exception as e:
        print("exception",str(e))
        return {"error": "Unable to process your request at this time. Please try again."}

@app.get("/vegas/apps/aider-retriever/dpf/health")
async def dpf_health_check():
    """
    Health check endpoint for DPF service
    """
    try:
        # Test Elasticsearch connectivity
        test_query = {
            "query": {"match_all": {}},
            "size": 1
        }
        
        es_result = await search_elasticsearch(
            DPF_ELASTICSEARCH_CONFIG["api_url"], 
            DPF_ELASTICSEARCH_CONFIG["api_key"], 
            DPF_ELASTICSEARCH_CONFIG["index_name"], 
            test_query
        )
        
        return {
            "status": "healthy",
            "elasticsearch": "connected",
            "index_name": DPF_ELASTICSEARCH_CONFIG["index_name"],
            "total_documents": es_result.get("hits", {}).get("total", {}).get("value", 0),
            "timestamp": time.time()
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": time.time()
        }

async def search_elasticsearch(api_url: str, api_key: str, index_name: str, query: dict):
    """
    Retrieve chunks from an Elasticsearch index based on a query.
    """
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"ApiKey {api_key}"
    }

    search_url = f"{api_url}/{index_name}/_search"
    
    try:
        response = requests.post(search_url, headers=headers, data=json.dumps(query))
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"Error querying Elasticsearch: {e}")
        raise HTTPException(status_code=500, detail="Unable to search documents at this time")

async def generate_dpf_embedding(text: str):
    """
    Generate embedding for DPF using the embedding endpoint
    """
    headers = {
        'Content-Type': 'application/json',
        'X-apikey': DPF_API_KEY # Use API key 
    }
    
    # # Use the payload format 
    payload = {
        # "region": "us-east4",
        # "project_id": "688379114786",
        "text": text,
        "usecase_name": "aider",
        "model_name": "all-distilroberta-v1",
        "client_metadata": {
            "client_tx_id": "f0f0a0b5-2ac5-4f56",
            "client_application": "aider",
        }

    }
    print("payload_embed",payload)
    
    try:
        response = requests.post(
            DPF_ENDPOINTS["embedding_endpoint"], 
            headers=headers, 
            data=json.dumps(payload),
            verify=False  # Added 
        )
        print(f"Embedding API Response Text: {response.text}")
        response.raise_for_status()
        result = response.json()
        print("result_embed",result)
        # For the simple endpoint, the response is directly a list of floats
        if isinstance(result, list) and all(isinstance(x, (int, float)) for x in result):
            return result
        
        # Fallback extraction methods for complex response formats
        if isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], list) and len(result[0]) > 0:
                if isinstance(result[0][0], dict) and 'embedding' in result[0][0]:
                    return result[0][0]['embedding']
        
        if isinstance(result, dict) and 'embedding' in result:
            return result['embedding']
        elif isinstance(result, list) and len(result) > 0:
            # If it's a list of embeddings, take the first one
            if isinstance(result[0], list):
                return result[0]
            elif isinstance(result[0], dict) and 'embedding' in result[0]:
                return result[0]['embedding']
            else:
                return result
        elif isinstance(result, list):
            return result
        else:
            print(f"Unexpected embedding response format: {result}")
            raise HTTPException(status_code=500, detail="Invalid embedding response format")
            
    except requests.exceptions.RequestException as e:
        print(f"Error generating embedding: {e}")
        raise HTTPException(status_code=500, detail="Unable to process your query at this time")

async def dpf_reranker(sentence_pairs: List[List[str]]):
    """
    Rerank results using cross-encoder for DPF
    """
    headers = {
        'Content-Type': 'application/json', 
        'X-apikey': DPF_API_KEY 
    }
    
    payload = {
        # "region": DPF_CROSS_ENCODER_CONFIG["region"],
        # "project_id": DPF_CROSS_ENCODER_CONFIG["project_id"],
        "endpoint_id": DPF_CROSS_ENCODER_CONFIG["endpoint_id"],
        "usecase_name": DPF_CROSS_ENCODER_CONFIG["usecase_name"],
        "model_type": DPF_CROSS_ENCODER_CONFIG["model_type"],
        "input_request": {
            "instances": sentence_pairs
        }
    }
    print("reranker",payload)
    try:
        response = requests.post(
            DPF_ENDPOINTS["cross_encoder_endpoint"], 
            headers=headers, 
            data=json.dumps(payload)
        )
        response.raise_for_status()
        result = response.json()
        print("result",result)
        # Handle different response formats
        if isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], list) and len(result[0]) > 0:
                if isinstance(result[0][0], dict) and "prediction_scores" in result[0][0]:
                    return result
                else:
                    # If the structure is different, wrap it properly
                    return [[{"prediction_scores": result[0] if isinstance(result[0], list) else [result[0]]}]]
            else:
                return [[{"prediction_scores": [0.0] * len(sentence_pairs)}]]
        elif isinstance(result, dict) and 'predictions' in result:
            return result['predictions']
        else:
            print(f"Unexpected reranker response format: {result}")
            return [[{"prediction_scores": [0.0] * len(sentence_pairs)}]]
            
    except requests.exceptions.RequestException as e:
        print(f"Error in reranking: {e}")
        # Return default scores on error
        return [[{"prediction_scores": [0.0] * len(sentence_pairs)}]]

async def generate_llm_response(user_query: str, context, use_case: str):
    """
    Generate LLM response using with Gemini fallback 
    """
    # Check cache first for absolute consistency
    # cache_key = get_llm_cache_key(user_query, context)
    # if cache_key in RESPONSE_CACHE:
    #     return RESPONSE_CACHE[cache_key]
    # Get primary response
    primary_response = await _call_llm_api(user_query, context, use_case)
    is_unhelpful = _is_unhelpful_response(primary_response)
    
    if DPF_LLM_CONFIG["enable_gemini_fallback"] and is_unhelpful:
        print(f"Primary LLM gave unhelpful response, trying aider_gemini fallback")
        print(f"Primary response: {primary_response[:100]}")
        
        # Ensure context is always a list of strings for fallback
        if isinstance(context, list):
            context_list = context
        else:
            context_list = [str(context)]
        
        print(f"Fallback context type: {type(context_list)}, length: {len(context_list)}")
        print(f"Fallback context preview: {context_list[0][:100] if context_list else 'No content'}...")
        
        # Call aider_gemini context fallback with list of strings
        gemini_response = await _call_llm_api_with_aider_gemini(
            user_query, 
            context_list, 
            use_case
        )

        is_gemini_unhelpful = _is_unhelpful_response(gemini_response)
        print(f"Is gemini response unhelpful? {is_gemini_unhelpful}")
        
        if not is_gemini_unhelpful:
            print(f"aider_gemini fallback successful: {gemini_response[:100]}...")
            print(f"Returning aider_gemini response instead of primary response")
            return f"{gemini_response}||GEMINI_RESPONSE||"
        else:
            print(f"aider_gemini also gave unhelpful response: {gemini_response[:100]}...")
    else:
        print(f"Primary response is helpful or fallback is disabled, using primary response")
            
            # # Try one more fallback with knowledge-based approach
            # print("Trying knowledge-based fallback...")
            # knowledge_response = await _generate_knowledge_based_response(user_query)
            # if knowledge_response and not _is_unhelpful_response(knowledge_response):
            #     print(f"Knowledge-based fallback successful: {knowledge_response[:100]}...")
            #     return knowledge_response
    
    print(f"Final LLM Response: {primary_response[:200]}...")
    return primary_response
    
#     if DPF_LLM_CONFIG["enable_gemini_fallback"] and is_unhelpful:
#         print(f"Primary LLM gave unhelpful response, trying Gemini fallback")
#         print(f"Primary response: {primary_response[:100]}")
#         # Create enhanced context for fallback ---use any prompts 
#         enhanced_context = f"""
# Based on the available information and general knowledge, please help answer this question: {user_query}

# Context from documents:
# {context}

# Instructions: If the exact answer isn't in the documents, provide a helpful general explanation based on common knowledge, industry practices, or similar concepts. Be helpful and informative even if the specific term isn't mentioned in the provided documents.
# """
        
#         gemini_response = await _call_llm_api_enhanced_with_different_usecase(
#             user_query, 
#             enhanced_context, 
#             DPF_LLM_CONFIG["gemini_fallback_usecase"],
#             DPF_LLM_CONFIG["gemini_fallback_context_id"]
#         )
        
#         if not _is_unhelpful_response(gemini_response):
#             print(f"Gemini fallback successful: {gemini_response[:100]}...")
#             # Cache and return successful Gemini response
#             RESPONSE_CACHE[cache_key] = gemini_response
#             save_response_cache()
#             return gemini_response
#         else:
#             print(f"Gemini also gave unhelpful response: {gemini_response[:100]}...")
            
#             # Try one more fallback with knowledge-based approach
#             print("Trying knowledge-based fallback...")
#             knowledge_response = await _generate_knowledge_based_response(user_query)
#             if knowledge_response and not _is_unhelpful_response(knowledge_response):
#                 print(f"Knowledge-based fallback successful: {knowledge_response[:100]}...")
#                 RESPONSE_CACHE[cache_key] = knowledge_response
#                 save_response_cache()
#                 return knowledge_response
    
#     # Cache and return primary response (whether helpful or not)
#     RESPONSE_CACHE[cache_key] = primary_response
#     save_response_cache()
#     print(f"Final LLM Response: {primary_response[:200]}...")
    

async def _call_llm_api(user_query: str, context, use_case: str):
    """
    Helper function to call LLM API with specified context ID
    """
    
    headers = {
        'Content-Type': 'application/json', 
        'X-apikey': DPF_API_KEY   # Use API key 
    }
     # Ensure context is always a list of strings (extract content only)
    if isinstance(context, list):
        # If it's a list of strings, use directly
        if all(isinstance(item, str) for item in context):
            context_strings = context
        else:
            # If it's a list of objects with text_content, extract the strings
            context_strings = []
            for item in context:
                if isinstance(item, dict) and 'text_content' in item:
                    context_strings.append(item['text_content'])
                elif isinstance(item, str):
                    context_strings.append(item)
                else:
                    context_strings.append(str(item))
    else:
        # Single item, convert to list
        if isinstance(context, str):
            context_strings = [context]
        elif isinstance(context, dict) and 'text_content' in context:
            context_strings = [context['text_content']]
        else:
            context_strings = [str(context)]
    
    # usecases and contextID's
    if use_case == "aider":
        payload = {
            "useCase": "AIDER",
            "contextId": "aider_context",
            "preSeed_injection_map": {
                "{CHUNK_INPUT}": context_strings,  # Used CHUNK_INPUT recent changes as discussed with team
                "{QUESTION}": user_query
            },
            "parameters": {
                "maxOutputTokens": DPF_LLM_CONFIG["max_output_tokens"]
            }
        }
    elif use_case == "dpf-agentic-copilot":
        payload = {
            "useCase": "dpf-agentic-copilot-prompt",
            "contextId": "copilot-rag",
            "preSeed_injection_map": {
                DPF_LLM_CONFIG["context_placeholder"]: context_strings,
                DPF_LLM_CONFIG["question_placeholder"]: user_query
            },
            "parameters": {
                "maxOutputTokens": DPF_LLM_CONFIG["max_output_tokens"]
            }
        }
    elif use_case == "ai_workmate":
        payload = {
            "useCase": "AIDER",
            "contextId": "aiworkmate_context",
            "preSeed_injection_map": {
                "{CHUNK_INPUT}": context_strings,  # Used CHUNK_INPUT recent changes as discussed with team
                "{QUESTION}": user_query
            },
            "parameters": {
                "maxOutputTokens": DPF_LLM_CONFIG["max_output_tokens"]
            }
        }
    else:
        # Default fallback
        payload = {
            "useCase": "AIDER",
            "contextId": "aider_context", 
            "preSeed_injection_map": {
                "{CHUNK_INPUT}": context_strings,
                "{QUESTION}": user_query
            }
        }
    print("payload",payload)
    try:
        response = requests.post(
            DPF_ENDPOINTS["llm_endpoint"], 
            headers=headers, 
            data=json.dumps(payload)
        )
        response.raise_for_status()
        result = response.json()
        print("llm_result",result)
        # Extract response from result - handle different response formats
        llm_response = None
        if isinstance(result, dict):
            if 'prediction' in result:
                llm_response = result['prediction']
            elif 'response' in result:
                llm_response = result['response']
            elif 'text' in result:
                llm_response = result['text']
            elif 'generated_text' in result:
                llm_response = result['generated_text']
        elif isinstance(result, str):
            llm_response = result
        elif isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], dict):
                llm_response = result[0].get('prediction', result[0].get('response', str(result[0])))
            else:
                llm_response = str(result[0])
        
        if llm_response is None:
            llm_response = "I apologize, but I encountered an issue generating a response. Please try again."
        
        # Cache the response immediately for consistency
        # RESPONSE_CACHE[cache_key] = llm_response
        # save_response_cache()
        print(f"LLM Response Generated: {llm_response[:200]}...")
        return llm_response
            
    except requests.exceptions.RequestException as e:
        error_response = f"I apologize, but I'm experiencing technical difficulties. Please try again in a moment."
        # # Cache error responses for consistency
        # RESPONSE_CACHE[cache_key] = error_response
        # save_response_cache()
        # print(f"LLM Error Response: {error_response}")
        # print(f"LLM API Error with context {context_id}: {e}")
        print(f"Request URL: {DPF_ENDPOINTS['llm_endpoint']}")
        print(f"Request payload: {json.dumps(payload, indent=2)}")
        print(f"Response status code: {getattr(e.response, 'status_code', 'N/A') if hasattr(e, 'response') else 'N/A'}")
        if hasattr(e, 'response') and e.response:
            print(f"Response text: {e.response.text}")
        return error_response

async def _call_llm_api_enhanced(user_query: str, enhanced_context: str, context_id: str):
    """
    Enhanced LLM API call with modified prompt strategy for fallback
    """
    headers = {
        'Content-Type': 'application/json', 
        'X-apikey': DPF_API_KEY #update key
    }
    
    # Use a different prompt structure for enhanced fallback
    payload = {
        "useCase": DPF_LLM_CONFIG["usecase"],
        "contextId": context_id,
        "preSeed_injection_map": {
            DPF_LLM_CONFIG["context_placeholder"]: [enhanced_context] if isinstance(enhanced_context, str) else enhanced_context,
            DPF_LLM_CONFIG["question_placeholder"]: "Please provide a helpful response based on the context and instructions above."
        }
    }
    
    try:
        response = requests.post(
            DPF_ENDPOINTS["llm_endpoint"], 
            headers=headers, 
            data=json.dumps(payload)
        )
        response.raise_for_status()
        result = response.json()
        
        # Extract response from result - handle different response formats
        llm_response = None
        if isinstance(result, dict):
            if 'prediction' in result:
                llm_response = result['prediction']
            elif 'response' in result:
                llm_response = result['response']
            elif 'text' in result:
                llm_response = result['text']
            elif 'generated_text' in result:
                llm_response = result['generated_text']
        elif isinstance(result, str):
            llm_response = result
        elif isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], dict):
                llm_response = result[0].get('prediction', result[0].get('response', str(result[0])))
            else:
                llm_response = str(result[0])
        
        if llm_response is None:
            llm_response = "I apologize, but I encountered an issue generating a response. Please try again."
        
        return llm_response
            
    except requests.exceptions.RequestException as e:
        error_response = f"I apologize, but I'm experiencing technical difficulties. Please try again in a moment."
        print(f"Enhanced LLM API Error with context {context_id}: {e}")
        print(f"Request URL: {DPF_ENDPOINTS['llm_endpoint']}")
        print(f"Request payload: {json.dumps(payload, indent=2)}")
        print(f"Response status code: {getattr(e.response, 'status_code', 'N/A') if hasattr(e, 'response') else 'N/A'}")
        if hasattr(e, 'response') and e.response:
            print(f"Response text: {e.response.text}")
        return error_response

async def _call_llm_api_with_aider_gemini(user_query: str, enhanced_context: str, usecase: str, context_id: str):
    """
    Enhanced LLM API call with different usecase and context for Gemini fallback
    """
    headers = {
        'Content-Type': 'application/json', 
        'X-apikey': DPF_API_KEY
    }
    
    # Use different usecase and context for Gemini fallback
    payload = {
        "useCase": usecase,  # Use AIDER instead of COMMON_RAG
        "contextId": context_id,  # Use aider_gemini instead of GENERIC
        "preSeed_injection_map": {
            DPF_LLM_CONFIG["context_placeholder"]: [enhanced_context] if isinstance(enhanced_context, str) else enhanced_context,
            DPF_LLM_CONFIG["question_placeholder"]: "Please provide a helpful response based on the context and instructions above."
        }
    }
    
    try:
        response = requests.post(
            DPF_ENDPOINTS["llm_endpoint"], 
            headers=headers, 
            data=json.dumps(payload)
        )
        response.raise_for_status()
        result = response.json()
        
        # Extract response from result - handle different response formats
        llm_response = None
        if isinstance(result, dict):
            if 'prediction' in result:
                llm_response = result['prediction']
            elif 'response' in result:
                llm_response = result['response']
            elif 'text' in result:
                llm_response = result['text']
            elif 'generated_text' in result:
                llm_response = result['generated_text']
        elif isinstance(result, str):
            llm_response = result
        elif isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], dict):
                llm_response = result[0].get('prediction', result[0].get('response', str(result[0])))
            else:
                llm_response = str(result[0])
        
        if llm_response is None:
            llm_response = "I apologize, but I encountered an issue generating a response. Please try again."
        
        return llm_response
            
    except requests.exceptions.RequestException as e:
        error_response = f"I apologize, but I'm experiencing technical difficulties. Please try again in a moment."
        print(f"Gemini LLM API Error with usecase {usecase} and context {context_id}: {e}")
        print(f"Request URL: {DPF_ENDPOINTS['llm_endpoint']}")
        print(f"Request payload: {json.dumps(payload, indent=2)}")
        print(f"Response status code: {getattr(e.response, 'status_code', 'N/A') if hasattr(e, 'response') else 'N/A'}")
        if hasattr(e, 'response') and e.response:
            print(f"Response text: {e.response.text}")
        return error_response

def _is_unhelpful_response(response: str) -> bool:
    """
    Check if LLM response is unhelpful and should trigger fallback
    """
    if not response:
        return True
        
    # Use configured unhelpful phrases
    unhelpful_phrases = DPF_LLM_CONFIG["unhelpful_phrases"]
    print(f"Checking response against {len(unhelpful_phrases)} unhelpful phrases")
    
    response_lower = response.lower()
    for phrase in unhelpful_phrases:
        if phrase.lower() in response_lower:
            print(f"Found unhelpful phrase: '{phrase}'")
            return True
    
    # Check if response is too short
    response_length = len(response.strip())
    min_length = DPF_LLM_CONFIG["min_response_length"]
    if response_length < min_length:
        print(f"DEBUG: Response too short: {response_length} < {min_length}")
        return True
        
    print(f"DEBUG: Response seems helpful (length: {response_length})")
    return False

async def dpf_search(request: DPFQuery):
    """
    Complete DPF search pipeline with enhanced caching for 100% consistency
    """
    # Generate comprehensive cache key including all parameters that affect results
    # search_cache_key = get_query_cache_key(
    #     request.query, 
    #     request.index_name,
    #     request.use_case,
    #     request.k, 
    #     request.include_llm_response,
    #     request.enable_hierarchical
    # )
    # # print("search_cache_key",search_cache_key)
    # # Check cache first for absolute consistency
    # if search_cache_key in RESPONSE_CACHE:
    #     cached_result = RESPONSE_CACHE[search_cache_key].copy()
        
    #     # Ensure response format matches current request
    #     if not request.include_llm_response and 'llm_response' in cached_result:
    #         cached_result['llm_response'] = None
    #     elif request.include_llm_response and not cached_result.get('llm_response'):
    #         # Generate LLM response for cached results if needed
    #         if cached_result.get('reranked_results'):
    #             top_doc = cached_result['reranked_results'][0]
    #             # source_links = cached_result.get('source_links', [])
                
    #             base_llm_response = await generate_llm_response(request.query, top_doc['text'])
                
    #             # if source_links:
    #             #     llm_response = f"{base_llm_response}\n\n**Sources:**\n"
    #             #     for i, source_info in enumerate(source_links, 1):
    #             #         llm_response += f"[{i}] {source_info['url']}\n"
    #             # else:
    #             llm_response = base_llm_response
                
    #             cached_result['llm_response'] = llm_response
        
    #     return cached_result
    
    # 1. Generate embedding with error handling
    embedding_start = time.time()
    try:
        embedding = await generate_dpf_embedding(request.query)
    except Exception as e:
        error_result = {
            "retrieved_documents": [],
            "reranked_results": [],
            "llm_response": "Unable to process your query at this time. Please try again.",
            "source_links": [],
            "search_metadata": {
                "embedding_time": 0,
                "search_time": 0,
                "rerank_time": 0,
                "llm_time": 0,
                "total_results": 0,
                "reranked_count": 0,
                "sources_found": 0
            }
        }
        # RESPONSE_CACHE[search_cache_key] = error_result.copy()
        # save_response_cache()
        return error_result
        
    embedding_time = time.time() - embedding_start

    # Set up dynamic filter clause based on use case
    filter_clause = None
    if hasattr(request, 'use_case') and request.use_case == "aider":
        filter_clause = [{ "term": { "usecase_name": "aider" } }]
        print(f"Using aider filter: {filter_clause}")
    if hasattr(request, 'use_case') and request.use_case == "dpf-agentic-copilot":
        filter_clause = [
                { "term": { "usecase_name": "aider_csv" } },
                { "term": { "usecase_name": "aider_pdf" } }
            # "term": { "usecase_name": "aider_json" }
        ]    
    if hasattr(request, 'use_case') and request.use_case == "ai_workmate":
        filter_clause = [
                { "term": { "usecase_name": "aider_ai_workmate" } }       
        ]     
    if filter_clause is None:
        # Default filter for aider
        filter_clause = [{ "term": { "usecase_name": "aider" } }]    
       
    
    # 2. Elasticsearch query 
    es_query = {
        "_source": [DPF_SEARCH_CONFIG["chunk_data_field"], DPF_SEARCH_CONFIG["custom_metadata_field"]],
        "query": {
            "bool": {
                "filter": [
                    {
                        "bool": {
                            "should": filter_clause,
                            "minimum_should_match": 1
                        }
                    }
                ],
                "should": [
                    {
                        "knn": {
                            "field": DPF_SEARCH_CONFIG["embedding_field"],
                            "query_vector": embedding,
                            "k": min(request.k, DPF_SEARCH_CONFIG["max_k"]),
                            "num_candidates": DPF_SEARCH_CONFIG["num_candidates"]
                        }
                    }
                ]
            }
        },
        "size": min(request.k, DPF_SEARCH_CONFIG["max_k"])
    }
    # print("es_query",es_query)
    search_start = time.time()
    try:
        es_results = await search_elasticsearch(
            DPF_ELASTICSEARCH_CONFIG["api_url"], 
            DPF_ELASTICSEARCH_CONFIG["api_key"], 
            request.index_name, 
            es_query
        )
    except Exception as e:
        error_result = {
            "retrieved_documents": [],
            "reranked_results": [],
            "llm_response": "Unable to search documents at this time. Please try again.",
            "source_links": [],
            "search_metadata": {
                "embedding_time": embedding_time,
                "search_time": 0,
                "rerank_time": 0,
                "llm_time": 0,
                "total_results": 0,
                "reranked_count": 0,
                "sources_found": 0
            }
        }
        # RESPONSE_CACHE[search_cache_key] = error_result
        return error_result
        
    search_time = time.time() - search_start
    elastic_search_results = es_results['hits']['hits']
    elastic_search_results = sorted(elastic_search_results, 
                                   key=lambda x: (-x.get('_score', 0), x.get('_id', '')))
    print("elastic_search_results",elastic_search_results)
    # Use enhanced document
    # elastic_search_contexts = await get_document_context(elastic_search_results, request.query)
    elastic_search_contexts = [contexts.get('_source').get('original_content') for contexts in elastic_search_results]
    # if not elastic_search_contexts:
    #     no_results = {
    #         "retrieved_documents": [],
    #         "reranked_results": [],
    #         "llm_response": "No relevant documents found for your query.",
    #         "source_links": [],
    #         "search_metadata": {
    #             "embedding_time": embedding_time,
    #             "search_time": search_time,
    #             "rerank_time": 0,
    #             "llm_time": 0,
    #             "total_results": 0,
    #             "reranked_count": 0,
    #             "sources_found": 0
    #         }
    #     }
    #     RESPONSE_CACHE[search_cache_key] = no_results
    #     return no_results
    
    # 3. Rerank results with error handling
    rerank_start = time.time()
    sentence_pairs = [[request.query, context] for context in elastic_search_contexts]
    print("sentence_pairs",sentence_pairs)
    try:
        reranked_response = await dpf_reranker(sentence_pairs)
        print("reranked_response",reranked_response)
    except Exception as e:
        # Use original order if reranking fails
        reranked_response = [[{"prediction_scores": [0.5] * len(sentence_pairs)}]]
        
    rerank_time = time.time() - rerank_start
    cross_scores = reranked_response[0][0].get("prediction_scores", [])
    print("cross_scores",cross_scores)
    # Combine scores with contexts
    context_list = []
    for score, context in zip(cross_scores, elastic_search_results):
        reranked_dict = {}
        reranked_dict['cross_score'] = score
        reranked_dict['text'] = context
        reranked_dict['text_content'] = context.get('_source', {}).get('original_content', '')
        context_list.append(reranked_dict)
        
    # Sort by cross-encoder 
    reranked_docs = sorted(context_list, key=lambda x: x["cross_score"], reverse=True)

    top_doc = max(reranked_docs, key=lambda x: x['cross_score'])
    
    llm_response = None
    llm_time = 0
    source_links = []
    all_content = []
    # all_content = [doc['text'] for doc in reranked_docs]
    all_content = [doc['text_content'] for doc in reranked_docs if doc['text_content']]
    print("all_content",all_content)
    base_llm_response = await generate_llm_response_with_google_fallback(request.query, all_content, request.use_case)

    # Check if response came from aider_gemini
    is_gemini_response = False
    if "||GEMINI_RESPONSE||" in base_llm_response:
        is_gemini_response = True
        base_llm_response = base_llm_response.replace("||GEMINI_RESPONSE||", "")

    llm_response = base_llm_response
    # if request.include_llm_response and top_doc:
    #     llm_start = time.time()
    #     print("entering _llm")
    #     # Extract source links from top documents
    #     # source_links = await extract_source_links(reranked_docs, top_k=1)
    #     # print(f"Source Links Extracted: {len(source_links)} links found")
    #     # for i, link in enumerate(source_links):
    #         # print(f"  Link {i+1}: {link['url']} (score: {link['score']:.4f})")
        
    #     # LLM response
    #     all_content = [doc['text'] for doc in reranked_docs]
    #     print("all_content",all_content)
    #     # if request.use_case == "dpf-agentic-copilot":
    #     base_llm_response = await generate_llm_response(request.query, all_content, request.use_case)
    #     llm_response = base_llm_response
    #     # else:
    #     #     # combined_context = "\n\n".join(all_content)
    #     #     # base_llm_response = await generate_llm_response(request.query, combined_context, request.use_case)
    #     #     # top_doc_context = top_doc['text']
    #     #     base_llm_response = await generate_llm_response(request.query, all_content, request.use_case)
    #     # llm_time = time.time() - llm_start
    print(f"Final LLM Response (source): {llm_response[:300]}...")
    # Prepare final result
     # Ensure top_document has proper structure for llmsummary function
    if is_gemini_response:
        # For aider_gemini responses, create a proper top_document structure
        top_doc = {
            'cross_score': 1.0,  # High score for gemini response
            'text': {},  # Empty dict to avoid errors
            'text_content': '',
            'original_result': {
                '_source': {
                    'custom_metadata': {
                        'source_link': 'Gemini Response'
                    }
                }
            }
        }
    elif top_doc and 'text' in top_doc and isinstance(top_doc['text'], dict):
        # Check if the response came from normal search (ensure original_result structure)
        source_data = top_doc['text'].get('_source', {})
        custom_metadata = source_data.get('custom_metadata', {})
        
        # If no source_link found or structure is missing, add default
        if not custom_metadata or not custom_metadata.get('source_link'):
            # Create the expected structure
            top_doc_with_source = {
                'cross_score': top_doc.get('cross_score', 0),
                'text': top_doc['text'],
                'text_content': top_doc.get('text_content', ''),
                'original_result': {
                    '_source': {
                        'custom_metadata': {
                            'source_link': 'Document Source'
                        }
                    }
                }
            }
            top_doc = top_doc_with_source
        else:
            # Ensure original_result structure exists
            if 'original_result' not in top_doc:
                top_doc['original_result'] = top_doc['text']
    elif top_doc and 'original_result' not in top_doc:
        # Handle case where top_doc exists but doesn't have original_result
        top_doc['original_result'] = {
            '_source': {
                'custom_metadata': {
                    'source_link': 'Document Source'
                }
            }
        }
    elif not top_doc:
        # No top document found, create a default one
        top_doc = {
            'cross_score': 0.0,
            'text': {},
            'text_content': '',
            'original_result': {
                '_source': {
                    'custom_metadata': {
                        'source_link': 'No Source Available'
                    }
                }
            }
        }
    result = {
        "retrieved_documents": elastic_search_results,
        "reranked_results": reranked_docs,
        "top_document": top_doc,
        "llm_response": llm_response,
        # "source_links": source_links,
        "search_metadata": {
            "embedding_time": embedding_time,
            "search_time": search_time,
            "rerank_time": rerank_time,
            "llm_time": llm_time,
            "total_results": len(elastic_search_results),
            "reranked_count": len(reranked_docs),
            # "sources_found": len(source_links)
        }
    }
    
    # RESPONSE_CACHE[search_cache_key] = result
    return result

async def detect_document_type(es_result):
    """
    Detect document type from Elasticsearch result for chunking strategy
    """
    source = es_result.get('_source', {})
    
    # Check for document type indicators in metadata
    if 'file_extension' in source:
        ext = source['file_extension'].lower()
        if ext == 'pdf':
            return 'pdf'
    
    if 'source_system' in source:
        system = source['source_system'].lower()
        if 'confluence' in system:
            return 'confluence'
        elif 'jira' in system:
            return 'jira'
        elif 'sharepoint' in system:
            return 'sharepoint'
    
    if 'document_type' in source:
        return source['document_type'].lower()
    
    return 'default'

# async def get_document_context(es_results, query):
#     """
#     Enhanced document context retrieval with hierarchical chunking
#     """
#     if not DPF_ADVANCED_SEARCH_CONFIG["enable_document_context"]:
#         # Fallback to original simple extraction
#         return [result.get('_source', {}).get(DPF_SEARCH_CONFIG["chunk_data_field"], '') 
#                 for result in es_results]
    
#     enhanced_contexts = []
    
#     # Process each result directly since they're already individual chunks
#     for result in es_results:
#         source = result.get('_source', {})
        
#         # Get the main content using the configured field name
#         main_content = source.get(DPF_SEARCH_CONFIG["chunk_data_field"], '')
        
#         # If the main field is empty, try alternative field names
#         if not main_content:
#             main_content = (source.get('original_content', '') or 
#                            source.get('chunk_data', '') or 
#                            source.get('content', '') or 
#                            source.get('text', ''))
        
#         # Add the content (with minimal metadata if needed)
#         if main_content:
#             enhanced_contexts.append(main_content)
#         else:
#             # Fallback: combine all available text fields
#             all_text = []
#             for key, value in source.items():
#                 if isinstance(value, str) and len(value) > 10:  # Reasonable content length
#                     all_text.append(value)
#             enhanced_contexts.append(' '.join(all_text) if all_text else '')
    
#     return enhanced_contexts

# Source link extraction disabled by Lila
# async def extract_source_links(reranked_docs, top_k=1):
#     """
#     Extract source links from reranked documents
#     """
#     source_links = []
#     seen_links = set()
    
#     # Extract from top K documents to provide multiple sources
#     for doc in reranked_docs[:top_k]:
#         if 'original_result' in doc:
#             source_data = doc['original_result'].get('_source', {})
#             custom_metadata = source_data.get(DPF_SEARCH_CONFIG["custom_metadata_field"], {})
            
#             source_link = None
#             if isinstance(custom_metadata, dict):
#                 source_link = (custom_metadata.get('source_link') or
#                              custom_metadata.get('source_url') or 
#                              custom_metadata.get('url') or 
#                              custom_metadata.get('link') or 
#                              custom_metadata.get('source') or
#                              custom_metadata.get('document_url') or
#                              custom_metadata.get('page_url'))
#             elif isinstance(custom_metadata, str) and custom_metadata.strip():
#                 source_link = custom_metadata.strip()
            
#             # Check other potential source fields in the main document
#             if not source_link:
#                 source_link = (source_data.get('source_url') or 
#                              source_data.get('url') or 
#                              source_data.get('source') or
#                              source_data.get('document_url'))
            
#             # Add unique source links
#             if source_link and source_link not in seen_links:
#                 source_links.append({
#                     'url': source_link,
#                     'score': doc.get('cross_score', 0),
#                     'preview': doc.get('text', '')[:100] + '...' if doc.get('text') else ''
#                 })
#                 seen_links.add(source_link)
#                 print(f"Added source link: {source_link} (score: {doc.get('cross_score', 0):.4f})")
    
#     print(f"Total unique source links found: {len(source_links)}")
#     return source_links

import hashlib
from typing import Dict, Any

# Response cache 
# RESPONSE_CACHE: Dict[str, Any] = {}

def load_response_cache():
    """Load response cache - DISABLED for in-memory only"""
    global RESPONSE_CACHE
    RESPONSE_CACHE = {}

def save_response_cache():
    """Save response cache - DISABLED for in-memory only"""
    pass

def get_query_cache_key(query: str, index_name: str,use_case: str, k: int, include_llm: bool = True, enable_hierarchical: bool = True) -> str:
    """Generate a comprehensive cache key for a query to ensure consistency"""
    normalized_query = query.strip().lower().replace('\n', ' ').replace('\r', ' ')
    normalized_query = ' '.join(normalized_query.split())
    
    cache_data = f"{normalized_query}|{index_name}|{use_case}|{k}|{include_llm}|{enable_hierarchical}"
    return hashlib.md5(cache_data.encode('utf-8')).hexdigest()

def get_llm_cache_key(query: str, context) -> str:
    """Generate a consistent cache key for LLM responses"""
    normalized_query = query.strip().lower().replace('\n', ' ').replace('\r', ' ')
    normalized_query = ' '.join(normalized_query.split())

    if isinstance(context, list):
        # For array context
        normalized_context = "|".join([str(item).strip().replace('\n', ' ').replace('\r', ' ') for item in context])
    else:
        # For string
        normalized_context = str(context).strip().replace('\n', ' ').replace('\r', ' ')
    normalized_context = ' '.join(normalized_context.split())
    
    cache_data = f"{normalized_query}|{normalized_context}"
    return hashlib.md5(cache_data.encode('utf-8')).hexdigest()

# Load cache on startup
# load_response_cache()

async def _generate_knowledge_based_response(user_query: str) -> str:
    """
    Generate a knowledge-based response as a final fallback
    """
    try:
        #knowledge-based response
        return f"Based on your query about '{user_query}', I recommend checking the relevant documentation or contacting the appropriate team for more specific information."
    except Exception as e:
        print(f"Error in knowledge-based response: {e}")
        return "I apologize, but I'm unable to provide a helpful response at this time."


async def _call_llm_api_with_aider_gemini(user_query: str, context_list: list, use_case: str):
    """
    Call LLM API with aider_gemini context for fallback when primary response is unhelpful
    """
    headers = {
        'Content-Type': 'application/json', 
        'X-apikey': DPF_API_KEY
    }
    
    # Ensure context_list is always a list of strings
    if not isinstance(context_list, list):
        context_list = [str(context_list)]
    
    print(f"aider_gemini fallback context type: {type(context_list)}, length: {len(context_list)}")
    
    # Use aider_gemini context based on use_case
    if use_case == "aider":
        payload = {
            "useCase": "AIDER",
            "contextId": "aider_gemini",  # Use aider_gemini context for fallback
            "preSeed_injection_map": {
                "{QUESTION}": user_query
            },
            "parameters": {
                "maxOutputTokens": DPF_LLM_CONFIG["max_output_tokens"]
            }
        }
    elif use_case == "dpf-agentic-copilot":
        payload = {
            "useCase": "datax-agentic-copilot-prompt",
            "contextId": "aider_gemini",  # Use aider_gemini context for fallback
            "preSeed_injection_map": {
                DPF_LLM_CONFIG["question_placeholder"]: user_query
            },
            "parameters": {
                "maxOutputTokens": DPF_LLM_CONFIG["max_output_tokens"]
            }
        }
    elif use_case == "ai_workmate":
        payload = {
            "useCase": "AIDER",
            "contextId": "aider_gemini",  # Use aider_gemini context for fallback
            "preSeed_injection_map": {  # Send list of strings directly
                "{QUESTION}": user_query
            },
            "parameters": {
                "maxOutputTokens": DPF_LLM_CONFIG["max_output_tokens"]
            }
        }
    else:
        # Default fallback with aider_gemini context
        payload = {
            "useCase": "AIDER",
            "contextId": "aider_gemini",  # Use aider_gemini context for fallback
            "preSeed_injection_map": {  # Send list of strings directly
                "{QUESTION}": user_query
            },
            "parameters": {
                "maxOutputTokens": DPF_LLM_CONFIG["max_output_tokens"]
            }
        }
    
    print(f"aider_gemini fallback payload CHUNK_INPUT type: {type(payload['preSeed_injection_map'].get('{CHUNK_INPUT}', payload['preSeed_injection_map'].get(DPF_LLM_CONFIG['context_placeholder'], 'NOT_FOUND')))}")
    print(f"aider_gemini fallback payload: {payload}")
    
    try:
        response = requests.post(
            DPF_ENDPOINTS["llm_endpoint"], 
            headers=headers, 
            data=json.dumps(payload)
        )
        response.raise_for_status()
        result = response.json()
        
        # Extract response from result - handle different response formats
        llm_response = None
        if isinstance(result, dict):
            if 'prediction' in result:
                llm_response = result['prediction']
            elif 'response' in result:
                llm_response = result['response']
            elif 'text' in result:
                llm_response = result['text']
            elif 'generated_text' in result:
                llm_response = result['generated_text']
        elif isinstance(result, str):
            llm_response = result
        elif isinstance(result, list) and len(result) > 0:
            if isinstance(result[0], dict):
                llm_response = result[0].get('prediction', result[0].get('response', str(result[0])))
            else:
                llm_response = str(result[0])
        
        if llm_response is None:
            llm_response = "I apologize, but I encountered an issue generating a response. Please try again."
        
        print(f"aider_gemini LLM Response Generated: {llm_response[:200]}...")
        return llm_response
            
    except requests.exceptions.RequestException as e:
        error_response = f"I apologize, but I'm experiencing technical difficulties with the fallback system. Please try again in a moment."
        print(f"aider_gemini LLM API Error: {e}")
        print(f"Request URL: {DPF_ENDPOINTS['llm_endpoint']}")
        print(f"Request payload: {json.dumps(payload, indent=2)}")
        print(f"Response status code: {getattr(e.response, 'status_code', 'N/A') if hasattr(e, 'response') else 'N/A'}")
        if hasattr(e, 'response') and e.response:
            print(f"Response text: {e.response.text}")
        return error_response

# LLM response with aider_gemini fallback
async def generate_llm_response_with_google_fallback(user_query: str, context, use_case: str):
    """
    Generate LLM response with aider_gemini fallback
    """
    return await generate_llm_response(user_query, context, use_case)

if __name__ == "__main__":
    # #, reload=True)#, debug=True, workers=1)
    uvicorn.run("main_serve:app", host='0.0.0.0',
                port=2000, reload=True)
