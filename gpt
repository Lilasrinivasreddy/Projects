import pandas as pd
import json
import re
import pytz
import pandas_gbq
import numpy as np
from croniter import croniter
from datetime import datetime, timedelta
data_bytes = '/Users/reddyvu/Desktop/Test/DataX_Master_Metadata.xlsx'

print("Script Started")

# pd.set_option("future.no_silent_downcasting", True)
files_meta = pd.read_excel(data_bytes, sheet_name="DataX_File_Meta",dtype=str,keep_default_na=False,index_col=None)

# Drop first column
files_meta=files_meta.iloc[:,1:]

# Drop row which contains description
files_meta=files_meta.reset_index(drop=True)
files_meta=files_meta.drop(index=[0,1])

# tables_column_list=['dag_id','task_id','step_id','table_name','db_name','server_name','platform_name','environment_name','is_active','notify_table_size','threshold_unit','threshold_low','threshold_up','volume_incident_team_name','table_size_notification_email_list','alert_table_size','volume_alert_channel','volume_query','montior_volume']
files_column_list=['dag_id','step_id','file_name','file_path','landing_server_name','environment_name','file_middle_pattern','file_prefix','file_suffix','schedule_interval','frequency','timezone','is_mandatory','is_active','file_arrival_custom_interval','sla','source_system','source_poc','sla_notification_buffer','notify_source','notify_sla_misses','alert_channel','alert_target_team_name','notify_file_size','size_threshold_low_limit','size_threshold_high_limit','size_threshold_limit_unit','interval','archive_locaton','failure_location','sourceFormat','sourceUris','fixedWidth','fieldDelimiter','skipLeadingRows','maxBadRecords','nullMarker','allowQuotedNewlines','allowJaggedRows','ignoreUnknownValues','filesensorretries','filesensorretrydelay','timeout','fileSensor','gcsBucket','object']
config__column_list  = ["variable","dev","test","prod"]
process_integer_column=['duration_threshold_high_value','max_active_runs','run_date_buffer_interval','interval']
files_integer_column=['step_id','file_arrival_custom_interval','size_threshold_low_limit','size_threshold_high_limit','interval']
files_boolean_column=['is_mandatory','is_active','notify_source','notify_sla_misses','notify_file_size']
files_mandatory_column_list=['dag_id','file_name','file_path','landing_server_name','environment_name','file_prefix','file_suffix','schedule_interval','frequency','sla','source_system','source_poc','alert_channel','alert_target_team_name']

frequency_list = ["real_time", "hourly_custom", "weekly", "monthly", "fixed", "hourly", "daily"]
flags = [True,False]
bool_flag = ["true","false"]
platform_list = ["gcp","unix","aws","teradata","edl"]
env_list = ["gcp_bq","unix","java","edw_td","gcp","edl_hdfs","gcp_gcs","onprem_hive","gcp_hive"]
scheduler_list = ["airflow","composer","espx","cron","oozie","trigger"]
businessunit_list = ["vbg","network","vcg","corporate"]
source_type_list = ["pubsub","table","esp","file","kafka"]
target_type_list = ["file","kafka","pubsub","table","topic"]
config_column_list  = ["variable","dev","test","prod"]
time_zone_list = ["UTC","EST"]
priority_list = ["P1","P2","P3","P4","P5"]
true_flag = ["y","Y"]
env="dev"

config_df = pd.read_excel(data_bytes, sheet_name="Variables", dtype=str)
config_df.dropna(axis=0, how="all", inplace=True)
config_df = config_df.applymap(str)
variable_list = list(config_df["variable"])
dev_list = list(config_df["dev"])
test_list = list(config_df["test"])
prod_list = list(config_df["prod"])
ple_list =  list(config_df["ple"])
print(variable_list)
print("-------------------------------------------------")
print(dev_list)
print("-------------------------------------------------")
print(test_list)
print("-------------------------------------------------")
print(prod_list)
print("-------------------------------------------------")
print(ple_list)
print("-------------------------------------------------")

if len(variable_list) != len(dev_list):
	raise ValueError(
		"Variable column and dev column values are not mapping correctly. Length of either of them has issues. Kindly check the config tab"
	)
if len(variable_list) != len(test_list):
	raise ValueError(
		"Variable column and test column values are not mapping correctly. Length of either of them has issues. Kindly check the config tab"
	)
if len(variable_list) != len(prod_list):
	raise ValueError(
		"Variable column and prod column values are not mapping correctly. Length of either of them has issues. Kindly check the config tab"
	)
if len(variable_list) != len(ple_list):
	raise ValueError(
		"Variable column and prod column values are not mapping correctly. Length of either of them has issues. Kindly check the config tab"
	)


#----------------------------------------column_validation-----------------------------------------------

metadata_files_columns = list(files_meta.columns.values)
missed_fle_col = (set(files_column_list).difference(metadata_files_columns))
add_fle_col = (set(metadata_files_columns).difference(metadata_files_columns))


missing_column_cnt = len(missed_fle_col) 
additional_column_cnt = len(add_fle_col) 

if len(missed_fle_col)== 0:
	missed_fle_col = 'NA'
if len(add_fle_col)== 0:
	add_fle_col = 'NA'
	
    #-----------------------FILE_VALIDATION_COLUMNS----------------------------------------------------------------

if not files_meta.empty:
	files_meta=files_meta.infer_objects(copy=False).replace(r'^\s*$',np.nan,regex=True)
	# print(files_meta['interval'])
	files_meta=files_meta.map(lambda x:x.strip() if isinstance(x,str) else x)
	files_meta.dropna(axis=0, how="all", inplace=True)


	f_application_name_validation_cnt=file_name_validation_cnt=f_frequency_validation_cnt=f_platform_name_validtion_cnt=f_env_name_validtion_cnt=f_is_active_validation_cnt=file_path_validation_cnt=f_timezone_validation_cnt=source_poc_validation_cnt=file_prefix_cnt=file_middle_pattern_cnt=file_suffix_cnt=notify_sla_misses_cnt=notify_file_size_cnt=file_size_limit_cnt=f_threshold_low_cnt=f_threshold_up_cnt = 0
	files_meta.dropna(axis=0, how="all", inplace=True)
	total_files_count = files_meta.shape[0]

	#Updating the Variables
	if "dev" in env:
		files_meta.infer_objects(copy=False).replace(variable_list, dev_list, inplace=True,regex=True)
		print(files_meta)
	elif "test" in env:
		files_meta.infer_objects(copy=False).replace(variable_list, test_list,inplace=True,regex=True)
		print(files_meta)
	elif "ple" in env:
		files_meta.infer_objects(copy=False).replace(variable_list, ple_list,inplace=True,regex=True)
		print(files_meta)
	else:
		files_meta.infer_objects(copy=False).replace(variable_list, prod_list,inplace=True,regex=True)
		print(files_meta)

	files_meta = files_meta.astype("string")
	
	#Updating the datatype for Integer Column 
	for col in files_integer_column:		
		files_meta[col]=files_meta[col].replace('',np.nan).fillna('0').astype(int)
	
	print(files_meta['size_threshold_limit_unit'])
	#Updating the datatype for Boolean Column     
	for col in files_boolean_column:
		files_meta[col]=files_meta[col].replace('',np.nan).map({'Y': True,'N':False}).fillna(False)

	print("validation started")
	#file name validation
	file_name_validation_cnt = files_meta['file_name'].isna().sum() +files_meta.loc[files_meta['file_name'].str.contains(r'\s',case=False), :].shape[0]

 	#env_name validation -[GCP_BQ,Unix,JAVA,EDW_TD,GCP,EDL_HDFS,GCP_GCS,OnPrem_Hive,GCP_HIVE)]
	f_env_name_validtion_cnt = total_files_count - files_meta['environment_name'].str.lower().isin(env_list).sum()

	#frequency validation - ["real_time", "hourly_custom", "weekly", "monthly", "fixed", "hourly", "daily"] case sensitive      
	f_frequency_validation_cnt = total_files_count - files_meta['frequency'].str.lower().isin(frequency_list).sum()

    #timezone validation [utc/est]
	f_timezone_validation_cnt = total_files_count - files_meta['timezone'].str.lower().isin(["utc","est"]).sum()

	print("validation started- 2")
	#file path validation
	# file_path_validation_cnt = ((files_meta['file_path'].isna()) | (~files_meta['file_path'].str.startswith('gs')) | (~files_meta['file_path'].str.startswith('/')) | (~files_meta['file_path'].str.endswith('/'))).sum()

	# #is_active validation
	# f_is_active_validation_cnt =  total_files_count - files_meta['is_active'].fillna('n').str.lower().isin(flags).sum()
	# files_meta['is_active'] = files_meta['is_active'].fillna('n').astype(str)
	# f_is_active_validation_cnt = total_files_count - files_meta['is_active'].str.lower().isin(flags).sum()


    #source_poc_validation
	null_source_poc_cnt = files_meta['source_poc'].isnull().sum()
	source_poc_validation_cnt =  total_files_count -  files_meta.loc[files_meta['source_poc'].fillna('na').str.contains(r'[^@]+@[^@]+\.[^@]+',case=False), :].shape[0]
	source_poc_validation_cnt = source_poc_validation_cnt - null_source_poc_cnt

    #file_prefix/file_middle_pattern/file_suffix validation
	#file_middle_pattern can be null the condition is file_name = file_prefix + file_middle_pattern + file_suffix
	#file_prefix_cnt = ((files_meta['file_middle_pattern'].notna()) & (files_meta['file_name'] != files_meta['file_prefix'] + files_meta['file_middle_pattern'] + files_meta['file_suffix'])).sum() + ((files_meta['file_middle_pattern'].isna()) & (files_meta['file_name'] != files_meta['file_prefix'] + files_meta['file_suffix'])).sum()
	file_suffix_cnt = file_middle_pattern_cnt = file_prefix_cnt

	print("validation started -3")
	#notify_sla_misses validation
	files_meta['notify_sla_misses'] = files_meta['notify_sla_misses'].fillna('n').astype(str)
	notify_sla_misses_cnt = len(files_meta.loc[~files_meta['notify_sla_misses'].str.lower().isin(flags)])
	# notify_sla_misses_cnt = len(files_meta['notify_sla_misses'].fillna('n').replace('','n',inplace = False).str.lower().loc[~files_meta['notify_sla_misses'].fillna('n').replace('','n',inplace = False).str.lower().isin(flags)])

	# #notify_file_size validation
	# notify_file_size_cnt =len(files_meta['notify_file_size'].fillna('n').replace('','n',inplace = False).str.lower().loc[~files_meta['notify_file_size'].fillna('n').replace('','n',inplace = False).str.lower().isin(flags)])

	# #file_size_limit validation
	# file_size_limit_cnt = len(files_meta['file_size_limit'].fillna('n').replace('','n',inplace = False).str.lower().loc[~files_meta['file_size_limit'].fillna('n').replace('','n',inplace = False).str.lower().isin(flags)])

	print('completed')






	file_col_check=[]
	

	#Checking for mandatory columns
	for mandatory_col in files_mandatory_column_list:
		if(files_meta[mandatory_col].isna().any() or (files_meta[mandatory_col].astype(str).str.strip()=='').any() or (files_meta[mandatory_col].astype(str)== '0').any()):
			# row_number=files_meta[mandatory_col].index.to_list()
			# print("row_number",row_number)
			print("checking for mandatory columns")
			file_col_check.append(mandatory_col)
	print("testing in file meta",files_meta['size_threshold_limit_unit'])

	if len(file_col_check)>0:
		raise ValueError("FILE META VALIDATION:\n\n Following are the mandatory column in FILES tab and can't be left empty :" + str(file_col_check))
	else:
		# load_table(files_meta,files_table_id)
		print("Data loaded to File staging Table")
else:
	print("Files Tab is Empty")
	file_stg = 0
	
print("end")

