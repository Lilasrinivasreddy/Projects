Here's the flow:
When a user uploads a pdf, we use the pdf plumber to parse and read the pdf contents, take the contents to LLM to summarize it and create embeddings on that summary.
2. When a request comes to converse(QVerse) API: If use_report toggle is set to 'on', we do a semantic search (on the LLM generated summary for each document) of available reports for that particular user and identify which report (top3) to use to answer the user question.
Now as for next steps,
We will replace pdf plumber with a multimodal meaning, we will go to gemini with the pdf directly and ask it summarize it for us because reports can have charts or graphical information as well.
And we need to also implement chunking as reports can be huge and we will hit token limit issues.
Please highlight these next items as well when you are discussing the architecture.





7:36
I've added couple of endpoints to archive/delete existing pdfs as well
7:36
you can look at qverse-development branch for latest changes
