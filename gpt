import argparse
import logging
import subprocess
from datetime import datetime
from google.cloud import bigquery

# Configure Logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

class LoggerExecution:
    def __init__(self, dq_gcp_data_project_id, dq_bq_dataset):
        """
        Initialize BigQuery client and logging.
        """
        self.client = bigquery.Client()
        self.project_id = dq_gcp_data_project_id
        self.dataset_name = dq_bq_dataset
        logging.info("LoggerExecution initialized.")

    def run_dq_processor(self):
        """
        Runs `dq_processor.py` as a subprocess and captures its output.
        """
        try:
            result = subprocess.run(["python3", "dq_processor.py"], capture_output=True, text=True, check=True)
            return result.stdout.strip()  # Capture stdout (Success or Failure message)
        except subprocess.CalledProcessError as e:
            return f"Error running dq_processor: {e.stderr.strip()}"  # Capture stderr if there's an error

    def log_job_monitoring(self, job_id, job_name, job_start_ts, job_end_ts, step_code, user_id):
        """
        Logs job monitoring details into `dqaas_job_monitor_report` table.
        """
        entry_ts = datetime.now()
        comments = self.run_dq_processor()  # Run dq_processor and capture its output

        query = f"""
        INSERT INTO `{self.project_id}.{self.dataset_name}.dqaas_job_monitor_report`
        (job_id, job_name, job_start_ts, job_end_ts, entry_ts, user_id, step_code, comments)
        VALUES (@job_id, @job_name, @job_start_ts, @job_end_ts, @entry_ts, @user_id, @step_code, @comments)
        """

        params = {
            "job_id": job_id,
            "job_name": job_name,
            "job_start_ts": job_start_ts,
            "job_end_ts": job_end_ts,
            "entry_ts": entry_ts,
            "user_id": user_id,
            "step_code": step_code,
            "comments": comments  # Get message dynamically from `dq_processor.py`
        }

        logging.info(f"Logging job monitoring: {params}")
        try:
            self.client.query(query, params).result()
            logging.info(f"Inserted job monitoring details into `dqaas_job_monitor_report`.")
        except Exception as e:
            logging.error(f"Error logging monitoring: {str(e)}")

def parse_arguments():
    """
    Parse command-line arguments for job monitoring logging.
    """
    parser = argparse.ArgumentParser(description="Log monitoring details into BigQuery.")
    parser.add_argument("--dq_gcp_data_project_id", required=True, help="Google Cloud Project ID")
    parser.add_argument("--dataset_name", required=True, help="BigQuery Dataset Name")
    parser.add_argument("--job_id", type=int, required=True, help="Job ID")
    parser.add_argument("--job_name", type=str, required=True, help="Job Name")
    parser.add_argument("--job_start_ts", type=str, required=True, help="Job Start Timestamp (YYYY-MM-DD HH:MM:SS)")
    parser.add_argument("--job_end_ts", type=str, required=True, help="Job End Timestamp (YYYY-MM-DD HH:MM:SS)")
    parser.add_argument("--step_code", type=str, required=True, help="Step Code (Module/Function Name)")
    parser.add_argument("--user_id", type=str, required=True, help="User ID")
    
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_arguments()
    logger = LoggerExecution(args.dq_gcp_data_project_id, args.dataset_name)

    logger.log_job_monitoring(
        job_id=args.job_id,
        job_name=args.job_name,
        job_start_ts=datetime.strptime(args.job_start_ts, "%Y-%m-%d %H:%M:%S"),
        job_end_ts=datetime.strptime(args.job_end_ts, "%Y-%m-%d %H:%M:%S"),
        step_code=args.step_code,
        user_id=args.user_id
    )