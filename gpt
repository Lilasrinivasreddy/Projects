from datetime import timedelta,datetime
from airflow import DAG
from airflow.contrib.operators.bigquery_operator import BigQueryOperator
from airflow.utils.dates import days_ago
import importlib
from airflow.models import Variable

env_config_module = importlib.import_module("vz-it-hgrv-aidedo-0.env_config")
ENVIRONMENT = Variable.get('composer_env')
PROJECT_ID = env_config_module.config[ENVIRONMENT]['project_id']
GCP_CONNECTION_ID = env_config_module.config[ENVIRONMENT]['gcp_connection_id']
EMAIL_LIST = env_config_module.config[ENVIRONMENT]['email_list']

VSAT = env_config_module.config[ENVIRONMENT]['VSAD']
PROJECT_SPACE = env_config_module.config[ENVIRONMENT]['ProjectSpace']
APPLICATION_NAME = env_config_module.config[ENVIRONMENT]['ApplicationName']


default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date':days_ago(1),
    'email': f"{EMAIL_LIST}",
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=1)
}


dag=DAG(
    dag_id=f"dg_{VSAT}_{PROJECT_SPACE}_{APPLICATION_NAME}_process_monitoring_report_daily",
    schedule_interval='0 0 * * *',
    max_active_runs= 1,
    catchup=False,
    default_args=default_args,
    description='Ths dag is for process monitoring which will run at 00:00',
    concurrency=3
)


dof_etl_process_monitor_report = BigQueryOperator(
    task_id=f'dof_etl_process_monitor_report',
    gcp_conn_id=f"{GCP_CONNECTION_ID}",
    sql='sql/dof_etl_process_monitor_report.sql',
    write_disposition='WRITE_APPEND',
    use_legacy_sql=False,
    dag = dag
)


dof_etl_realtime_process_monitor_report = BigQueryOperator(
    task_id=f'dof_etl_realtime_process_monitor_report',
    gcp_conn_id=f"{GCP_CONNECTION_ID}",
    sql='sql/dof_etl_realtime_process_monitor_report.sql',
    write_disposition='WRITE_APPEND',
    use_legacy_sql=False,
    dag = dag
)

dof_etl_process_monitor_report >> dof_etl_realtime_process_monitor_report
----------------------------------------------------------------------------------------------------------------------------------------------------------------
import importlib
from airflow import DAG
from datetime import timedelta
from airflow.utils.dates import days_ago
from airflow.operators.dummy import DummyOperator
from airflow.operators.python_operator import PythonOperator
from airflow.models import Variable


dof_suppress_alert_python_callables = importlib.import_module(
    f"vz-it-hgrv-aidedo-0.alerting_framework.dof_suppress_alert_python_callables")
env_config_module = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.env_config")
ENVIRONMENT = Variable.get('composer_env')

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=1)
}

with DAG(
        f"dg_{env_config_module.config[ENVIRONMENT]['VSAD']}_{env_config_module.config[ENVIRONMENT]['ProjectSpace']}_{env_config_module.config[ENVIRONMENT]['ApplicationName']}_suppress_alert",
        default_args=default_args,
        schedule_interval='*/5 * * * *',
        max_active_runs=1,
        catchup=False,
        on_failure_callback=dof_suppress_alert_python_callables.custom_failure_alert,
        description='Ths dag is for enable/disable alerts',
        concurrency=3
) as dag:
    disable_alert = PythonOperator(task_id='disable_alert',
                                   python_callable=dof_suppress_alert_python_callables.disable_alert)

    enable_alert = PythonOperator(task_id='enable_alert',
                                  python_callable=dof_suppress_alert_python_callables.enable_alert)

    end = DummyOperator(
        task_id="end")

    disable_alert >> enable_alert >> end
	-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
"""
Author: sreekanth.kc1@
Description: This Python files contains function which is invoked by the Alert Suppression DAG.
"""


def custom_failure_alert(context):
    import importlib
    python_callable_module = importlib.import_module(f"vz-it-hgrv-aidedo-0.alerting_framework.python_callables")
    environment_value = python_callable_module.get_environment_value()
    from_email = 'do-not-reply@verizon.com'
    to_email = "sreekanth.kc1@verizon.com"

    task_instance = context['task_instance']
    print(f"task {task_instance.task_id} failed in dag {task_instance.dag_id} ")

    email_body = f"""
    DAG ID : {task_instance.dag_id}
    Task ID: {task_instance.task_id}
    Run ID: {context['run_id']}
    Log URL: {task_instance.log_url}
    Try: {task_instance.try_number}
    Error: {str(context['exception'])}
    """
    email_subject = f"Airflow|Failure|{environment_value.upper()}|{task_instance.dag_id}|{task_instance.task_id}"
    python_callable_module.send_email(from_email, to_email, email_subject, email_body)


def disable_alert(**context):
    """
    Generate and send process alerts to Opsgenie.
    input:
        **context: task variable
    return:
        status: boolean
    """
    import importlib
    from airflow.models import Variable
    from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook

    python_callable_module = importlib.import_module(f"vz-it-hgrv-aidedo-0.alerting_framework.python_callables")
    ENVIRONMENT = Variable.get('composer_env')
    env_config_module = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.env_config")
    PROJECT_ID = env_config_module.config[ENVIRONMENT]['project_id']
    DATASET_ID_V2 = 'aid_epdo_prd_tbls'
    DATASET_ID_V1 = 'dataobservability_tbls'
    from_email = 'do-not-reply@verizon.com'
    to_email = "sreekanth.kc1@verizon.com"
    GCP_CONNECTION_ID = env_config_module.config[ENVIRONMENT]['gcp_connection_id']
    Obj = BigQueryHook(gcp_conn_id=GCP_CONNECTION_ID)
    bigquery_client = Obj.get_client(project_id=PROJECT_ID)

    get_alert_disable_list = f"""
        SELECT
          *
        FROM
          `{PROJECT_ID}.{DATASET_ID_V2}.dof_alert_suppression_details`
        WHERE
          is_active IS NULL
          AND LOWER(status) = 'disable'
          AND start_time IS NOT NULL
          AND (CURRENT_TIMESTAMP() > start_time AND start_time > CURRENT_TIMESTAMP() - INTERVAL 10 minute)"""

    print(get_alert_disable_list)
    query_job = bigquery_client.query(get_alert_disable_list)
    disable_list = query_job.result()

    for item in disable_list:
        # Update meta table, set supress_alert = True
        notification_update_query = f"""UPDATE
              `{PROJECT_ID}.{DATASET_ID_V1}.dof_process_alert_notification_meta`
            SET
              suppress_alert = TRUE,
              updated_at = CURRENT_TIMESTAMP()
            WHERE
              process_name IN (
              SELECT
                DISTINCT process_name
              FROM
                `{PROJECT_ID}.{DATASET_ID_V1}.dof_process_meta`
              WHERE
                {item['suppression_column']}='{item['value']}')"""

        print("Update Query------------->>>>>>>>", notification_update_query)

        bigquery_client.query(notification_update_query)

        # Update suppress details table

        update_supress_table = f"""UPDATE
              `{PROJECT_ID}.{DATASET_ID_V2}.dof_alert_suppression_details`
            SET
              is_active = TRUE
            WHERE
              id = '{item['id']}'"""
        print("update_supress_table--------------->>", update_supress_table)

        bigquery_client.query(update_supress_table)

        environment_value = python_callable_module.get_environment_value()

        email_subject = f"TraceX|{environment_value}|Alert Disabled"
        email_body = f"""<!DOCTYPE html>
                            <html>
                            <body>
                              <p>Team,</br>Alert is disabled for the below.</p>
                              <p>{item['suppression_column']}: {item['value']} </br>
                              User: {item['user']}</p>
                            </body>
                            </html>"""
        python_callable_module.send_email(from_email, to_email, email_subject, email_body)

    return True


def enable_alert(**context):
    """
    Generate and send process alerts to Opsgenie.
    input:
        **context: task variable
    return:
        status: boolean
    """
    import importlib
    from airflow.models import Variable
    from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook

    ENVIRONMENT = Variable.get('composer_env')
    env_config_module = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.env_config")
    python_callable_module = importlib.import_module(f"vz-it-hgrv-aidedo-0.alerting_framework.python_callables")
    PROJECT_ID = env_config_module.config[ENVIRONMENT]['project_id']
    DATASET_ID_V2 = 'aid_epdo_prd_tbls'
    DATASET_ID_V1 = 'dataobservability_tbls'
    from_email = 'do-not-reply@verizon.com'
    to_email = "sreekanth.kc1@verizon.com"
    GCP_CONNECTION_ID = env_config_module.config[ENVIRONMENT]['gcp_connection_id']
    Obj = BigQueryHook(gcp_conn_id=GCP_CONNECTION_ID)
    bigquery_client = Obj.get_client(project_id=PROJECT_ID)

    get_alert_enable_list = f"""SELECT
              *
            FROM
              `{PROJECT_ID}.{DATASET_ID_V2}.dof_alert_suppression_details`
            WHERE
              is_active IS TRUE
              AND LOWER(status) = 'disable'
              AND end_time IS NOT NULL
              AND (CURRENT_TIMESTAMP() > end_time AND end_time > CURRENT_TIMESTAMP() - INTERVAL 10 minute)"""

    print(get_alert_enable_list)
    query_job = bigquery_client.query(get_alert_enable_list)
    enable_list = query_job.result()

    for item in enable_list:
        # Update meta table, set supress_alert = False
        notification_update_query = f"""UPDATE
              `{PROJECT_ID}.{DATASET_ID_V1}.dof_process_alert_notification_meta`
            SET
              suppress_alert = False,
              updated_at = CURRENT_TIMESTAMP()
            WHERE
              process_name IN (
              SELECT
                DISTINCT process_name
              FROM
                `{PROJECT_ID}.{DATASET_ID_V1}.dof_process_meta`
              WHERE
                {item['suppression_column']}='{item['value']}')"""

        print("Update Query------------->>>>>>>>", notification_update_query)

        bigquery_client.query(notification_update_query)

        # Update suppress details table
        update_supress_table = f"""UPDATE
              `{PROJECT_ID}.{DATASET_ID_V2}.dof_alert_suppression_details`
            SET
              is_active = False
            WHERE
              id = '{item['id']}'"""

        print("update_supress_table--------------->>", update_supress_table)
        bigquery_client.query(update_supress_table)

        environment_value = python_callable_module.get_environment_value()

        email_subject = f"TraceX|{environment_value}|Alert Enabled"
        email_body = f"""<!DOCTYPE html>
                                    <html>
                                    <body>
                                      <p>Team,</br>Alert is enabled for the below.</p>
                                      <p>{item['suppression_column']}: {item['value']} </br>
                                      User: {item['user']}</p>
                                    </body>
                                    </html>"""
        python_callable_module.send_email(from_email, to_email, email_subject, email_body)
    return True
-------------------------------------------------------------------------------------------------------------------------------------------------------------------
"""
Author: sreekanth.kc1@
Description: This Python files contains function which is invoked by the Alerting framework DAG.
"""


def get_environment_value():
    """
    Get environment value.
    input:
        None
    return:
        environment_value: string - Environment value (Dev, Test or Production)
    """
    from airflow.models import Variable
    ENVIRONMENT = Variable.get('composer_env')
    if ENVIRONMENT == 'dev':
        environment_value = 'Dev'
    elif ENVIRONMENT == 'test':
        environment_value = 'Test'
    else:
        environment_value = 'Production'
    return environment_value


def get_execution_date(ds, **context):
    """
    Get the DAG execution date.
    input:
        ds: date - execution date
    return:
        Boolean
    """
    task_instance = context['task_instance']
    task_instance.xcom_push(key='execution_date_dashed', value=ds)
    print("execution_date_dashed:", ds)
    return True


def generate_and_sent_table_volume_notification(table_details, to_email_list):
    """
    Send email notification.
    input:
        table_details: json - details of the table.
        to_email_list: string - to email address.
    return:
        status: boolean
    """
    import importlib
    email_template_file = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.email_template")
    FROM_EMAIL_ADDRESS = 'do-not-reply@verizon.com'
    colour_code = "#A52A2A"
    table_string = ""
    environment_value = get_environment_value()
    email_subject = f"TraceX | {environment_value} | Warning | {table_details['table_name']} table size is outside of the threshold limit on {table_details['date']}"
    greeting_text = "Table size is outside of the threshold limit, please find the details below"

    volume_alert_template = email_template_file.volume_alert_template
    for key, value in table_details.items():
        table_string += f"""<tr>
                                <td>{key.capitalize().replace("_", " ")}</td>
                                <td>{value}</td>
                            </tr>"""
    email_body = volume_alert_template.format(greeting_text, colour_code, table_string, email_template_file.email_css)
    status = send_email(FROM_EMAIL_ADDRESS, to_email_list, email_subject, email_body)
    return status


def send_email(from_email, to_email, email_subject, email_body):
    """
    Send email notification.
    input:
        from_email: string - from email address.
        to_email: string - to email address.
        email_subject: string - email subject.
        email_body: string - email body
    return:
        status: boolean
    """
    import smtplib
    from email.message import EmailMessage
    try:
        msg = EmailMessage()
        msg.set_content(email_body, subtype='html')
        msg["Subject"] = email_subject
        msg["From"] = from_email
        msg["To"] = to_email
        smtp_object = smtplib.SMTP("vzsmtp.verizon.com", 25)
        smtp_object.starttls()
        smtp_object.send_message(msg)
        print("Successfully sent email!")
        smtp_object.quit()
        return True
    except Exception as e:
        print(f"----------------->>>>>>>>>>>>>Unable to send the email, {e}")
        return False


def generate_email_body(notification_type, process_details):
    """
    Generate Email Body.
    input:
        process_details: dict - Process details
        notification_type: string - Type of notification
    return:
        email_body: string - email body in HTML format.
    """
    import importlib
    from airflow.models import Variable
    ENVIRONMENT = Variable.get('composer_env')
    env_config_module = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.env_config")
    email_template_file = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.email_template")
    UNSUBSCRIBE_LINK = env_config_module.config[ENVIRONMENT]['unsubscribe_link']
    SUBSCRIBE_LINK = env_config_module.config[ENVIRONMENT]['subscribe_link']
    import string
    colour_code = {
        "success": "#2E8B57",
        "failure": "#cc0000",
        "sla_miss": "#e6b800",
        "long_pending": "#e6b800"
    }
    greeting_text = None
    if notification_type == 'failure':
        greeting_text = f"Process {process_details['process_name']} is failed. Please find the details below."
        # greeting_text = 'The below process is failed, Please find the details below.'
        del process_details['sla_time']
        del process_details['completion_time']
        del process_details['schedule_datetime']
    if notification_type == 'success':
        greeting_text = f"Process {process_details['process_name']} is successfully Completed. Please find the details below."
        # greeting_text = 'The below process is succeeded, Please find the details.'
        del process_details['sla_time']
        del process_details['failure_time']
        del process_details['schedule_datetime']
    if notification_type == 'sla_miss':
        # greeting_text = 'The process is missed SLA, Please find the details below.'
        greeting_text = f"The {process_details['process_name']} which is expected to be completed by {process_details['sla_time']} breached SLA. Please find the details below."
        del process_details['completion_time']
        del process_details['failure_time']
    if notification_type == 'long_pending':
        # greeting_text = 'The process is missed SLA, Please find the details below.'
        greeting_text = f"The {process_details['process_name']} is pending from {process_details['process_pending_from']}. Please find the details below."
    template = email_template_file.html_template
    table_string = ""
    for key, value in process_details.items():
        if 'process_status' == key:
            print("key check")
            stat = value
            stat_clr = ""
            print(stat)
            if stat.upper() in ["COMPLETED", "SUCCESS"]:
                print("success check")
                stat_clr = f"""<span style="display:inline-block;padding:4px 20px;background-color:#1b8241;color:#fff;border-radius:5px;">Success</span>"""
            if stat.upper() in ["FAILED", "FAILURE"]:
                stat_clr = f"""<span style="display:inline-block;padding:4px 20px;color:#fff;background-color:#bd1c1a;border-radius:5px;">Failed</span>"""
            if stat.upper() in ['PENDING', 'OVERDUE']:
                stat_clr = f"""<span style="display:inline-block;padding:4px 20px;background-color:#ffbc3d;border-radius:5px;">Pending</span>"""
            print(stat_clr)
            if '_' in key:
                table_string += f"""<tr><td style="padding-top:20px;padding-left:20px;padding-right:20px;border-bottom:1px solid #e0e0e0;padding-bottom:20px;font-weight:600;color:#6d6e70;">{string.capwords(key.replace("_", " "))}</td><td style="padding-top:20px;padding-left:20px;padding-right:20px;border-bottom:1px solid #e0e0e0;padding-bottom:20px;">{stat_clr}</td></tr>"""
            else:
                table_string += f"""<tr><td style="padding-top:20px;padding-left:20px;padding-right:20px;border-bottom:1px solid #e0e0e0;padding-bottom:20px;font-weight:600;color:#6d6e70;">{key}</td><td style="padding-top:20px;padding-left:20px;padding-right:20px;border-bottom:1px solid #e0e0e0;padding-bottom:20px;">{stat_clr}</td></tr>"""
        else:
            if '_' in key:
                table_string += f"""<tr><td style="padding-top:20px;padding-left:20px;padding-right:20px;border-bottom:1px solid #e0e0e0;padding-bottom:20px;font-weight:600;color:#6d6e70;">{string.capwords(key.replace("_", " "))}</td><td style="padding-top:20px;padding-left:20px;padding-right:20px;border-bottom:1px solid #e0e0e0;padding-bottom:20px;">{value}</td></tr>"""
            else:
                table_string += f"""<tr><td style="padding-top:20px;padding-left:20px;padding-right:20px;border-bottom:1px solid #e0e0e0;padding-bottom:20px;font-weight:600;color:#6d6e70;">{key}</td><td style="padding-top:20px;padding-left:20px;padding-right:20px;border-bottom:1px solid #e0e0e0;padding-bottom:20px;">{value}</td></tr>"""
    unsubscribe_link_formatted = UNSUBSCRIBE_LINK.format(process_details['process_name'],
                                                         process_details['program_name'],
                                                         process_details['application_name'])
    subscribe_link_formatted = SUBSCRIBE_LINK.format(process_details['process_name'], process_details['program_name'],
                                                     process_details['application_name'])
    return template.format(greeting_text, colour_code[notification_type], table_string,
                           email_template_file.process_email_css, unsubscribe_link_formatted, subscribe_link_formatted)


def evaluate_notifications(process_details, notification_type, email_subject, to_email_list, unique_id,
                           generated_notification_types, process_notification_uid_list):
    """
    Process reporting table data and send alert.
    input:
        process_details: dict - Process details
        notification_type: string - Notification type
        email_subject: string - Email subject
        to_email_list: list - list of email address.
        unique_id: string - table unique id
        generated_notification_types: list - list of generated notifications
        process_notification_uid_list: list - List of unique id of notifications which already generated in this execution.
    return:
        row_to_insert: dict - row to insert into alert status table.
    """
    import uuid
    from datetime import datetime
    FROM_EMAIL_ADDRESS = 'do-not-reply@verizon.com'
    row_to_insert = {"report_table_unique_id": unique_id,
                     "notification_created_timestamp": str(datetime.utcnow())
                     }
    if notification_type not in generated_notification_types and f"{unique_id}|{notification_type}" not in process_notification_uid_list:
        email_body = generate_email_body(notification_type, process_details)
        print("====================================================================")
        print("Email Body:", email_body)
        print("====================================================================")
        to_email_address = ','.join(to_email_list)
        status = send_email(FROM_EMAIL_ADDRESS, to_email_address, email_subject, email_body)
        if not status:
            return None
        row_to_insert['notification_types'] = notification_type
        row_to_insert['id'] = str(uuid.uuid4())
        return row_to_insert
    else:
        return None


def send_alert_to_opsgenie(message, description, details, priority, alert_type, additional_alert_metadata):
    """
    Send alert to opsgenie.
    input:
        message: string - The message which we want to send to opsgenie
        description: string - alert description
        details: json - alert details
        priority: string - Alert priority
    return:
        status: boolean
    """
    import json
    import time
    import importlib
    import requests
    from datetime import datetime
    from airflow.models import Variable
    environment_value = get_environment_value()
    ENVIRONMENT = Variable.get('composer_env')
    env_config_module = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.env_config")
    OPSGENIE_API_URL = env_config_module.config[ENVIRONMENT]['opsgenie_api']
    print(f"Started sending alert to opsgenie at {datetime.utcnow()}")
    OPSGENIE_API_KEY = env_config_module.config[ENVIRONMENT]['opsgenie_api_key']
    try:
        if details['opsgenie_team']:
            print(f"{details['opsgenie_team']}_opsgenie_api_key")
            OPSGENIE_API_KEY = Variable.get(f"{details['opsgenie_team']}_opsgenie_api_key", default_var=None)
            if not OPSGENIE_API_KEY:
                OPSGENIE_API_KEY = env_config_module.config[ENVIRONMENT][f"{details['opsgenie_team']}_opsgenie_api_key"]
                print("Fetched Opsgenie API key from config file.")
            else:
                print("Fetched Opsgenie API key from Airflow  variable.")
    except KeyError:
        print(
            f"Opsgenie Key for the team {details['opsgenie_team']} is not found in config file, sending alert to default group.")
        OPSGENIE_API_KEY = env_config_module.config[ENVIRONMENT]['opsgenie_api_key']

    opsgenie_auth_headers = {
        "Content-Type": "application/json",
        "Authorization": f"GenieKey {OPSGENIE_API_KEY}",
    }
    opsgenie_team = details['opsgenie_team']
    del details['opsgenie_team']
    # Prepare json data which needs to send to Opsgenie
    json_data = {
        "message": message,
        "description": description,
        "details": details,
        "priority": priority
    }
    # Add additional metadata information into the existing json data.
    json_data.update(additional_alert_metadata)

    # Convert tags into list.
    try:
        json_data['tags'] = json_data['tags'].split(",")
    except KeyError as e:
        json_data['tags'] = []

    # Add additional tags.
    try:
        json_data['tags'].extend([environment_value,
                                  alert_type,
                                  details['process_name'],
                                  details['program_name'],
                                  details['process_date']])
    except Exception as e:
        print(f"Error--------->{e}")

    if opsgenie_team:
        json_data["responders"] = [{"name": opsgenie_team, "type": "team"}]

    try:
        # Retry 5 times if the API request is failed.
        for i in range(1, 6):
            # Call Opsgenie alert creation API
            response = requests.post(OPSGENIE_API_URL,
                                     headers=opsgenie_auth_headers,
                                     json=json_data)
            # Check whether the response is 202 or not
            if response.status_code == 202:
                api_response = json.loads(response.text)
                print("Response:", api_response)
                return api_response["requestId"]
            # If the response is not 202 wait for some times and
            else:
                print(f"Status is not 200 ({str(response.text)}), retying")
                time.sleep(i * 5)
    except Exception as e:
        print("Unable to connect to Opsgenie, Error:", e)

    print(f"Ended sending alert to opsgenie at {datetime.utcnow()}")
    return None


def process_reporting_table_details_and_send_alert(process_details, message, description,
                                                   alert_type, unique_id, priority,
                                                   generated_alert_types, process_alert_uid,
                                                   additional_alert_metadata):
    """
    Process reporting table data and send alert.
    input:
        process_details: dict - Process details
        message: string - Alert message
        description: string - Alert description
        alert_type: string - Type of alert
        unique_id: string - table unique id
        priority: string - Alert priority
        generated_alert_types: list - list of generated alerts
        process_alert_uid: list - List of unique id of alerts which already generated in this execution.
    return:
        row_to_insert: dict - row to insert into alert status table.
    """
    import uuid
    from datetime import datetime
    print(f"process_reporting_table_details_and_send_alert function started execution at {datetime.utcnow()}")
    current_date_time = datetime.utcnow()
    row_to_insert = {"report_table_unique_id": unique_id,
                     "alert_created_date": str(current_date_time.date())
                     }
    # Check whether the alert which we currently fetched is already exist in alert status table or not.
    if alert_type not in generated_alert_types and f"{unique_id}|{alert_type}" not in process_alert_uid:
        alert_request_id = send_alert_to_opsgenie(message, description, process_details,
                                                  priority, alert_type, additional_alert_metadata)
        if alert_request_id is None:
            return None
        row_to_insert['alert_type'] = alert_type
        row_to_insert['alert_request_id'] = alert_request_id
        row_to_insert['id'] = str(uuid.uuid4())
        print(f"process_reporting_table_details_and_send_alert function started execution at {datetime.utcnow()}")
        return row_to_insert
    else:
        print(f"process_reporting_table_details_and_send_alert function started execution at {datetime.utcnow()}")
        return None


def send_dependency_alert_emails(dependency_response, email_list):
    """
    Send dependency email notification.
    input:
        dependency_response: json - details of the process and its dependent.
    return:
        status: boolean
    """
    import importlib
    email_template_file = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.email_template")
    FROM_EMAIL_ADDRESS = 'do-not-reply@verizon.com'
    environment_value = get_environment_value()
    dependency_alert_email_template = email_template_file.dependency_alert_email_template
    table_string = ""
    overall_status = None
    for data in dependency_response['dependency_process_details']:
        if not data['status']:
            overall_status = "failed"
        elif data['status'].lower() in ["pending", "failure", "progress"]:
            overall_status = "failed"
        table_string += f"""<tr>
                                <td>{data['dependency_process_name']}</td>
                                <td>{data['dependency_sub_process_name']}</td>
                                <td>{data['table_name']}</td>
                                <td>{data['process_date']}</td>
                                <td>{data['status']}</td>
                                <td>{data['poc']}</td>
                            </tr>"""
    if overall_status:
        email_subject = f"TraceX | {environment_value} | Alert | {dependency_response['process_name']} Process Dependency Failed - {dependency_response['process_date']}"
        greeting_text = f"{dependency_response['process_name']} Process dependency failed on {dependency_response['process_date']}, please find the dependencies below."
    else:
        email_subject = f"TraceX | {environment_value} | Notification | {dependency_response['process_name']} Process Dependency suceeded - {dependency_response['process_date']}"
        greeting_text = f"{dependency_response['process_name']} Process dependency succeeded on {dependency_response['process_date']}, please find the dependencies below."

    email_body = dependency_alert_email_template.format(email_template_file.dependency_alert_email_css,
                                                        greeting_text,
                                                        dependency_response['process_name'],
                                                        str(dependency_response['process_date']),
                                                        dependency_response['lineage_url'],
                                                        table_string
                                                        )
    # print("Email Body------->>>>>", email_body)
    response = send_email(FROM_EMAIL_ADDRESS, email_list, email_subject, email_body)
    if response:
        print("Email sent successfully.")
        return True
    else:
        print("Unable to sent the email.")
        return False


def convert_millisecond_to_date_string(millisecond):
    seconds = int(millisecond / 1000)
    day = seconds // (24 * 3600)
    hour = (seconds // 3600) % 24
    minute = (seconds % 3600) // 60
    second = seconds % 60
    return f"{day} days {hour:02}:{minute:02}:{second:02}"


def get_ticket_details(ticket_id):
    import requests
    import json
    OPS_KEY = "5e1f420c-5e64-4853-ad69-c888d583d6e7"
    headers = {"Content-Type": "application/json; charset=utf-8",
               "Authorization": f"GenieKey {OPS_KEY}"}
    opsgenie_base_url = f"https://api.opsgenie.com/v2/alerts/{ticket_id}?identifierType=id"

    response = requests.get(opsgenie_base_url, headers=headers)
    response = json.loads(response.text)
    return response


def load_data_to_staging_table(ticket_details, is_first_load, available_ids):
    import pandas as pd
    import importlib
    from datetime import datetime
    from google.cloud import bigquery
    from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
    from airflow.models import Variable
    ENVIRONMENT = Variable.get('composer_env')
    env_config_module = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.env_config")
    PROJECT_ID = env_config_module.config[ENVIRONMENT]['project_id']
    GCP_CONNECTION_ID = env_config_module.config[ENVIRONMENT]['gcp_connection_id']
    Obj = BigQueryHook(gcp_conn_id=GCP_CONNECTION_ID)
    bigquery_client = Obj.get_client(project_id=PROJECT_ID)
    DATASET_ID = env_config_module.config[ENVIRONMENT]['dataset_id']
    OPSGENIE_TICKET_STAGING_TABLE_ID = env_config_module.config[ENVIRONMENT]['opsgenie_ticket_staging_table_id']
    result_list = []

    print("Collected Ticket Details------------------>>>>>>", ticket_details)

    for data in ticket_details:
        opsgenie_ticket_info = dict()
        opsgenie_ticket_info['ticket_id'] = data['id']
        opsgenie_ticket_info['alias'] = data['alias']
        opsgenie_ticket_info['createdAt'] = datetime.strptime(data['createdAt'][:19], '%Y-%m-%dT%H:%M:%S')

        opsgenie_ticket_info['lastOccurredAt'] = datetime.strptime(data['lastOccurredAt'][:19], '%Y-%m-%dT%H:%M:%S')
        opsgenie_ticket_info['message'] = data['message']
        opsgenie_ticket_info['owner'] = data['owner']
        opsgenie_ticket_info['priority'] = data['priority']

        if data.get('report', None):
            report = data['report']
            opsgenie_ticket_info['ackTime'] = int(report.get('ackTime', 0))
            opsgenie_ticket_info['acknowledgedBy'] = report.get('acknowledgedBy', None)
            if report.get('closeTime', None):
                opsgenie_ticket_info['closeTime'] = convert_millisecond_to_date_string(int(report['closeTime']))
            else:
                opsgenie_ticket_info['closeTime'] = None
        else:
            opsgenie_ticket_info['ackTime'] = 0
            opsgenie_ticket_info['acknowledgedBy'] = None
            opsgenie_ticket_info['closeTime'] = None
        opsgenie_ticket_info['snoozed'] = data['snoozed']
        opsgenie_ticket_info['status'] = data['status']
        opsgenie_ticket_info['tinyId'] = int(data['tinyId'])
        opsgenie_ticket_info['updatedAt'] = datetime.strptime(data['updatedAt'][:19], '%Y-%m-%dT%H:%M:%S')
        opsgenie_ticket_info['responders'] = str(data['responders'])
        if data['id'] in available_ids:
            print("ID is available in BQ table")
            opsgenie_ticket_info['description'] = ''
            opsgenie_ticket_info['params'] = ''
        else:
            print("ID not available in BQ table")
            ticket_complete_info = get_ticket_details(data['id'])
            ticket_complete_info = ticket_complete_info['data']
            opsgenie_ticket_info['description'] = str(ticket_complete_info['description'])
            opsgenie_ticket_info['params'] = str(ticket_complete_info['details'])
        result_list.append(opsgenie_ticket_info)

    opsgenie_log_table_id = f"{DATASET_ID}.{OPSGENIE_TICKET_STAGING_TABLE_ID}"

    if is_first_load:
        write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE
    else:
        write_disposition = bigquery.WriteDisposition.WRITE_APPEND
    job_config = bigquery.LoadJobConfig(write_disposition=write_disposition,
                                        schema=[
                                            bigquery.SchemaField('ticket_id', 'STRING'),
                                            bigquery.SchemaField('alias', 'STRING'),
                                            bigquery.SchemaField('createdAt', 'TIMESTAMP'),
                                            bigquery.SchemaField('lastOccurredAt', 'TIMESTAMP'),
                                            bigquery.SchemaField('message', 'STRING'),
                                            bigquery.SchemaField('owner', 'STRING'),
                                            bigquery.SchemaField('priority', 'STRING'),
                                            bigquery.SchemaField('ackTime', 'INTEGER'),
                                            bigquery.SchemaField('acknowledgedBy', 'STRING'),
                                            bigquery.SchemaField('snoozed', 'BOOLEAN'),
                                            bigquery.SchemaField('status', 'STRING'),
                                            bigquery.SchemaField('tinyId', 'INTEGER'),
                                            bigquery.SchemaField('updatedAt', 'TIMESTAMP'),
                                            bigquery.SchemaField('responders', 'STRING'),
                                            bigquery.SchemaField('description', 'STRING'),
                                            bigquery.SchemaField('params', 'STRING'),
                                            bigquery.SchemaField('closeTime', 'STRING')
                                        ])
    df = pd.DataFrame(result_list)
    print("DF---------->>>>>>>", df)
    load_job = bigquery_client.load_table_from_dataframe(df,
                                                         opsgenie_log_table_id,
                                                         job_config=job_config)
    load_job.result()


def generate_and_sent_file_notification(file_details, to_email_list, email_subject, greeting_text, alert_type,
                                        report_table_unique_id):
    """
    Send email notification.
    input:
        table_details: json - details of the table.
        to_email_list: string - to email address.
    return:
        status: boolean
    """
    print("---------------->>>>>>>> Inside file notification function.")
    import uuid
    import string
    import importlib
    from datetime import datetime
    email_template_file = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.email_template")
    FROM_EMAIL_ADDRESS = 'do-not-reply@verizon.com'
    file_notification_email_template = email_template_file.file_notification_template
    table_string = ""
    for key, value in file_details.items():
        if 'file_arrival_status' == key:
            print("key check")
            stat = value
            stat_clr = ""
            print(stat)
            if stat.upper() == 'PENDING':
                stat_clr = f"""<span style="display:inline-block;padding:4px 20px;background-color:#ffbc3d;border-radius:5px;">Pending</span>"""
            print(stat_clr)
            if '_' in key:
                table_string += f"""<tr><td style="padding-top:20px;padding-left:20px;padding-right:20px;border-bottom:1px solid #e0e0e0;padding-bottom:20px;font-weight:600;color:#6d6e70;">{string.capwords(key.replace("_", " "))}</td><td style="padding-top:20px;padding-left:20px;padding-right:20px;border-bottom:1px solid #e0e0e0;padding-bottom:20px;">{stat_clr}</td></tr>"""
            else:
                table_string += f"""<tr><td style="padding-top:20px;padding-left:20px;padding-right:20px;border-bottom:1px solid #e0e0e0;padding-bottom:20px;font-weight:600;color:#6d6e70;">{key}</td><td style="padding-top:20px;padding-left:20px;padding-right:20px;border-bottom:1px solid #e0e0e0;padding-bottom:20px;">{stat_clr}</td></tr>"""
        else:
            if '_' in key:
                table_string += f"""<tr><td style="padding-top:20px;padding-left:20px;padding-right:20px;border-bottom:1px solid #e0e0e0;padding-bottom:20px;font-weight:600;color:#6d6e70;">{string.capwords(key.replace("_", " "))}</td><td style="padding-top:20px;padding-left:20px;padding-right:20px;border-bottom:1px solid #e0e0e0;padding-bottom:20px;">{value}</td></tr>"""
            else:
                table_string += f"""<tr><td style="padding-top:20px;padding-left:20px;padding-right:20px;border-bottom:1px solid #e0e0e0;padding-bottom:20px;font-weight:600;color:#6d6e70;">{key}</td><td style="padding-top:20px;padding-left:20px;padding-right:20px;border-bottom:1px solid #e0e0e0;padding-bottom:20px;">{value}</td></tr>"""

    email_body = file_notification_email_template.format(email_template_file.process_email_css, greeting_text,
                                                         table_string)

    print("Email body----------------->>>")
    print("---------------------------------------->>>>>>>>>>>>>>>>>>>>>>>>>>>>>")
    print(email_body)
    print("---------------------------------------->>>>>>>>>>>>>>>>>>>>>>>>>>>>>")
    status = send_email(FROM_EMAIL_ADDRESS, to_email_list, email_subject, email_body)
    print("Email sent status---------------->>>>>>>>>>>", status)

    if status:
        row_to_insert = {
            "id": str(uuid.uuid4()),
            "report_table_unique_id": report_table_unique_id,
            "alert_created_date": str(datetime.utcnow().date()),
            "alert_type": alert_type,
            "alert_tool": "email"
        }
        return row_to_insert
    else:
        return False


def generate_process_alert(**context):
    """
    Generate and send process alerts to Opsgenie.
    input:
        **context: task variable
    return:
        status: boolean
    """
    import re
    import json
    import importlib
    from datetime import datetime, timedelta
    from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
    from airflow.models import Variable
    environment_value = get_environment_value()
    ENVIRONMENT = Variable.get('composer_env')
    env_config_module = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.env_config")
    DATASET_ID = env_config_module.config[ENVIRONMENT]['dataset_id']
    PROJECT_ID = env_config_module.config[ENVIRONMENT]['project_id']
    GCP_CONNECTION_ID = env_config_module.config[ENVIRONMENT]['gcp_connection_id']
    Obj = BigQueryHook(gcp_conn_id=GCP_CONNECTION_ID)
    bigquery_client = Obj.get_client(project_id=PROJECT_ID)
    PROCESS_ALERT_STATUS_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_alert_status_table_id']
    PROCESS_METADATA_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_metadata_table_id']
    REALTIME_PROCESS_META_TABLE_ID = env_config_module.config[ENVIRONMENT]['realtime_process_meta_table_id']
    PROCESS_MONITORING_REPORT_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_monitoring_report_table_id']
    VIEW_DATASET_ID = env_config_module.config[ENVIRONMENT]['view_dataset_id']
    TAG_RELATION_VIEW_ID = env_config_module.config[ENVIRONMENT]['tag_relation_view_id']
    PROCESS_ALERT_NOTIFICATION_META_TABLE_ID = env_config_module.config[ENVIRONMENT][
        'dof_process_alert_notification_meta_table_id']
    existing_opsgenie_alert_metadata_fields = ["tags", "entity", "source", "note"]
    priority_list = ["P1", "P2", "P3", "P4", "P5"]
    PROGRAM_NAME_MAPPING_VIEW_ID = env_config_module.config[ENVIRONMENT]['program_name_mapping_view_id']
    PROCESS_OPSGENIE_ALERT_STATUS_TABLE_ID = env_config_module.config[ENVIRONMENT][
        'process_opsgenie_alert_status_table_id']
    task_instance = context['task_instance']
    execution_date = task_instance.xcom_pull(key='execution_date_dashed')
    print("Type of execution date:", type(execution_date))
    # Get previous execution date to fetch details from alerting table.
    previous_day_of_execution_date = str(datetime.strptime(str(execution_date), "%Y-%m-%d") - timedelta(1))[:10]
    print("previous_day_of_execution_date:", previous_day_of_execution_date)
    # Query to fetch details from process report table

    get_process_details_query = f"""SELECT
                                  *
                                FROM (
                                  SELECT
                                    DISTINCT *EXCEPT(opsgenie_teams),
                                    REPLACE(ARRAY_TO_STRING( [program_name, project_name, process_name, SAFE_CAST(process_date AS STRING), COALESCE(running_instance_id,SAFE_CAST(trace_id AS STRING), '1'), opsgenie_team, application_name],'|'), ' ', '') AS unique_id,
                                  FROM (
                                    SELECT
                                      reporting.*,
                                      pgm_name_map.*,
                                      IFNULL(alert_failure, "N") as alert_failure,
                                      IFNULL(process_failed_alert_priority, "P3") AS process_failed_alert_priority,
                                      IFNULL(alert_sla_misses, "N") as alert_sla_misses,
                                      process_sla_miss_alert_priority,
                                      IFNULL(process_sla_miss_alert_priority, "P3") AS process_sla_miss_alert_priority,
                                      alert_metadata,
                                      alert_additional_info,
                                      alert_required_columns,
                                      sla_buffer,
                                      updated_at,
                                      suppress_alert,
                                      IFNULL(alert_pending, "N") as alert_pending,
                                      IFNULL(process_pending_alert_priority, "P3") AS process_pending_alert_priority,        
                                      SPLIT(IFNULL("", ""), ',') AS opsgenie_teams
                                    FROM
                                      (
                                        SELECT
                                          "" AS scheduler_name,
                                          "" AS process_poc,
                                          frequency,
                                          expected_run_date AS schedule_datetime,
                                          application_name,
                                          process_id,
                                          process_name,
                                          sub_process,
                                          project_name,
                                          project_name AS program_name,
                                          process_date,
                                          run_status AS process_status,
                                          start_time,
                                          end_time,
                                          composer_env,
                                          PARSE_DATE('%Y%m%d', run_date) AS run_date,
                                          run_hour,
                                          CAST(inserted_timestamp AS TIMESTAMP) AS inserted_timestamp,
                                          tool_name,
                                          system_name,
                                          gcp_project,
                                          target_name,
                                          target_type,
                                          source_name,
                                          source_type,
                                          error_message,
                                          step_id,
                                          process_start_time,
                                          process_end_time,
                                          trace_id,
                                          timezone,
                                          running_instance_id,
                                          sla AS expected_datetime,
                                          try_number,
                                          sla,
                                          sla_met,
                                          process_status AS overall_process_status,
                                          process_run_duration
                                        FROM
                                          `aid_epdo_prd_tbls.dof_data_observability_v2`
                                        WHERE process_date >= '{previous_day_of_execution_date}'
                                      ) AS reporting
                                    LEFT JOIN
                                      `{DATASET_ID}.{PROCESS_ALERT_NOTIFICATION_META_TABLE_ID}` notification_alert_meta
                                    ON
                                      reporting.process_name = notification_alert_meta.process_name
                                      AND reporting.application_name = notification_alert_meta.application_name
                                      AND reporting.program_name = notification_alert_meta.program_name
                                    LEFT JOIN
                                      `{VIEW_DATASET_ID}.{PROGRAM_NAME_MAPPING_VIEW_ID}` as pgm_name_map
                                    ON
                                      LOWER(reporting.program_name) = LOWER(pgm_name_map.meta_program_name)
                                    WHERE
                                      (suppress_alert is False or suppress_alert is NULL)
                                      AND LOWER(reporting.program_name) NOT IN ("5g",
                                          "oneex",
                                          "5g_esp",
                                          "exec report",
                                          "executive report",
                                          "trade in promo",
                                          "live person chat",
                                          "dapr",
                                          "one talk(daapr)",
                                          "5g trade in employee profile")
                                      AND (LOWER(notification_alert_meta.alert_tool) = 'opsgenie' OR notification_alert_meta.alert_tool IS NULL OR notification_alert_meta.alert_tool = '')
                                      AND (LOWER(notification_alert_meta.notification_level) = 'process' OR notification_alert_meta.notification_level is NULL OR notification_alert_meta.notification_level='')
                                      AND ((LOWER(reporting.process_status) in ('failure', 'failed')
                                          AND LOWER(notification_alert_meta.alert_failure) = 'y')
                                        OR (reporting.sla_met IS NOT NULL
                                          AND LOWER(reporting.sla_met) = 'no'
                                          AND (LOWER(notification_alert_meta.alert_sla_misses) = 'y' OR LOWER(notification_alert_meta.alert_pending) = 'y')
                                          AND LOWER(reporting.overall_process_status) IN ("pending", "in progress", "overdue")))) AS master
                                  CROSS JOIN
                                    UNNEST(master.opsgenie_teams) AS opsgenie_team)
                                LEFT JOIN (
                                  SELECT
                                    metadata_id,
                                    ARRAY_AGG(tag_value) tag_value
                                  FROM
                                    `{VIEW_DATASET_ID}.{TAG_RELATION_VIEW_ID}`
                                  WHERE
                                    LOWER(is_active) = 'y'
                                  GROUP BY
                                    metadata_id)
                                ON
                                  metadata_id = process_id"""

    print(get_process_details_query)
    query_job = bigquery_client.query(get_process_details_query)
    process_list = query_job.result()
    process_alert_to_insert = list()
    process_alert_uid_list = list()

    # Fetch all the alert generated alerts for past two days.
    seven_days_back_date = str(datetime.strptime(str(execution_date), "%Y-%m-%d") - timedelta(7))[:10]

    generated_alert_id_and_type = list()
    # ===================================================================================================================
    get_details_from_alert_status_table_query = f"""SELECT
                                          CONCAT(report_table_unique_id,"|", alert_type) AS generated_alert_id_and_type
                                        FROM
                                          `{DATASET_ID}.{PROCESS_ALERT_STATUS_TABLE_ID}`
                                        WHERE
                                          alert_created_date >= '{seven_days_back_date}'"""
    query_job = bigquery_client.query(get_details_from_alert_status_table_query)
    alert_type_list = query_job.result()

    # Create a list of generated alert.
    for alert in alert_type_list:
        generated_alert_id_and_type.append(alert['generated_alert_id_and_type'])
    # ===================================================================================================================

    _get_details_from_alert_status_table_query = f"""SELECT
                                              CONCAT(report_table_unique_id,"|", alert_type) AS generated_alert_id_and_type
                                            FROM
                                              `{DATASET_ID}.{PROCESS_OPSGENIE_ALERT_STATUS_TABLE_ID}`
                                            WHERE
                                              DATE(created_timestamp) >= '{seven_days_back_date}'"""

    _query_job = bigquery_client.query(_get_details_from_alert_status_table_query)
    _alert_type_list = _query_job.result()

    # Create a list of generated alert.
    for alert in _alert_type_list:
        generated_alert_id_and_type.append(alert['generated_alert_id_and_type'])
    DAG_BASE_PATH = env_config_module.config[ENVIRONMENT]['airflow_base_path']
    for process in process_list:
        schedule_datetime_utc = str(process['schedule_datetime'])[:19]
        if process['end_time']:
            end_time = str(process['end_time'])[:19]
        else:
            end_time = schedule_datetime_utc

        print(f"========Iteration for process {process['process_name']} is started at {datetime.utcnow()}========.")
        expected_datetime_utc = str(process['expected_datetime'])[:19]
        # Create process details dict which we need to send to Opsgenie.
        process_details = {"program_name": process['program_name'],
                           "process_name": process['process_name'],
                           "frequency": process['frequency'],
                           "schedule_datetime": f"{schedule_datetime_utc} UTC",
                           "process_date": str(process['process_date']),
                           "process_poc": process['process_poc'],
                           "application_name": process['application_name'],
                           "sla_time": f"{expected_datetime_utc} UTC",
                           "opsgenie_team": process['opsgenie_team']}
        if process['scheduler_name'] and process['scheduler_name'].lower() == "airflow":
            process_details['DAG URL'] = f"{DAG_BASE_PATH}{process['process_name']}"

        # Add required fields from reporting table into the alert.
        alert_required_columns = process['alert_required_columns']
        if alert_required_columns:
            try:
                # Remove white spaces.
                alert_required_columns = alert_required_columns.replace(" ", "")
                # Split the value using comma.
                alert_required_columns = alert_required_columns.split(",")
                for column in alert_required_columns:
                    try:
                        # Add the fields into process details dict.
                        process_details[column] = process[column]
                    except KeyError:
                        print(f"Error: {column} is not found inside reporting table.")
            except Exception as e:
                print(f"Error:{e}")

        # Add additional info to alert.
        alert_additional_info = process['alert_additional_info']
        if alert_additional_info:
            try:
                # Convert the data to json
                alert_additional_info = json.loads(alert_additional_info)
                for key, value in alert_additional_info.items():
                    process_details[key] = value
            except Exception as e:
                print(f"Error:{e}")

        # Add additional metadata information to the alert.
        additional_alert_metadata = {}
        alert_metadata = process['alert_metadata']
        if alert_metadata:
            try:
                # Convert the data to json
                alert_metadata = json.loads(alert_metadata)
                for key, value in alert_metadata.items():
                    if key in existing_opsgenie_alert_metadata_fields:
                        additional_alert_metadata[key] = value
            except Exception as e:
                print(f"Error:{e}")

        # Add tags
        if process['tag_value']:
            existing_tags = ','.join(process['tag_value'])
            try:
                if additional_alert_metadata['tags']:
                    additional_alert_metadata['tags'] += f",{existing_tags}"
            except KeyError as e:
                additional_alert_metadata['tags'] = existing_tags
                print("additional_alert_metadata------>", additional_alert_metadata)
                print(f"Error:{e}")
        """
        if process['vsad']:
            try:
                if additional_alert_metadata['tags']:
                    additional_alert_metadata['tags'] += f",{process['vsad']}"
            except KeyError as e:
                additional_alert_metadata['tags'] = process['vsad']
                print("additional_alert_metadata------>", additional_alert_metadata)
                print(f"Error:{e}")
        """

        print(f"additional_alert_metadata----->>>{additional_alert_metadata}")

        # If a process failed
        date_format = '%Y-%m-%d %H:%M:%S'
        failed_time = datetime.strptime(end_time, date_format)
        if process['process_status'].lower() in ('failure', 'failed') and process['alert_failure'].lower() == 'y' \
                and process['overall_process_status'] and process['overall_process_status'].lower() == 'failed' \
                and f"{process['unique_id']}|process_failed" not in generated_alert_id_and_type \
                and f"{process['unique_id']}|Failure" not in generated_alert_id_and_type\
                and (process['suppress_alert'] is None or (process['suppress_alert'] is False and process['updated_at'] and process['updated_at'] < failed_time)):
            # Define details for the alert
            alert_type = 'Failure'

            process_details["error_message"] = process['error_message']
            _message = f"TraceX | {environment_value} | {alert_type} | {process['process_name']} | {process['application_name']} | {process['program_name']} | {end_time} UTC"

            priority = process.get('process_failed_alert_priority', 'P3')
            if priority.upper() not in priority_list:
                priority = "P3"
            description = f"The following process is failed, Please find the details in extra properties section."
            # Send the details to process and send alert.
            row_to_insert = process_reporting_table_details_and_send_alert(process_details, _message, description,
                                                                           alert_type,
                                                                           process['unique_id'], priority,
                                                                           [],
                                                                           process_alert_uid_list,
                                                                           additional_alert_metadata)
            # If the row needs to insert to BigQuery table append to the below list
            if row_to_insert:
                data_to_insert = {
                    "ticket_id": row_to_insert['alert_request_id'],
                    "tool_name": "opsgenie",
                    "process_name": process_details['process_name'],
                    "application_name": process_details['application_name'],
                    "program_name": process_details['program_name'],
                    "process_date": str(process_details['process_date']),
                    "alert_type": alert_type,
                    "process_timestamp": end_time,
                    "created_timestamp": str(datetime.now()),
                    "message": _message,
                    "alert_team": process['opsgenie_team'],
                    "report_table_unique_id": process['unique_id']
                }
                process_alert_to_insert.append(data_to_insert)
                process_alert_uid_list.append(f"{process['unique_id']}|{alert_type}")

        # Alerting for process missed SLA
        sla_time = datetime.strptime(str(process['sla'])[:19], date_format)
        if process['sla_met'] and process['sla_met'].lower() == 'no' and \
                (process['alert_sla_misses'].upper() == 'Y' or process['alert_pending'].upper() == 'Y') \
                and f"{process['unique_id']}|process_sla_not_met" not in generated_alert_id_and_type \
                and f"{process['unique_id']}|Pending" not in generated_alert_id_and_type \
                and f"{process['unique_id']}|SLA" not in generated_alert_id_and_type\
                and (process['suppress_alert'] is None or (process['suppress_alert'] is False and process['updated_at'] and process['updated_at'] < sla_time)):

            # Define details for the alert
            if process['overall_process_status'].lower() in ('pending', 'overdue') and process['alert_pending'].upper() == 'Y':
                alert_type = "SLA"
                _message = f"TraceX | {environment_value} | SLA Breach | {process['process_name']} | {process['application_name']} | {process['program_name']} | {expected_datetime_utc} UTC"

                description = f"Process {process['process_name']} which is expected to be completed by {process_details['sla_time']} is breached SLA. Please find the details in extra properties section."

                priority = process.get('process_pending_alert_priority', 'P3')
                if priority.upper() not in priority_list:
                    priority = "P3"
            elif process['overall_process_status'].lower() == 'in progress' and process['alert_sla_misses'].upper() == 'Y':
                alert_type = "SLA"
                _message = f"TraceX | {environment_value} | SLA Breach|{process['process_name']} | {process['application_name']} | {process['program_name']} | {expected_datetime_utc} UTC"
                description = f"Process {process['process_name']} which is expected to be completed by {process_details['sla_time']} is breached SLA. Please find the details in extra properties section."

                priority = process.get('process_sla_miss_alert_priority', 'P3')
                if priority.upper() not in priority_list:
                    priority = "P3"
            else:
                continue
            row_to_insert = None

            # message = f"Observability Notification({environment_value}): SLA missed for the process {process['process_name']} on {expected_datetime_utc} UTC"

            # Check whether the sla_notification_buffer is greater than 0 or not.
            send_email_flag = False
            if process['sla_buffer']:
                time_component = process['sla_buffer'].replace(" ", "")
                if re.match(r"^\d\d:\d\d:\d\d$", time_component):
                    time_component = process['sla_buffer'].split(":")
                    sla_date_time = datetime.strptime(str(process['expected_datetime'])[0:19], '%Y-%m-%d %H:%M:%S')

                    sla_buffer_added_date_time = sla_date_time + timedelta(hours=int(time_component[0]),
                                                                           minutes=int(time_component[1]),
                                                                           seconds=int(time_component[2]))
                    print("sla_buffer_added_date_time:", sla_buffer_added_date_time)
                    print("Current Date and Time:", datetime.utcnow())
                    print("sla_buffer_added_date_time:", sla_buffer_added_date_time)
                    # If the current datetime is greater than SLA + sla_notification_buffer then, send the alert
                    if datetime.utcnow() > sla_buffer_added_date_time and \
                            process['process_status'].lower() in ("pending", "in progress"):
                        send_email_flag = True
                    else:
                        print("Buffer condition failed.")
                else:
                    send_email_flag = True
                    print("Value in SLA buffer is not a valid time format.")
            # If sla_notification_buffer is less than or equal to zero send the alert.
            else:
                send_email_flag = True
            if send_email_flag:
                row_to_insert = process_reporting_table_details_and_send_alert(process_details, _message, description,
                                                                               alert_type,
                                                                               process['unique_id'], priority,
                                                                               [],
                                                                               process_alert_uid_list,
                                                                               additional_alert_metadata)

            # If the row needs to insert to BigQuery table append to the below list
            if row_to_insert:
                data_to_insert = {
                    "ticket_id": row_to_insert['alert_request_id'],
                    "tool_name": "opsgenie",
                    "process_name": process_details['process_name'],
                    "application_name": process_details['application_name'],
                    "program_name": process_details['program_name'],
                    "process_date": str(process_details['process_date']),
                    "alert_type": alert_type,
                    "process_timestamp": end_time,
                    "created_timestamp": str(datetime.now()),
                    "message": _message,
                    "alert_team": process['opsgenie_team'],
                    "report_table_unique_id": process['unique_id']
                }
                process_alert_to_insert.append(data_to_insert)
                process_alert_uid_list.append(f"{process['unique_id']}|{alert_type}")

    print("==========================================================================")
    print(process_alert_to_insert)
    print("==========================================================================")
    # If there is any dict inside process_alert_to_insert then insert it into BigQuery
    if process_alert_to_insert:
        print(f"Started BQ insert at {datetime.utcnow()}")
        errors = bigquery_client.insert_rows_json(f"{PROJECT_ID}.{DATASET_ID}.{PROCESS_OPSGENIE_ALERT_STATUS_TABLE_ID}",
                                                  process_alert_to_insert)
        if errors:
            print("Encountered errors while inserting rows: {}".format(errors))
        print(f"Ended BQ insert at {datetime.utcnow()}")
    return True


def generate_file_alert(**context):
    """
    Generate and send file alert to Opsgenie.
    input:
        **context: task variable
    return:
        status: boolean
    """
    import json
    import importlib
    from airflow.models import Variable
    from datetime import datetime, timedelta
    from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
    env_config_module = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.env_config")
    ENVIRONMENT = Variable.get('composer_env')
    environment_value = get_environment_value()
    DATASET_ID = env_config_module.config[ENVIRONMENT]['dataset_id']
    PROJECT_ID = env_config_module.config[ENVIRONMENT]['project_id']
    GCP_CONNECTION_ID = env_config_module.config[ENVIRONMENT]['gcp_connection_id']
    Obj = BigQueryHook(gcp_conn_id=GCP_CONNECTION_ID)
    bigquery_client = Obj.get_client(project_id=PROJECT_ID)
    FILE_METADATA_TABLE_ID = env_config_module.config[ENVIRONMENT]['file_metadata_table_id']
    FILE_ALERT_STATUS_TABLE_ID = env_config_module.config[ENVIRONMENT]['file_alert_status_table_id']
    FILE_MONITORING_TABLE_ID = env_config_module.config[ENVIRONMENT]['file_monitoring_table_id']
    priority_list = ["P1", "P2", "P3", "P4", "P5"]
    alert_tool = 'opsgenie'
    task_instance = context['task_instance']
    execution_date = task_instance.xcom_pull(key='execution_date_dashed')
    print("Type of execution date:", type(execution_date))
    # Get previous execution date to fetch details from alerting table.
    previous_day_of_execution_date = str(datetime.strptime(str(execution_date), "%Y-%m-%d") - timedelta(1))[:10]
    print("previous_day_of_execution_date:", previous_day_of_execution_date)
    # Query to fetch details from file report table
    get_file_details_query = f"""SELECT
                                  REPLACE(
                                    ARRAY_TO_STRING(
                                      [reporting.program_name, 
                                      reporting.process_name, 
                                      SAFE_CAST(reporting.process_date AS STRING),
                                      SAFE_CAST(reporting.sla AS STRING), 
                                      reporting.file_pattern, 
                                      reporting.received_filename, 
                                      reporting.project_name, 
                                      reporting.file_arrival_status, 
                                      reporting.application_name, 
                                      reporting.frequency],'|'), " ", "") AS unique_id,
                                      *
                                FROM
                                  `{PROJECT_ID}.{DATASET_ID}.{FILE_MONITORING_TABLE_ID}` AS reporting

                                LEFT JOIN
                                  `{PROJECT_ID}.{DATASET_ID}.{FILE_METADATA_TABLE_ID}` as metadata
                                ON
                                  reporting.application_name = metadata.application_name
                                  AND reporting.file_id = metadata.file_id
                                  AND reporting.file_pattern = metadata.file_name
                                WHERE
                                  reporting.process_date >= '{previous_day_of_execution_date}'
                                  AND LOWER(metadata.is_active) = 'y'
                                  AND ((LOWER(metadata.notify_file_size) = 'y')
                                  OR (reporting.sla_met IS NOT NULL AND LOWER(reporting.sla_met) = 'no' AND LOWER(metadata.notify_sla_misses) = 'y'))"""

    print(get_file_details_query)
    query_job = bigquery_client.query(get_file_details_query)
    file_list = query_job.result()
    file_alert_to_insert = list()
    file_alert_uid = list()

    # -----------------------------------------------------------------
    # Get details of alerts created for last two days.
    seven_days_back_date = str(datetime.strptime(str(execution_date), "%Y-%m-%d") - timedelta(7))[:10]
    get_details_from_alert_status_table_query = f"""SELECT
                                              CONCAT(report_table_unique_id,"|",alert_tool,"|",alert_type) AS generated_alert_id_and_type
                                            FROM
                                              `{DATASET_ID}.{FILE_ALERT_STATUS_TABLE_ID}`
                                            WHERE
                                              alert_created_date >= '{seven_days_back_date}'"""

    query_job = bigquery_client.query(get_details_from_alert_status_table_query)
    alert_type_list = query_job.result()
    generated_alert_id_and_type = list()
    # Create a list of generated alert.
    for alert in alert_type_list:
        generated_alert_id_and_type.append(alert['generated_alert_id_and_type'])

    # -----------------------------------------------------------------
    for file in file_list:
        # Create file details dict which we need to send to Opsgenie.
        file_details = {"program_name": file['program_name'],
                        "process_name": file['process_name'],
                        "process_date": str(file['process_date']),
                        "project_name": file['project_name'],
                        "source_poc": file['source_poc'],
                        "file_arrival_status": file['file_arrival_status'],
                        "application_name": file['application_name'],
                        "file_pattern": file['file_pattern'],
                        "sla_time": str(file['sla']),
                        "opsgenie_team": file['opsgenie_team']
                        }

        additional_alert_metadata = {}
        # Get value from alert_info field
        alert_info = file['alert_info']
        # Check whether the  is null or not.
        if not alert_info or alert_info.upper() == 'null':
            # set alert_info as empty.
            alert_info = {}
        else:
            try:
                # convert alert_info into JSON.
                alert_info = json.loads(alert_info)
            except Exception as e:
                alert_info = {}
                print("Invalid format in alert_info field.")

        # Get the required_columns details from alert_info column
        required_columns = alert_info.get('required_columns', None)
        if required_columns:
            try:
                # Remove white spaces.
                required_columns = required_columns.replace(" ", "")
                # Split the value using comma.
                required_columns = required_columns.split(",")
                for column in required_columns:
                    try:
                        # Add the fields into process details dict.
                        file_details[column] = file[column]
                    except KeyError:
                        print(f"Error: {column} is not found inside reporting table.")
            except Exception as e:
                print(f"Error:{e}")

        """
        # Generate file size alert
        if file['file_size'] and (file['notify_file_size'] and file['notify_file_size'].upper() == 'Y'):
            try:
                file_size_in_float = float(file['file_size'])
                file_size_limit = float(file['file_size_limit'])
                if file_size_in_float > file_size_limit:
                    # Define details for the alert
                    priority = alert_info.get('file_size_alert_priority', 'P3')
                    if priority.upper() not in priority_list:
                        priority = "P3"
                    alert_type = 'file_size_exceeded_limit'
                    message = f'Observability Notification({environment_value}): File size is more than {file_size_limit} MB'
                    description = f"Your file size is {str(file_size_in_float)}, Please have a look"
                    # Send the details to process and send alert.
                    row_to_insert = process_reporting_table_details_and_send_alert(file_details,
                                                                                   message,
                                                                                   description,
                                                                                   alert_type,
                                                                                   file['unique_id'],
                                                                                   priority,
                                                                                   [],
                                                                                   file_alert_uid,
                                                                                   additional_alert_metadata)

                    # If the row needs to insert to BigQuery table append to the below list
                    if row_to_insert:
                        file_alert_uid.append(f"{file['unique_id']}|{alert_type}")
                        file_alert_to_insert.append(row_to_insert)
            except Exception as e:
                print("Error while try to cast file_size into float:", e)
        """

        # Alerting for files missed SLA.
        if file['sla_met'] and file['sla_met'].lower() == 'no' and \
                (file['notify_sla_misses'] and file['notify_sla_misses'].upper() == 'Y') and \
                f"{file['unique_id']}|{alert_tool}|file_sla_not_met" not in generated_alert_id_and_type:
            # Define details for the alert
            priority = alert_info.get('sla_alert_priority', 'P3')
            if priority.upper() not in priority_list:
                priority = "P3"
            alert_type = 'file_sla_not_met'
            # message = f"Observability Notification({environment_value}): File SLA missed for file {file['file_pattern']} on {str(file['process_date'])} at {str(file['sla'])}"
            message = f"DOF|{environment_value}|SLA|{file['file_pattern']}|{str(file['process_date'])}|{str(file['sla'])}"
            description = "File SLA is not met, please have look"
            # Send the details to process and send alert.
            row_to_insert = None
            if file['sla_notification_buffer']:
                # Create SLA date time
                sla_date_time = datetime.strptime(f"{str(file['process_date'])} {str(file['sla'])}",
                                                  '%Y-%m-%d %H:%M:%S')
                # Add SLA date time and sla_notification_buffer
                try:
                    time_component = file['sla_notification_buffer'].split(":")
                    sla_buffer_added_date_time = sla_date_time + timedelta(hours=int(time_component[0]),
                                                                           minutes=int(time_component[1]),
                                                                           seconds=int(time_component[2]))
                except Exception as e:
                    print("Invalid time format")
                    continue

                print("Current Date and Time:", datetime.utcnow())
                print("sla_buffer_added_date_time:", sla_buffer_added_date_time)
                # If the current datetime is greater than SLA + sla_notification_buffer then send the alert
                if datetime.utcnow() > sla_buffer_added_date_time and file['file_arrival_status'].lower() == 'pending':
                    print("Inside SLA Tab")
                    row_to_insert = process_reporting_table_details_and_send_alert(file_details, message, description,
                                                                                   alert_type,
                                                                                   file['unique_id'], priority,
                                                                                   [],
                                                                                   file_alert_uid,
                                                                                   additional_alert_metadata)
            # If sla_notification_buffer is less than or equal to zero send the alert.
            else:
                row_to_insert = process_reporting_table_details_and_send_alert(file_details, message, description,
                                                                               alert_type,
                                                                               file['unique_id'], priority,
                                                                               [],
                                                                               file_alert_uid,
                                                                               additional_alert_metadata)
            # If the row needs to insert to BigQuery table append to the below list
            if row_to_insert:
                row_to_insert['alert_tool'] = 'opsgenie'
                file_alert_uid.append(f"{file['unique_id']}|{alert_type}")
                file_alert_to_insert.append(row_to_insert)

    print("==========================================================================")
    print(file_alert_to_insert)
    print("==========================================================================")
    # If there is any dict inside process_alert_to_insert then insert it into BigQuery
    if file_alert_to_insert:
        errors = bigquery_client.insert_rows_json(f"{PROJECT_ID}.{DATASET_ID}.{FILE_ALERT_STATUS_TABLE_ID}",
                                                  file_alert_to_insert)
        if errors:
            print("Encountered errors while inserting rows: {}".format(errors))
    return True


def generate_process_notification(**context):
    """
    Generate and send process email notification.
    input:
        **context: task variable
    return:
        status: boolean
    """
    import re
    import json
    import importlib
    from datetime import datetime, timedelta
    from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
    from airflow.models import Variable
    ENVIRONMENT = Variable.get('composer_env')
    environment_value = get_environment_value()
    env_config_module = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.env_config")
    DATASET_ID = env_config_module.config[ENVIRONMENT]['dataset_id']
    PROJECT_ID = env_config_module.config[ENVIRONMENT]['project_id']
    GCP_CONNECTION_ID = env_config_module.config[ENVIRONMENT]['gcp_connection_id']
    Obj = BigQueryHook(gcp_conn_id=GCP_CONNECTION_ID)
    bigquery_client = Obj.get_client(project_id=PROJECT_ID)

    PROCESS_METADATA_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_metadata_table_id']
    REALTIME_PROCESS_META_TABLE_ID = env_config_module.config[ENVIRONMENT]['realtime_process_meta_table_id']
    PROCESS_MONITORING_REPORT_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_monitoring_report_table_id']
    PROCESS_NOTIFICATION_STATUS_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_notification_status_table_id']
    VIEW_DATASET_ID = env_config_module.config[ENVIRONMENT]['view_dataset_id']
    TAG_RELATION_VIEW_ID = env_config_module.config[ENVIRONMENT]['tag_relation_view_id']
    PROCESS_ALERT_NOTIFICATION_META_TABLE_ID = env_config_module.config[ENVIRONMENT][
        'dof_process_alert_notification_meta_table_id']
    LINEAGE_BASE_URL = env_config_module.config[ENVIRONMENT]['lineage_base_url']
    PROGRAM_NAME_MAPPING_VIEW_ID = env_config_module.config[ENVIRONMENT]['program_name_mapping_view_id']
    task_instance = context['task_instance']
    execution_date = task_instance.xcom_pull(key='execution_date_dashed')
    print("Type of execution date:", type(execution_date))
    # Get previous execution date to fetch details from alerting table.
    previous_day_of_execution_date = str(datetime.strptime(str(execution_date), "%Y-%m-%d") - timedelta(1))[:10]
    print("previous_day_of_execution_date:", previous_day_of_execution_date)

    get_process_details_query = f"""
                SELECT 
                * 
                FROM
                (SELECT 
                    DISTINCT
                    REPLACE(ARRAY_TO_STRING(
                    [reporting.program_name, 
                    reporting.project_name, 
                    reporting.process_name, 
                    SAFE_CAST(reporting.process_date AS STRING), 
                    COALESCE(running_instance_id,SAFE_CAST(reporting.trace_id AS STRING), '1'),
                    reporting.application_name],'|'), ' ', '') AS unique_id,
                    reporting.*,
                    'dof-epdo-notification@verizon.com' AS failure_notification_email_list,
                    'dof-epdo-notification@verizon.com' AS success_notification_email_list,
                    'dof-epdo-notification@verizon.com' AS sla_miss_notification_email_list,
                    alert_meta.*EXCEPT(failure_notification_email_list, success_notification_email_list, sla_miss_notification_email_list),
                    pgm_name_map.*
                FROM
                   (
                    SELECT
                      "" AS log_file,
                      "" AS scheduler_name,
                      "" AS process_poc,
                      frequency,
                      expected_run_date AS schedule_datetime,
                      application_name,
                      process_id,
                      process_name,
                      sub_process,
                      project_name,
                      project_name AS program_name,
                      process_date,
                      run_status AS process_status,
                      start_time,
                      end_time,
                      composer_env,
                      PARSE_DATE('%Y%m%d', run_date) AS run_date,
                      run_hour,
                      CAST(inserted_timestamp AS TIMESTAMP) AS inserted_timestamp,
                      tool_name,
                      system_name,
                      gcp_project,
                      target_name,
                      target_type,
                      source_name,
                      source_type,
                      error_message,
                      step_id,
                      process_start_time,
                      process_end_time,
                      trace_id,
                      timezone,
                      running_instance_id,
                      sla AS expected_datetime,
                      try_number,
                      sla,
                      sla_met,
                      process_status AS overall_process_status,
                      process_run_duration
                    FROM
                      `aid_epdo_prd_tbls.dof_data_observability_v2`
                    WHERE process_date >= '{previous_day_of_execution_date}'
                  ) AS reporting
                  LEFT JOIN
                  `{PROJECT_ID}.{DATASET_ID}.{PROCESS_ALERT_NOTIFICATION_META_TABLE_ID}` alert_meta
                ON 
                  reporting.process_name = alert_meta.process_name
                  AND reporting.application_name = alert_meta.application_name
                  AND reporting.program_name = alert_meta.program_name
                  AND reporting.tool_name = alert_meta.env_name
                LEFT JOIN
                  `{VIEW_DATASET_ID}.{PROGRAM_NAME_MAPPING_VIEW_ID}` as pgm_name_map
                ON
                  LOWER(reporting.program_name) = LOWER(pgm_name_map.meta_program_name)
                WHERE
                  (suppress_alert is False or suppress_alert is NULL)
                  AND LOWER(reporting.program_name) NOT IN ("5g",
                                          "oneex",
                                          "exec report",
                                          "5g_esp",
                                          "executive report",
                                          "trade in promo",
                                          "live person chat",
                                          "dapr",
                                          "one talk(daapr)",
                                          "5g trade in employee profile")
                  AND (LOWER(alert_meta.notification_level) = 'process' OR alert_meta.notification_level is NULL OR alert_meta.notification_level='')
                  AND((LOWER(reporting.process_status) = 'success' AND LOWER(alert_meta.notify_success) = 'y' AND success_notification_email_list IS NOT NULL)
                  OR (LOWER(reporting.process_status) in ('failure', 'failed') AND LOWER(alert_meta.notify_failure) = 'y' AND failure_notification_email_list IS NOT NULL)
                  OR (reporting.sla_met IS NOT NULL AND LOWER(reporting.sla_met) = 'no' AND (LOWER(reporting.overall_process_status) in ('pending', 'overdue') OR LOWER(reporting.overall_process_status) = 'in progress') AND LOWER(alert_meta.notify_sla_miss) = 'y' AND sla_miss_notification_email_list IS NOT NULL)))
                LEFT JOIN (
                  SELECT
                    metadata_id,
                    ARRAY_AGG(tag_value) tag_value
                  FROM
                    `{PROJECT_ID}.{VIEW_DATASET_ID}.{TAG_RELATION_VIEW_ID}`
                  WHERE
                    LOWER(is_active) = 'y'
                  GROUP BY
                    metadata_id)
                ON
                  metadata_id = process_id"""

    print("====================================================================")
    print("get_process_details_query:", get_process_details_query)
    print("====================================================================")

    query_job = bigquery_client.query(get_process_details_query)
    process_list = query_job.result()

    # -------------------------------------------------------
    # Fetch all the alert generated alerts for past two days.
    seven_days_back_date = str(datetime.strptime(str(execution_date), "%Y-%m-%d") - timedelta(7))[:10]
    get_details_from_alert_status_table_query = f"""SELECT
                                          CONCAT(report_table_unique_id,"|", notification_types) AS generated_alert_id_and_type
                                        FROM
                                          `{DATASET_ID}.{PROCESS_NOTIFICATION_STATUS_TABLE_ID}`
                                        WHERE
                                          DATE(notification_created_timestamp) >= '{seven_days_back_date}'"""

    query_job = bigquery_client.query(get_details_from_alert_status_table_query)
    alert_type_list = query_job.result()
    generated_alert_id_and_type = list()
    # Create a list of generated alert.
    for alert in alert_type_list:
        generated_alert_id_and_type.append(alert['generated_alert_id_and_type'])

    # -------------------------------------------------------

    process_notification_uid_list = list()
    process_notification_to_insert = list()
    for process in process_list:
        mapped_program_name = process['mapped_program_name'] if process['mapped_program_name'] else process[
            'program_name']
        # Building lineage URL
        program_name_name_without_space = mapped_program_name.replace(" ", "+")
        lineage_url = LINEAGE_BASE_URL + f"run_date={str(process['process_date'])}&project_name={program_name_name_without_space}"
        print("Lineage URL------------------>>>>>>>>>>", lineage_url)

        expected_datetime_utc = str(process['expected_datetime'])[:19]
        schedule_datetime_utc = str(process['schedule_datetime'])[:19]

        if process['end_time']:
            end_time = str(process['end_time'])[:19]
        else:
            end_time = schedule_datetime_utc

        process_details = {"program_name": process['program_name'],
                           "process_name": process['process_name'],
                           "process_status": process['overall_process_status'],
                           "process_date": str(process['process_date']),
                           "application_name": process['application_name'],
                           "sla_time": f"{expected_datetime_utc} UTC",
                           "schedule_datetime": f"{schedule_datetime_utc} UTC",
                           "completion_time": f"{end_time} UTC",
                           "failure_time": f"{end_time} UTC",
                           "Pipeline Lineage": lineage_url}

        if process['tag_value']:
            process_details['tags'] = ",".join(process['tag_value'])

        # Add additional columns from reporting table to notification.
        required_columns = process['notification_required_columns']
        if required_columns:
            try:
                # Remove white spaces.
                required_columns = required_columns.replace(" ", "")
                # Split the value using comma.
                required_columns = required_columns.split(",")
                for column in required_columns:
                    try:
                        # Add the fields into process details dict.
                        process_details[column] = process[column]
                    except KeyError:
                        print(f"Error: {column} is not found inside reporting table.")
            except Exception as e:
                print(f"Error:{e}")

        print("======================================================")
        print("Process details----------->>>>>>>>>>>>>>>>>", process_details)
        print("======================================================")

        # # Fetch all the notification generated for the specific unique_id
        # get_details_from_notification_status_table_query = f"""SELECT
        #                                                   DISTINCT notification_types
        #                                                 FROM
        #                                                   `{DATASET_ID}.{PROCESS_NOTIFICATION_STATUS_TABLE_ID}`
        #                                                 WHERE
        #                                                   report_table_unique_id='{process['unique_id']}'"""
        # query_job = bigquery_client.query(get_details_from_notification_status_table_query)
        # notification_type_list = query_job.result()
        # generated_notification_types = list()
        # for notification in notification_type_list:
        #     generated_notification_types.append(notification['notification_types'])

        # Sent success notification.
        date_format = '%Y-%m-%d %H:%M:%S'
        process_end_time = datetime.strptime(end_time, date_format)
        if process['process_status'] and process['process_status'].lower() == 'success' \
                and process['overall_process_status'] and process['overall_process_status'].lower() == 'completed' \
                and process[
            'notify_success'].lower() == 'y' and f"{process['unique_id']}|success" not in generated_alert_id_and_type\
                and (process['suppress_alert'] is None or (process['suppress_alert'] is False and process['updated_at'] and process['updated_at'] < process_end_time)):
            print("Inside success notification.")

            # Alert for failure rerunning jobs

            if process['notify_failure'].lower() == 'y' and f"{process['unique_id']}|failure" in generated_alert_id_and_type:
                to_email_list = list()
                if process['failure_notification_email_list']:
                    to_email = process['failure_notification_email_list'].replace(' ', '')
                    to_email = to_email.replace('\n', '')
                    # Convert the string to list.
                    to_email_list = to_email.split(",")
                    # Remove null character from list.
                    to_email_list = list(filter(None, to_email_list))

                if to_email_list:
                    notification_type = "success"
                    # email_subject = f"Info[{environment_value}]: Process Completed - Process {process['process_name']} completed successfully on {expected_datetime_utc} UTC"

                    email_subject = f"TraceX | {environment_value} | Notification | RE-RUN SUCCESS {process['process_name']} succeeded on {end_time} UTC"

                    row_to_insert = evaluate_notifications(process_details, notification_type, email_subject,
                                                           to_email_list,
                                                           process['unique_id'], [],
                                                           process_notification_uid_list)
                    if row_to_insert:
                        process_notification_to_insert.append(row_to_insert)
                        process_notification_uid_list.append(f"{process['unique_id']}|{notification_type}")
                else:
                    print("Failure notification email list is empty.")

            else:
                to_email_list = list()
                if process['success_notification_email_list']:
                    to_email = process['success_notification_email_list'].replace(' ', '')
                    to_email = to_email.replace('\n', '')
                    # Convert the string to list.
                    to_email_list = to_email.split(",")
                    # Remove null character from list.
                    to_email_list = list(filter(None, to_email_list))

                if to_email_list:
                    notification_type = "success"
                    # email_subject = f"Info[{environment_value}]: Process Completed - Process {process['process_name']} completed successfully on {expected_datetime_utc} UTC"
                    email_subject = f"TraceX | {environment_value} | Success | {process['process_name']} | {process['program_name']} | {end_time} UTC"
                    # email_subject = f"Notification[{environment_value}]: {process['process_name']} succeeded on {end_time} UTC"
                    row_to_insert = evaluate_notifications(process_details, notification_type, email_subject,
                                                           to_email_list,
                                                           process['unique_id'], [],
                                                           process_notification_uid_list)
                    if row_to_insert:
                        process_notification_to_insert.append(row_to_insert)
                        process_notification_uid_list.append(f"{process['unique_id']}|{notification_type}")
                else:
                    print("Success notification email list is empty.")

        # Add sub-process details in email body.
        process_details['Sub-Process Name'] = process['sub_process']

        # Add additional info to notification(Only for SLA miss and failure.).
        notification_additional_info = process['notification_additional_info']
        if notification_additional_info:
            try:
                # Convert the data to json
                notification_additional_info = json.loads(notification_additional_info)
                for key, value in notification_additional_info.items():
                    process_details[key] = value
            except Exception as e:
                print(f"Error:{e}")

        # Sent failure notification.
        if process['process_status'] and process['process_status'].lower() == 'failure' \
                and process['overall_process_status'] and process['overall_process_status'].lower() == 'failed' \
                and process[
            'notify_failure'].lower() == 'y' and f"{process['unique_id']}|failure" not in generated_alert_id_and_type\
                and (process['suppress_alert'] is None or (process['suppress_alert'] is False and process['updated_at'] and process['updated_at'] < process_end_time)):
            print("Inside failure notification.")

            to_email_list = list()
            if process['failure_notification_email_list']:
                to_email = process['failure_notification_email_list'].replace(' ', '')
                to_email = to_email.replace('\n', '')
                # Convert the string to list.
                to_email_list = to_email.split(",")
                # Remove null character from list.
                to_email_list = list(filter(None, to_email_list))

            if to_email_list:
                process_details['error_message'] = process['error_message']
                process_details['Log URL'] = process['log_file']
                notification_type = "failure"
                email_subject = f"TraceX | {environment_value} | Failure | {process['process_name']} | {process['application_name']} | {process['program_name']} | {end_time} UTC"
                row_to_insert = evaluate_notifications(process_details, notification_type, email_subject,
                                                       to_email_list,
                                                       process['unique_id'], [],
                                                       process_notification_uid_list)
                if row_to_insert:
                    process_notification_to_insert.append(row_to_insert)
                    process_notification_uid_list.append(f"{process['unique_id']}|{notification_type}")
            else:
                print("Failure notification email list is empty.")
        # If status is still in pending and SLA cross ----------->>>> Overdue
        # If status is in progress and SLA cross then --------->>>> SLA miss
        # Sent SLA miss notification
        sla_time = datetime.strptime(str(process['sla'])[:19], date_format)
        if process['sla_met'] and process['sla_met'].lower() == 'no' and process['notify_sla_miss'].lower() == 'y' and \
                process['overall_process_status'] and \
                (process['overall_process_status'].lower() in ('pending', 'overdue', 'in progress')) and process[
            'process_status'].lower() in [
            "pending", "progress", "overdue"] and f"{process['unique_id']}|sla_miss" not in generated_alert_id_and_type\
                and (process['suppress_alert'] is None or (process['suppress_alert'] is False and process['updated_at'] and process['updated_at'] < sla_time)):
            # Inside SLA miss notification.
            to_email_list = list()
            # This flag will decide whether we need to send email or not, when there is a sla buffer.
            send_email_flag = False
            if process['sla_miss_notification_email_list']:
                to_email = process['sla_miss_notification_email_list'].replace(' ', '')
                to_email = to_email.replace('\n', '')
                # Convert the string to list.
                to_email_list = to_email.split(",")
                # Remove null character from list.
                to_email_list = list(filter(None, to_email_list))
            # Add process duration information in the email if process duration exceeded the average duration.

            try:
                if process['duration_alert'].lower() == 'y':
                    process_details['Sub-process Execution Duration'] = str(
                        timedelta(seconds=int(process['duration_in_seconds'])))
                    process_details['Sub-process Average Execution Duration'] = str(
                        timedelta(seconds=int(process['avg_duration_in_seconds'])))
                    process_details['Sub-process Threshold Limit'] = str(
                        timedelta(seconds=int(process['duration_threshold_high_limit'])))
            except Exception as e:
                print(e)

            if process['sla_buffer']:
                # Removes spaces.
                time_component = process['sla_buffer'].replace(" ", "")
                if re.match(r"^\d\d:\d\d:\d\d$", time_component):
                    time_component = process['sla_buffer'].split(":")
                    sla_date_time = datetime.strptime(str(process['expected_datetime'])[0:19], '%Y-%m-%d %H:%M:%S')

                    sla_buffer_added_date_time = sla_date_time + timedelta(hours=int(time_component[0]),
                                                                           minutes=int(time_component[1]),
                                                                           seconds=int(time_component[2]))
                    print("Current Date and Time:", datetime.utcnow())
                    print("sla_buffer_added_date_time:", sla_buffer_added_date_time)
                    # Check whether the current time is greater than buffer added sla time
                    if datetime.utcnow() > sla_buffer_added_date_time and process[
                        'process_status'].lower() == 'pending':
                        send_email_flag = True
                else:
                    print("Value in SLA buffer column is not a time format. Proceeding without SLA buffer.")
                    send_email_flag = True
            else:
                send_email_flag = True

            if to_email_list and send_email_flag:
                notification_type = "sla_miss"
                # email_subject = f"Warning[{environment_value}]: SLA Breach - Process {process['process_name']} crossed the SLA time at {expected_datetime_utc} UTC"

                email_subject = f"TraceX | {environment_value} | SLA Breach | {process['process_name']} | {process['program_name']} | {process['application_name']} | {expected_datetime_utc} UTC"
                row_to_insert = evaluate_notifications(process_details, notification_type, email_subject,
                                                       to_email_list,
                                                       process['unique_id'], [],
                                                       process_notification_uid_list)
                if row_to_insert:
                    process_notification_to_insert.append(row_to_insert)
                    process_notification_uid_list.append(f"{process['unique_id']}|{notification_type}")
            else:
                print("SLA miss notification email list is empty.")

    if process_notification_to_insert:
        errors = bigquery_client.insert_rows_json(f"{PROJECT_ID}.{DATASET_ID}.{PROCESS_NOTIFICATION_STATUS_TABLE_ID}",
                                                  process_notification_to_insert)
        if errors:
            print("Encountered errors while inserting rows: {}".format(errors))
    return True


# function for long pending job notification
def generate_long_pending_process_notification(**context):
    """
    Generate and send process email notification.
    input:
        **context: task variable
    return:
        status: boolean
    """
    import importlib
    from datetime import datetime, timedelta
    from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
    from airflow.models import Variable
    ENVIRONMENT = Variable.get('composer_env')
    environment_value = get_environment_value()
    env_config_module = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.env_config")
    DATASET_ID = env_config_module.config[ENVIRONMENT]['dataset_id']
    PROJECT_ID = env_config_module.config[ENVIRONMENT]['project_id']
    GCP_CONNECTION_ID = env_config_module.config[ENVIRONMENT]['gcp_connection_id']
    Obj = BigQueryHook(gcp_conn_id=GCP_CONNECTION_ID)
    bigquery_client = Obj.get_client(project_id=PROJECT_ID)

    PROCESS_METADATA_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_metadata_table_id']
    REALTIME_PROCESS_META_TABLE_ID = env_config_module.config[ENVIRONMENT]['realtime_process_meta_table_id']
    PROCESS_MONITORING_REPORT_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_monitoring_report_table_id']
    PROCESS_NOTIFICATION_STATUS_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_notification_status_table_id']
    VIEW_DATASET_ID = env_config_module.config[ENVIRONMENT]['view_dataset_id']
    TAG_RELATION_VIEW_ID = env_config_module.config[ENVIRONMENT]['tag_relation_view_id']
    PROCESS_ALERT_NOTIFICATION_META_TABLE_ID = env_config_module.config[ENVIRONMENT][
        'dof_process_alert_notification_meta_table_id']
    LINEAGE_BASE_URL = env_config_module.config[ENVIRONMENT]['lineage_base_url']
    PROGRAM_NAME_MAPPING_VIEW_ID = env_config_module.config[ENVIRONMENT]['program_name_mapping_view_id']
    task_instance = context['task_instance']
    execution_date = task_instance.xcom_pull(key='execution_date_dashed')
    print("Type of execution date:", type(execution_date))
    # Get previous execution date to fetch details from alerting table.
    previous_day_of_execution_date = str(datetime.strptime(str(execution_date), "%Y-%m-%d") - timedelta(1))[:10]
    print("previous_day_of_execution_date:", previous_day_of_execution_date)

    get_process_details_query = f"""
                          SELECT
                            *
                          FROM (
                            SELECT
                              DISTINCT REPLACE(ARRAY_TO_STRING( [program_name, project_name, process_name, SAFE_CAST(STRING_AGG(process_date
                                    ORDER BY
                                      process_date DESC
                                    LIMIT
                                      1) AS string), application_name],'|'), ' ', '') AS unique_id,
                              SAFE_CAST(STRING_AGG(process_date
                                    ORDER BY
                                      process_date DESC
                                    LIMIT
                                      1) AS string) as process_date,
                              program_name,
                              project_name,
                              process_name,
                              frequency,
                              overall_process_status,
                              application_name,
                              COUNT(*) AS all_days,
                              STRING_AGG(CAST(process_date AS string),"|"
                              ORDER BY
                                process_date DESC) AS dates,
                              SAFE_CAST(STRING_AGG(process_date
                                    ORDER BY
                                      process_date ASC
                                    LIMIT
                                      1) AS string) as pending_start_date
                            FROM (
                              SELECT
                                DISTINCT program_name,
                                project_name,
                                process_name,
                                overall_process_status,
                                CAST(process_date AS string) AS process_date,
                                frequency,
                                application_name
                              FROM
                                `{PROJECT_ID}.{DATASET_ID}.{PROCESS_MONITORING_REPORT_TABLE_ID}`
                              WHERE
                                is_valid = TRUE
                                AND LOWER(overall_process_status) = 'pending'
                                AND
                                CASE
                                  WHEN LOWER(frequency) IN ('daily', 'fixed', 'hourly', 'hourly_custom') THEN (process_date BETWEEN CURRENT_DATE()-4 AND CURRENT_DATE()) AND process_date <>CURRENT_DATE()
                                  WHEN LOWER(frequency) IN ('weekly') THEN (process_date BETWEEN CURRENT_DATE()-21
                                  AND CURRENT_DATE())
                                AND process_date <>CURRENT_DATE()
                                  ELSE (process_date BETWEEN CURRENT_DATE()-60
                                  AND CURRENT_DATE())
                                AND process_date <>CURRENT_DATE()
                              END
                              ORDER BY
                                process_date)
                            GROUP BY
                              program_name,
                              project_name,
                              process_name,
                              frequency,
                              overall_process_status,
                              application_name
                            HAVING
                              CASE
                                WHEN LOWER(frequency) IN ('daily', 'fixed', 'hourly', 'hourly_custom', 'weekly') THEN all_days >= 3
                                ELSE all_days >= 2
                            END
                              AND LOWER(overall_process_status) = 'pending'
                              AND LOWER(application_name) IN ('soi',
                                'ml',
                                'soi-api',
                                'de')) reporting
                          LEFT JOIN
                            `{PROJECT_ID}.{DATASET_ID}.{PROCESS_ALERT_NOTIFICATION_META_TABLE_ID}` alert_meta
                          ON
                            reporting.process_name = alert_meta.process_name
                            AND reporting.application_name = alert_meta.application_name
                            AND reporting.program_name = alert_meta.program_name
                          LEFT JOIN
                            `{VIEW_DATASET_ID}.{PROGRAM_NAME_MAPPING_VIEW_ID}` AS pgm_name_map
                          ON
                            LOWER(reporting.program_name) = LOWER(pgm_name_map.meta_program_name)
                          WHERE
                            LOWER(reporting.program_name) NOT IN ("5g","oneex","exec report","5g_esp",
                              "executive report","trade in promo","live person chat","dapr",
                              "one talk(daapr)","5g trade in employee profile")
                            AND (LOWER(alert_meta.notification_level) = 'process'
                              OR alert_meta.notification_level IS NULL
                              OR alert_meta.notification_level='') AND((LOWER(alert_meta.notify_failure) = 'y'
                                AND failure_notification_email_list IS NOT NULL)
                              OR (LOWER(alert_meta.notify_sla_miss) = 'y'
                                AND sla_miss_notification_email_list IS NOT NULL))
                                """

    print("====================================================================")
    print("get_process_details_query:", get_process_details_query)
    print("====================================================================")

    query_job = bigquery_client.query(get_process_details_query)
    process_list = query_job.result()

    # -------------------------------------------------------
    # Fetch all the alert generated alerts for past two days.
    seven_days_back_date = str(datetime.strptime(str(execution_date), "%Y-%m-%d") - timedelta(7))[:10]
    get_details_from_alert_status_table_query = f"""SELECT
                                          CONCAT(report_table_unique_id,"|", notification_types) AS generated_alert_id_and_type
                                        FROM
                                          `{DATASET_ID}.{PROCESS_NOTIFICATION_STATUS_TABLE_ID}`
                                        WHERE
                                          DATE(notification_created_timestamp) >= '{seven_days_back_date}'"""

    query_job = bigquery_client.query(get_details_from_alert_status_table_query)
    alert_type_list = query_job.result()
    generated_alert_id_and_type = list()
    # Create a list of generated alert.
    for alert in alert_type_list:
        generated_alert_id_and_type.append(alert['generated_alert_id_and_type'])

    # -------------------------------------------------------

    process_notification_uid_list = list()
    process_notification_to_insert = list()
    for process in process_list:
        mapped_program_name = process['mapped_program_name'] if process['mapped_program_name'] else process[
            'program_name']
        # Building lineage URL
        # program_name_name_without_space = mapped_program_name.replace(" ", "+")
        # lineage_url = LINEAGE_BASE_URL + f"run_date={str(process['process_date'])}&project_name={program_name_name_without_space}"
        # print("Lineage URL------------------>>>>>>>>>>", lineage_url)
        process_details = {"program_name": process['program_name'],
                           "process_name": process['process_name'],
                           "process_status": process['overall_process_status'],
                           "process_date": str(process['process_date']),
                           "application_name": process['application_name'],
                           "process_pending_from": process['pending_start_date']}
        """
        if process['tag_value']:
            process_details['tags'] = ",".join(process['tag_value'])
        """
        print("======================================================")
        print("Process details----------->>>>>>>>>>>>>>>>>", process_details)
        print("======================================================")

        # Sent long pending notification.
        if process['overall_process_status'].lower() == 'pending' \
                and process[
            'notify_failure'].lower() == 'y' and f"{process['unique_id']}|long_pending" not in generated_alert_id_and_type:
            print("Inside long pending notification.")

            to_email_list = list()
            """
            if process['failure_notification_email_list']:
                to_email = process['failure_notification_email_list'].replace(' ', '')
                to_email = to_email.replace('\n', '')
                # Convert the string to list.
                to_email_list = to_email.split(",")
                # Remove null character from list.
                to_email_list = list(filter(None, to_email_list))
            """
            to_email_list = ["aid-dof-vzi-notify@verizon.com"]
            if to_email_list:
                # process_details['error_message'] = process['error_message']
                # process_details['Log URL'] = process['log_file']
                notification_type = "long_pending"
                email_subject = f"TraceX | {environment_value} |Long Pending | {process['process_name']} | {process['application_name']} | {process['program_name']} | {process['process_date']}"
                row_to_insert = evaluate_notifications(process_details, notification_type, email_subject,
                                                       to_email_list,
                                                       process['unique_id'], [],
                                                       process_notification_uid_list)
                if row_to_insert:
                    process_notification_to_insert.append(row_to_insert)
                    process_notification_uid_list.append(f"{process['unique_id']}|{notification_type}")
            else:
                print("Failure notification email list is empty.")
    if process_notification_to_insert:
        errors = bigquery_client.insert_rows_json(f"{PROJECT_ID}.{DATASET_ID}.{PROCESS_NOTIFICATION_STATUS_TABLE_ID}",
                                                  process_notification_to_insert)
        if errors:
            print("Encountered errors while inserting rows: {}".format(errors))
    return True


def generate_table_alert(**context):
    """
    Generate and send volume based notification.
    input:
        **context: task variable
    return:
        status: boolean
    """
    from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
    import uuid
    import importlib
    from datetime import datetime, timedelta
    from airflow.models import Variable
    ENVIRONMENT = Variable.get('composer_env')
    environment_value = get_environment_value()
    env_config_module = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.env_config")
    DATASET_ID = env_config_module.config[ENVIRONMENT]['dataset_id']
    PROJECT_ID = env_config_module.config[ENVIRONMENT]['project_id']
    GCP_CONNECTION_ID = env_config_module.config[ENVIRONMENT]['gcp_connection_id']
    Obj = BigQueryHook(gcp_conn_id=GCP_CONNECTION_ID)
    bigquery_client = Obj.get_client(project_id=PROJECT_ID)

    PROCESS_ALERT_STATUS_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_alert_status_table_id']
    PROCESS_MONITORING_REPORT_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_monitoring_report_table_id']
    PROCESS_NOTIFICATION_STATUS_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_notification_status_table_id']
    TABLE_METADATA_TABLE_ID = env_config_module.config[ENVIRONMENT]['table_metadata_table_id']
    task_instance = context['task_instance']
    execution_date = task_instance.xcom_pull(key='execution_date_dashed')
    print("Type of execution date:", type(execution_date))
    current_date_time = datetime.utcnow()
    # Get previous execution date to fetch details from alerting table.
    previous_day_of_execution_date = str(datetime.strptime(str(execution_date), "%Y-%m-%d") - timedelta(1))[:10]
    print("previous_day_of_execution_date:", previous_day_of_execution_date)

    get_table_details_query = f"""SELECT 
                      * 
                    FROM
                    (SELECT
                      DISTINCT REPLACE(
                        ARRAY_TO_STRING(
                          [reporting.application_name, 
                          reporting.program_name,
                          reporting.sub_process,
                          SAFE_CAST(reporting.trace_id AS STRING), 
                          meta.table_name, 
                          SAFE_CAST(reporting.process_date AS STRING)
                          ] ,'|'), ' ', '') AS unique_id,
                      reporting.*,
                      notify_table_size,
                      table_size_notification_email_list,
                      opsgenie_team,
                      alert_table_size,
                      table_name
                    FROM
                      ((SELECT
                            rpt.*EXCEPT(target_count),
                            GREATEST(IFNULL(target_count, 0),IFNULL(table_count, 0)) AS target_count
                          FROM
                            `{PROJECT_ID}.{DATASET_ID}.{PROCESS_MONITORING_REPORT_TABLE_ID}`  as rpt
                          LEFT JOIN
                            `{PROJECT_ID}.{DATASET_ID}.dof_volume_rpt` AS volume
                          ON
                            rpt.process_id = volume.process_id
                            AND rpt.process_date=volume.process_date)) AS reporting
                    INNER JOIN
                      (SELECT *, CONCAT (server_name, ".", db_name,".", table_name) as target_table FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_METADATA_TABLE_ID}`) AS meta
                    ON
                      reporting.target_name=meta.target_table
                      AND reporting.application_name=meta.application_name
                    WHERE
                      (LOWER(alert_table_size) = 'y' OR LOWER(notify_table_size) = 'y')
                      AND reporting.process_date >= '{previous_day_of_execution_date}'
                      AND LOWER(reporting.size_alert) = 'y'
                    )
                    WHERE unique_id NOT IN (SELECT
                                                DISTINCT report_table_unique_id
                                            FROM
                                              `{PROJECT_ID}.{DATASET_ID}.{PROCESS_NOTIFICATION_STATUS_TABLE_ID}`)
                    OR unique_id NOT IN (SELECT
                                                DISTINCT report_table_unique_id
                                            FROM
                                              `{PROJECT_ID}.{DATASET_ID}.{PROCESS_ALERT_STATUS_TABLE_ID}`)"""

    print("Get Table detail Query---------------->>>>>>>>>>>>> ", get_table_details_query)
    generated_alert_uuid_list = list()
    generated_notification_uuid_list = list()
    query_job = bigquery_client.query(get_table_details_query)
    table_list = query_job.result()
    table_notification_to_insert = list()
    table_alert_to_insert = list()
    for table in table_list:
        table_details = {
            "table_name": table['table_name'],
            "table_count": table["target_count"],
            "program_name": table['program_name'],
            "process_name": table['process_name'],
            "project_name": table['project_name'],
            "application_name": table['application_name'],
            "threshold_unit": table['threshold_unit'],
            "threshold_up": table['threshold_up'],
            "threshold_low": table['threshold_low'],
            "threshold_low_value": table['threshold_low_value'],
            "threshold_up_value": table['threshold_up_value'],
            "date": str(table['process_date'])
        }

        # Generate table volume notification.
        if table['notify_table_size'].lower() == 'y' and \
                table['table_size_notification_email_list'] and \
                table['unique_id'] not in generated_notification_uuid_list:

            to_email = table['table_size_notification_email_list'].replace(' ', '')
            to_email = to_email.replace('\n', '')
            # Convert the string to list.
            to_email_list = to_email.split(",")
            # Remove null character from list.
            to_email_list = list(filter(None, to_email_list))
            to_email_address = ','.join(to_email_list)
            status = generate_and_sent_table_volume_notification(table_details, to_email_address)
            if status:
                row_to_insert = {"report_table_unique_id": table['unique_id'],
                                 "notification_created_timestamp": str(datetime.utcnow()),
                                 "notification_types": 'table_size_alert',
                                 "id": str(uuid.uuid4())
                                 }
                table_notification_to_insert.append(row_to_insert)
                generated_notification_uuid_list.append(table['unique_id'])

        # Generate table volume opsgeie alert.
        if table['alert_table_size'].lower() == 'y' and table['unique_id'] not in generated_alert_uuid_list:
            message = f"Warning[{environment_value}]: {table['table_name']} Table size is outside of the threshold limit on {str(table_details['date'])}"
            description = 'Table size is outside of the threshold limit, Please check the details.'
            priority = 'P3'
            alert_type = 'table_volume'
            table_details['opsgenie_team'] = table['opsgenie_team']
            status = send_alert_to_opsgenie(message, description, table_details, priority, alert_type, [])
            if status:
                row_to_insert = {
                    "report_table_unique_id": table['unique_id'],
                    "alert_created_date": str(current_date_time.date()),
                    "alert_type": alert_type,
                    "alert_request_id": status,
                    "id": str(uuid.uuid4())
                }
                table_alert_to_insert.append(row_to_insert)
                generated_alert_uuid_list.append(table['unique_id'])

    # Insert table alert details to BigQuery
    if table_alert_to_insert:
        print(f"Started BQ insert at {datetime.utcnow()}")
        errors = bigquery_client.insert_rows_json(f"{PROJECT_ID}.{DATASET_ID}.{PROCESS_ALERT_STATUS_TABLE_ID}",
                                                  table_alert_to_insert)
        if errors:
            print("Encountered errors while inserting rows: {}".format(errors))
        print(f"Ended BQ insert at {datetime.utcnow()}")

    # Insert table notification details to BigQuery
    if table_notification_to_insert:
        errors = bigquery_client.insert_rows_json(f"{PROJECT_ID}.{DATASET_ID}.{PROCESS_NOTIFICATION_STATUS_TABLE_ID}",
                                                  table_notification_to_insert)
        if errors:
            print("Encountered errors while inserting rows: {}".format(errors))
    return True


def generate_dependency_notification(**context):
    """
    Generate and send dependency notification.
    input:
        **context: task variable
    return:
        status: boolean
    """
    import uuid
    import importlib
    from datetime import datetime, timedelta
    from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
    from airflow.models import Variable
    ENVIRONMENT = Variable.get('composer_env')
    env_config_module = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.env_config")
    DATASET_ID = env_config_module.config[ENVIRONMENT]['dataset_id']
    PROJECT_ID = env_config_module.config[ENVIRONMENT]['project_id']
    GCP_CONNECTION_ID = env_config_module.config[ENVIRONMENT]['gcp_connection_id']
    Obj = BigQueryHook(gcp_conn_id=GCP_CONNECTION_ID)
    bigquery_client = Obj.get_client(project_id=PROJECT_ID)

    PROCESS_NOTIFICATION_STATUS_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_notification_status_table_id']
    LINEAGE_BASE_URL = env_config_module.config[ENVIRONMENT]['lineage_base_url']
    PROGRAM_NAME_MAPPING_VIEW_ID = env_config_module.config[ENVIRONMENT]['program_name_mapping_view_id']
    VIEW_DATASET_ID = env_config_module.config[ENVIRONMENT]['view_dataset_id']
    PROCESS_ALERT_NOTIFICATION_META_TABLE_ID = env_config_module.config[ENVIRONMENT][
        'dof_process_alert_notification_meta_table_id']
    PROCESS_MONITORING_REPORT_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_monitoring_report_table_id']
    PROCESS_METADATA_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_metadata_table_id']
    task_instance = context['task_instance']
    execution_date = task_instance.xcom_pull(key='execution_date_dashed')
    print("Type of execution date:", type(execution_date))
    # Get previous execution date to fetch details from alerting table.
    previous_day_of_execution_date = str(datetime.strptime(str(execution_date), "%Y-%m-%d") - timedelta(1))[:10]
    print("previous_day_of_execution_date:", previous_day_of_execution_date)

    dependency_details_query = f"""
        SELECT
      DISTINCT *
    FROM (
      SELECT
        REPLACE(ARRAY_TO_STRING( [main.application_name, main.program_name, main.process_name, SAFE_CAST(report_1.process_date AS STRING)],'|'), ' ', '') AS unique_id,
        main.process_name,
        main.subprocess_name,
        main.application_name,
        main.program_name,
        main.process_poc,
        main.source,
        main.project_name,
        dependent.process_name AS dependent_process_name,
        dependent.process_id AS dependent_process_id,
        dependent.subprocess_name AS dependent_subprocess_name,
        report.process_status AS dependent_process_status,
        report.process_date AS process_date,
        dependent.target AS dependent_target,
        dependent.process_poc AS dependent_process_poc_poc,
        notification_meta.dependency_notification,
        notification_meta.dependency_notification_email_list,
        report_1.max_expected_datetime AS expected_completion_datetime,
        mapped_program_name
      FROM (
        SELECT
          *EXCEPT(SOURCE,
            sources),
          TRIM(sources) AS SOURCE
        FROM
          `{PROJECT_ID}.{DATASET_ID}.{PROCESS_METADATA_TABLE_ID}`,
          UNNEST(SPLIT(SOURCE, ',')) sources ) AS main
      INNER JOIN (
        SELECT
          *EXCEPT(TARGET,
            targets),
          TRIM(targets) AS TARGET
        FROM
          `{PROJECT_ID}.{DATASET_ID}.{PROCESS_METADATA_TABLE_ID}`,
          UNNEST(SPLIT(TARGET, ',')) targets ) AS dependent
      ON
        main.source=dependent.target
        AND main.process_name!=dependent.process_name
        AND dependent.source IS NOT NULL
        AND dependent.process_name IS NOT NULL
      LEFT JOIN
        `{VIEW_DATASET_ID}.{PROGRAM_NAME_MAPPING_VIEW_ID}` as pgm_name_map
      ON
        LOWER(main.program_name) = LOWER(pgm_name_map.meta_program_name)
      INNER JOIN
        `{PROJECT_ID}.{DATASET_ID}.{PROCESS_ALERT_NOTIFICATION_META_TABLE_ID}` AS notification_meta
      ON
        main.application_name=notification_meta.application_name
        AND main.program_name = notification_meta.program_name
        AND main.process_name = notification_meta.process_name
        AND main.platform_name = notification_meta.platform_name
      LEFT JOIN (
        SELECT
          *
        FROM (
          SELECT
            process_name,
            process_date,
            application_name,
            program_name,
            process_status,
            process_id,
            sub_process,
            ROW_NUMBER() OVER (PARTITION BY process_id, program_name, sub_process, process_date, schedule_time ORDER BY start_time DESC) AS row_number
          FROM
            `{PROJECT_ID}.{DATASET_ID}.{PROCESS_MONITORING_REPORT_TABLE_ID}`) AS ranked
        WHERE
          row_number=1
          AND process_date = '{execution_date}'
          ) AS report
      ON
        dependent.process_id = report.process_id 
      -- find main sla
      LEFT JOIN (
        SELECT
          *
        FROM (
          SELECT
            process_name,
            process_date,
            process_id,
            MAX(expected_datetime) OVER (PARTITION BY application_name, program_name, process_name, process_date) AS max_expected_datetime,
          FROM
            `{PROJECT_ID}.{DATASET_ID}.{PROCESS_MONITORING_REPORT_TABLE_ID}`) AS ranked
        WHERE
          process_date = '{execution_date}' ) AS report_1
      ON
        main.process_id = report_1.process_id
      WHERE
        LOWER(notification_meta.dependency_notification) ='y'
        AND CURRENT_TIMESTAMP() > max_expected_datetime )
    WHERE unique_id NOT IN (SELECT
              DISTINCT report_table_unique_id
            FROM
              `{PROJECT_ID}.{DATASET_ID}.{PROCESS_NOTIFICATION_STATUS_TABLE_ID}`)
    ORDER BY
      unique_id DESC"""

    print("dependency_details_query----------->>>>>>>>>", dependency_details_query)
    query_job = bigquery_client.query(dependency_details_query)
    dependency_process_list = query_job.result()
    active_process_unique_id = None
    send_notification_info = list()
    dependency_response = None
    for process in dependency_process_list:
        if not process['process_date'] or not process['dependent_process_status']:
            continue

        if process['unique_id'] == active_process_unique_id:
            dependency_process_details = {
                "dependency_process_name": process['dependent_process_name'],
                "dependency_sub_process_name": process['dependent_subprocess_name'],
                "table_name": process['source'],
                "process_date": str(process['process_date']),
                "status": process['dependent_process_status'],
                "poc": process['dependent_process_poc_poc']
            }
            dependency_response['dependency_process_details'].append(dependency_process_details)

        if process['unique_id'] != active_process_unique_id:
            if active_process_unique_id:
                # Sending Email along with the details
                response = send_dependency_alert_emails(dependency_response,
                                                        process['dependency_notification_email_list'])
                print("Process Details----->>>>>>>", dependency_response)
                if response:
                    print(f"Successfully sent email for {active_process_unique_id}")
                    # Insert data into email status table
                    row_to_insert = {"id": str(uuid.uuid4()),
                                     "report_table_unique_id": process['unique_id'],
                                     "notification_types": "dependency",
                                     "notification_created_timestamp": str(datetime.utcnow())}
                    send_notification_info.append(row_to_insert)
                else:
                    print(f"Unable to sent email for {active_process_unique_id}")
            active_process_unique_id = process['unique_id']
            dependency_response = dict()
            dependency_response['process_name'] = process['process_name']
            dependency_response['process_date'] = str(process['process_date'])
            mapped_program_name = process['mapped_program_name'] if process['mapped_program_name'] else process[
                'program_name']
            # Building lineage URL
            program_name_name_without_space = mapped_program_name.replace(" ", "+")
            lineage_url = LINEAGE_BASE_URL + f"run_date={str(process['process_date'])}&project_name={program_name_name_without_space}"
            print("Lineage URL------------------>>>>>>>>>>", lineage_url)
            dependency_response['lineage_url'] = lineage_url
            dependency_response['dependency_process_details'] = list()
            dependency_process_details = {
                "dependency_process_name": process['dependent_process_name'],
                "dependency_sub_process_name": process['dependent_subprocess_name'],
                "table_name": process['source'],
                "process_date": str(process['process_date']),
                "status": process['dependent_process_status'],
                "poc": process['dependent_process_poc_poc']
            }
            dependency_response['dependency_process_details'].append(dependency_process_details)
    else:
        print("----------->>>>>>>>>>>>>Inside for else")
        if dependency_response:
            response = send_dependency_alert_emails(dependency_response, process['dependency_notification_email_list'])

            print("Process Details----->>>>>>>", dependency_response)
            if response:
                print(f"Successfully sent email for {active_process_unique_id}")
                # Insert data into email status table
                row_to_insert = {"id": str(uuid.uuid4()),
                                 "report_table_unique_id": process['unique_id'],
                                 "notification_types": "dependency",
                                 "notification_created_timestamp": str(datetime.utcnow())}
                send_notification_info.append(row_to_insert)
            else:
                print(f"Unable to sent email for {active_process_unique_id}")

    if send_notification_info:
        if send_notification_info:
            print(f"Started BQ insert at {datetime.utcnow()}")
            errors = bigquery_client.insert_rows_json(
                f"{PROJECT_ID}.{DATASET_ID}.{PROCESS_NOTIFICATION_STATUS_TABLE_ID}",
                send_notification_info)
            if errors:
                print("Encountered errors while inserting rows: {}".format(errors))
            print(f"Ended BQ insert at {datetime.utcnow()}")
    return True


def get_opsgenie_ticket_details(**context):
    """
    Get and load Opsgenie ticket details to BigQuery.
    input:
        **context: task variable
    return:
        status: boolean
    """
    import json
    import requests
    import importlib
    from datetime import datetime, timedelta
    from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
    from airflow.models import Variable
    ENVIRONMENT = Variable.get('composer_env')
    env_config_module = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.env_config")
    DATASET_ID = env_config_module.config[ENVIRONMENT]['dataset_id']
    PROJECT_ID = env_config_module.config[ENVIRONMENT]['project_id']
    GCP_CONNECTION_ID = env_config_module.config[ENVIRONMENT]['gcp_connection_id']
    Obj = BigQueryHook(gcp_conn_id=GCP_CONNECTION_ID)
    bigquery_client = Obj.get_client(project_id=PROJECT_ID)

    OPSGENIE_TICKET_MAIN_TABLE_ID = env_config_module.config[ENVIRONMENT]['opsgenie_ticket_main_table_id']
    OPSGENIE_TICKET_STAGING_TABLE_ID = env_config_module.config[ENVIRONMENT]['opsgenie_ticket_staging_table_id']
    OPS_KEY = "5e1f420c-5e64-4853-ad69-c888d583d6e7"
    headers = {"Content-Type": "application/json; charset=utf-8",
               "Authorization": f"GenieKey {OPS_KEY}"}
    opsgenie_base_url = 'https://api.opsgenie.com/v2/alerts'

    # Get available ticket ids for last 3 days
    three_day_before_date = str(datetime.now() - timedelta(days=3))

    three_day_before_date = datetime.strptime(three_day_before_date[:19], '%Y-%m-%d %H:%M:%S')
    get_available_alert_ids_query = f"""SELECT
                                              DISTINCT ticket_id
                                            FROM
                                              `{DATASET_ID}.{OPSGENIE_TICKET_MAIN_TABLE_ID}`
                                            WHERE
                                                updatedAt>'{three_day_before_date}'"""

    query_job = bigquery_client.query(get_available_alert_ids_query)
    query_job.result()
    available_ids = list()
    for data in query_job:
        available_ids.append(data['ticket_id'])

    params = {'limit': '50',
              'query': f'updatedAt>{round(datetime.timestamp(datetime.now() - timedelta(hours=1)) * 1000)}'}

    response = requests.get(opsgenie_base_url, headers=headers, params=params)

    print("Status Code ITR-1---------->>>>>>>", response.status_code)

    opsgenie_data = json.loads(response.text)

    print("API Response--------->>>>>>", opsgenie_data)
    ticket_details = opsgenie_data['data']

    if not ticket_details:
        print("No Opsgenie ticket to fetch.")
        return True

    page_details = opsgenie_data['paging']
    next_page = page_details.get('next', None)

    load_data_to_staging_table(ticket_details, True, available_ids)

    print("Next Page------------->>>>>>>>>", next_page)

    while next_page:
        response = requests.get(next_page, headers=headers)
        print("Status Code ITR-2---------->>>>>>>", response.status_code)
        json_output = json.loads(response.text)
        page_details = json_output['paging']
        next_page = page_details.get('next', None)
        print("Next Page------------->>>>>>>>>", next_page)

        print("DATA SIZE IN CURRENT ALERT------------>>>>>", len(json_output['data']))
        load_data_to_staging_table(json_output['data'], False, available_ids)

    merge_query = f"""MERGE
                          `{DATASET_ID}.{OPSGENIE_TICKET_MAIN_TABLE_ID}` AS main
                        USING
                            (SELECT
                                DISTINCT *
                            FROM
                            `{DATASET_ID}.{OPSGENIE_TICKET_STAGING_TABLE_ID}`) AS staging
                        ON
                          main.ticket_id = staging.ticket_id
                          WHEN MATCHED THEN UPDATE SET main.alias = staging.alias,			
                            main.createdAt	= staging.createdAt,				
                            main.lastOccurredAt	= staging.lastOccurredAt ,		
                            main.message= staging.message,					
                            main.owner=staging.owner,					
                            main.priority=staging.priority,				
                            main.ackTime=staging.ackTime,					
                            main.acknowledgedBy=staging.acknowledgedBy,					
                            main.snoozed=staging.snoozed,					
                            main.status=staging.status,					
                            main.tinyId	= staging.tinyId, 	
                            main.updatedAt=staging.updatedAt,					
                            main.responders=staging.responders,
                            main.closeTime=staging.closeTime
                          WHEN NOT MATCHED
                          THEN
                        INSERT
                          (ticket_id,
                          alias,
                          createdAt,
                          lastOccurredAt,
                          message,
                          owner,
                          priority,
                          ackTime,
                          acknowledgedBy,
                          snoozed,
                          status,
                          tinyId,
                          updatedAt,
                          responders,
                          description,
                          params,
                          closeTime
                          )
                        VALUES
                          (staging.ticket_id,
                          staging.alias,
                          staging.createdAt,
                          staging.lastOccurredAt ,
                          staging.message,
                          staging.owner,
                          staging.priority,
                          staging.ackTime,
                          staging.acknowledgedBy,
                          staging.snoozed,
                          staging.status,
                          staging.tinyId,
                          staging.updatedAt,
                          staging.responders,
                          staging.description,
                          staging.params,
                          staging.closeTime)"""
    merge_query_job = bigquery_client.query(merge_query)
    result = merge_query_job.result()
    print("Merge Query Result ------->>>>>>", result)
    return "Success"


def generate_file_email_notification(**context):
    """
    Generate and send file alert to Opsgenie.
    input:
        **context: task variable
    return:
        status: boolean
    """
    return 0
    import json
    import importlib
    from airflow.models import Variable
    from datetime import datetime, timedelta
    from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
    env_config_module = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.env_config")
    ENVIRONMENT = Variable.get('composer_env')
    environment_value = get_environment_value()
    DATASET_ID = env_config_module.config[ENVIRONMENT]['dataset_id']
    PROJECT_ID = env_config_module.config[ENVIRONMENT]['project_id']
    GCP_CONNECTION_ID = env_config_module.config[ENVIRONMENT]['gcp_connection_id']
    Obj = BigQueryHook(gcp_conn_id=GCP_CONNECTION_ID)
    bigquery_client = Obj.get_client(project_id=PROJECT_ID)
    FILE_METADATA_TABLE_ID = env_config_module.config[ENVIRONMENT]['file_metadata_table_id']
    FILE_ALERT_STATUS_TABLE_ID = env_config_module.config[ENVIRONMENT]['file_alert_status_table_id']
    FILE_MONITORING_TABLE_ID = env_config_module.config[ENVIRONMENT]['file_monitoring_table_id']
    alert_tool = 'email'
    task_instance = context['task_instance']
    execution_date = task_instance.xcom_pull(key='execution_date_dashed')
    print("Type of execution date:", type(execution_date))
    # Get previous execution date to fetch details from alerting table.
    previous_day_of_execution_date = str(datetime.strptime(str(execution_date), "%Y-%m-%d") - timedelta(1))[:10]
    print("previous_day_of_execution_date:", previous_day_of_execution_date)
    # Query to fetch details from file report table
    get_file_details_query = f"""SELECT
                                  REPLACE(
                                    ARRAY_TO_STRING(
                                      [reporting.program_name, 
                                      reporting.process_name, 
                                      SAFE_CAST(reporting.process_date AS STRING),
                                      SAFE_CAST(reporting.sla AS STRING), 
                                      reporting.file_pattern, 
                                      reporting.received_filename, 
                                      reporting.project_name, 
                                      reporting.file_arrival_status, 
                                      reporting.application_name, 
                                      reporting.frequency],'|'), " ", "") AS unique_id,
                                      *
                                FROM
                                  `{PROJECT_ID}.{DATASET_ID}.{FILE_MONITORING_TABLE_ID}` AS reporting

                                LEFT JOIN
                                  `{PROJECT_ID}.{DATASET_ID}.{FILE_METADATA_TABLE_ID}` as metadata
                                ON
                                  reporting.application_name = metadata.application_name
                                  AND reporting.file_id = metadata.file_id
                                  AND reporting.file_pattern = metadata.file_name
                                WHERE
                                  reporting.process_date >= '{previous_day_of_execution_date}'
                                  AND LOWER(metadata.is_active) = 'y'
                                  AND (reporting.sla_met IS NOT NULL AND LOWER(reporting.sla_met) = 'no' AND LOWER(metadata.notify_sla_misses) = 'y' and LOWER(file_arrival_status)="pending")"""

    print(get_file_details_query)
    query_job = bigquery_client.query(get_file_details_query)
    file_list = query_job.result()
    file_alert_to_insert = list()
    file_alert_uid = list()

    # -----------------------------------------------------------------
    # Get details of alerts created for last two days.
    get_details_from_alert_status_table_query = f"""SELECT
                                              CONCAT(report_table_unique_id,"|",alert_tool,"|",alert_type) AS generated_alert_id_and_type
                                            FROM
                                              `{DATASET_ID}.{FILE_ALERT_STATUS_TABLE_ID}`
                                            WHERE
                                              alert_created_date >= '{previous_day_of_execution_date}'"""

    print("get_details_from_alert_status_table_query------------>", get_details_from_alert_status_table_query)

    query_job = bigquery_client.query(get_details_from_alert_status_table_query)
    alert_type_list = query_job.result()
    generated_alert_id_and_type = list()
    # Create a list of generated alert.
    for alert in alert_type_list:
        generated_alert_id_and_type.append(alert['generated_alert_id_and_type'])

    # -----------------------------------------------------------------
    for file in file_list:
        # Create file details dict which we need to send to Opsgenie.
        file_details = {
            "process_date": str(file['process_date']),
            "application_name": file['application_name'],
            "file_pattern": file['file_pattern'],
            "file_arrival_status": file['file_arrival_status'],
            "sla_time": str(file['sla']),
            "source_poc": file['source_poc'],
            "program_name": file['program_name'],
            "process_name": file['process_name']
        }

        # Consolidating to email address.
        to_email_list = ''
        if file['source_poc']:
            to_email_list = file['source_poc']
        if file['process_owner']:
            to_email_list = f"{to_email_list},{file['process_owner']}"
        if file['downstream_email']:
            to_email_list = f"{to_email_list},{file['downstream_email']}"

        if to_email_list:
            to_email_list = to_email_list.replace(' ', '')
            to_email_list = to_email_list.replace('\n', '')
        else:
            print("Email list is not defined in metadata, Sending email to dof-epdo-notification@verizon.com")
            to_email_list = "dof-epdo-notification@verizon.com"

        additional_alert_metadata = {}
        # Get value from alert_info field
        alert_info = file['alert_info']
        # Check whether the  is null or not.
        if not alert_info or alert_info.upper() == 'null':
            # set alert_info as empty.
            alert_info = {}
        else:
            try:
                # convert alert_info into JSON.
                alert_info = json.loads(alert_info)
            except Exception as e:
                alert_info = {}
                print("---------------->>>>>>>>Invalid format in alert_info field.")

        # Get the required_columns details from alert_info column
        required_columns = alert_info.get('required_columns', None)
        if required_columns:
            try:
                # Remove white spaces.
                required_columns = required_columns.replace(" ", "")
                # Split the value using comma.
                required_columns = required_columns.split(",")
                for column in required_columns:
                    try:
                        # Add the fields into process details dict.
                        file_details[column] = file[column]
                    except KeyError:
                        print(f"Error: {column} is not found inside reporting table.")
            except Exception as e:
                print(f"Error:{e}")

        # Alerting for files missed SLA.

        if file['sla_met'] and file['sla_met'].lower() == 'no' and (
                file['notify_sla_misses'] and file['notify_sla_misses'].upper() == 'Y') and \
                f"{file['unique_id']}|{alert_tool}|file_sla_not_met" not in generated_alert_id_and_type \
                and f"{file['unique_id']}|file_sla_not_met" not in file_alert_uid:
            print("---------------->>>>>>>>Inside alert success condition.")
            # Define details for the alert
            alert_type = 'file_sla_not_met'
            email_subject = f"TraceX | {environment_value} | Notification | {file['file_pattern']} breached SLA on {str(file['process_date'])} at {str(file['sla'])} UTC"
            greeting_text = "File breached SLA, please have look into the details."
            # Send the details to process and send alert.
            row_to_insert = None
            if file['sla_notification_buffer']:
                print("------->>>>Inside Notification Buffer")
                # Create SLA date time
                sla_date_time = datetime.strptime(f"{str(file['process_date'])} {str(file['sla'])}",
                                                  '%Y-%m-%d %H:%M:%S')
                # Add SLA date time and sla_notification_buffer
                try:
                    time_component = file['sla_notification_buffer'].split(":")
                    sla_buffer_added_date_time = sla_date_time + timedelta(hours=int(time_component[0]),
                                                                           minutes=int(time_component[1]),
                                                                           seconds=int(time_component[2]))
                except Exception as e:
                    print("---------------->>>>>>>>Invalid time format")
                    continue

                print("---------------->>>>>>>>Current Date and Time:", datetime.utcnow())
                print("---------------->>>>>>>>sla_buffer_added_date_time:", sla_buffer_added_date_time)
                # If the current datetime is greater than SLA + sla_notification_buffer then send the alert
                if datetime.utcnow() > sla_buffer_added_date_time and file['file_arrival_status'].lower() == 'pending':
                    print("---------------->>>>>>>>Inside SLA Tab")
                    row_to_insert = generate_and_sent_file_notification(file_details, to_email_list, email_subject,
                                                                        greeting_text, alert_type, file['unique_id'])

            # If sla_notification_buffer is less than or equal to zero send the alert.
            else:

                generate_and_sent_file_notification(file_details, to_email_list, email_subject, greeting_text,
                                                    alert_type, file['unique_id'])
            # If the row needs to insert to BigQuery table append to the below list
            if row_to_insert:
                file_alert_uid.append(f"{file['unique_id']}|{alert_type}")
                file_alert_to_insert.append(row_to_insert)
        else:
            print("--------------->>>>>>Main If condition not passed.")

    print("------------------------------------------------------------")
    print(file_alert_to_insert)
    print("------------------------------------------------------------")
    # If there is any dict inside process_alert_to_insert then insert it into BigQuery
    if file_alert_to_insert:
        errors = bigquery_client.insert_rows_json(f"{PROJECT_ID}.{DATASET_ID}.{FILE_ALERT_STATUS_TABLE_ID}",
                                                  file_alert_to_insert)
        if errors:
            print("---------------->>>>>>>>Encountered errors while inserting rows: {}".format(errors))
    return True


def generate_sub_process_level_notification(**context):
    """
    Generate and send sub-process level email notification.
    input:
        **context: task variable
    return:
        status: boolean
    """
    import re
    import json
    import importlib
    from datetime import datetime, timedelta
    from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
    from airflow.models import Variable

    ENVIRONMENT = Variable.get('composer_env')
    environment_value = get_environment_value()
    env_config_module = importlib.import_module("vz-it-hgrv-aidedo-0.alerting_framework.env_config")
    DATASET_ID = env_config_module.config[ENVIRONMENT]['dataset_id']
    PROJECT_ID = env_config_module.config[ENVIRONMENT]['project_id']
    GCP_CONNECTION_ID = env_config_module.config[ENVIRONMENT]['gcp_connection_id']
    Obj = BigQueryHook(gcp_conn_id=GCP_CONNECTION_ID)
    bigquery_client = Obj.get_client(project_id=PROJECT_ID)

    PROCESS_METADATA_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_metadata_table_id']
    REALTIME_PROCESS_META_TABLE_ID = env_config_module.config[ENVIRONMENT]['realtime_process_meta_table_id']
    PROCESS_MONITORING_REPORT_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_monitoring_report_table_id']
    PROCESS_NOTIFICATION_STATUS_TABLE_ID = env_config_module.config[ENVIRONMENT]['process_notification_status_table_id']
    VIEW_DATASET_ID = env_config_module.config[ENVIRONMENT]['view_dataset_id']
    TAG_RELATION_VIEW_ID = env_config_module.config[ENVIRONMENT]['tag_relation_view_id']
    PROCESS_ALERT_NOTIFICATION_META_TABLE_ID = env_config_module.config[ENVIRONMENT][
        'dof_process_alert_notification_meta_table_id']
    LINEAGE_BASE_URL = env_config_module.config[ENVIRONMENT]['lineage_base_url']
    PROGRAM_NAME_MAPPING_VIEW_ID = env_config_module.config[ENVIRONMENT]['program_name_mapping_view_id']
    task_instance = context['task_instance']
    execution_date = task_instance.xcom_pull(key='execution_date_dashed')
    print("Type of execution date:", type(execution_date))
    # Get previous execution date to fetch details from alerting table.
    previous_day_of_execution_date = str(datetime.strptime(str(execution_date), "%Y-%m-%d") - timedelta(1))[:10]
    print("previous_day_of_execution_date:", previous_day_of_execution_date)

    get_process_details_query = f"""
                SELECT 
                * 
                FROM
                (SELECT 
                    DISTINCT
                    REPLACE(ARRAY_TO_STRING(
                    [SAFE_CAST(reporting.process_id AS STRING), 
                    SAFE_CAST(reporting.process_date AS STRING), 
                    SAFE_CAST(reporting.trace_id AS STRING)],'|'), ' ', '') AS unique_id,
                    reporting.*,
                    alert_meta.*,
                    pgm_name_map.*
                FROM
                    `{PROJECT_ID}.{DATASET_ID}.{PROCESS_MONITORING_REPORT_TABLE_ID}` AS reporting
                LEFT JOIN
                  `{PROJECT_ID}.{DATASET_ID}.{PROCESS_ALERT_NOTIFICATION_META_TABLE_ID}` alert_meta
                ON 
                  reporting.process_name = alert_meta.process_name
                  AND reporting.application_name = alert_meta.application_name
                  AND reporting.program_name = alert_meta.program_name
                  AND reporting.tool_name = alert_meta.env_name
                LEFT JOIN
                  `{VIEW_DATASET_ID}.{PROGRAM_NAME_MAPPING_VIEW_ID}` as pgm_name_map
                ON
                  LOWER(reporting.program_name) = LOWER(pgm_name_map.meta_program_name)
                WHERE
                  ((suppress_alert IS NULL OR suppress_alert = False) AND (updated_at IS NULL OR updated_at < CURRENT_TIMESTAMP() - INTERVAL 30 MINUTE))
                  AND insert_timestamp > CURRENT_TIMESTAMP() - INTERVAL 30 MINUTE 
                  AND LOWER(alert_meta.notification_level) = 'subprocess'
                  AND((LOWER(reporting.process_status) = 'success' AND LOWER(alert_meta.notify_success) = 'y' AND success_notification_email_list IS NOT NULL)
                  OR (LOWER(reporting.process_status) = 'failure' AND LOWER(alert_meta.notify_failure) = 'y' AND failure_notification_email_list IS NOT NULL)
                  OR (reporting.sla_met IS NOT NULL AND LOWER(reporting.sla_met) = 'no' AND (LOWER(reporting.overall_process_status) = 'pending' OR LOWER(reporting.overall_process_status) = 'in progress') AND LOWER(alert_meta.notify_sla_miss) = 'y' AND sla_miss_notification_email_list IS NOT NULL)))
                LEFT JOIN (
                  SELECT
                    metadata_id,
                    ARRAY_AGG(tag_value) tag_value
                  FROM
                    `{PROJECT_ID}.{VIEW_DATASET_ID}.{TAG_RELATION_VIEW_ID}`
                  WHERE
                    LOWER(is_active) = 'y'
                  GROUP BY
                    metadata_id)
                ON
                  metadata_id = process_id"""

    print("====================================================================")
    print("get_process_details_query:", get_process_details_query)
    print("====================================================================")

    query_job = bigquery_client.query(get_process_details_query)
    process_list = query_job.result()

    # -------------------------------------------------------
    # Fetch all the alert generated alerts for past two days.
    seven_days_back_date = str(datetime.strptime(str(execution_date), "%Y-%m-%d") - timedelta(7))[:10]
    get_details_from_alert_status_table_query = f"""SELECT
                                          CONCAT(report_table_unique_id,"|", notification_types) AS generated_alert_id_and_type
                                        FROM
                                          `{DATASET_ID}.{PROCESS_NOTIFICATION_STATUS_TABLE_ID}`
                                        WHERE
                                          DATE(notification_created_timestamp) >= '{seven_days_back_date}'"""

    query_job = bigquery_client.query(get_details_from_alert_status_table_query)
    alert_type_list = query_job.result()
    generated_alert_id_and_type = list()
    # Create a list of generated alert.
    for alert in alert_type_list:
        generated_alert_id_and_type.append(alert['generated_alert_id_and_type'])
    # -------------------------------------------------------

    process_notification_uid_list = list()
    process_notification_to_insert = list()
    for process in process_list:
        # Building lineage URL
        mapped_program_name = process['mapped_program_name'] if process['mapped_program_name'] else process[
            'program_name']
        # Building lineage URL
        program_name_name_without_space = mapped_program_name.replace(" ", "+")
        lineage_url = LINEAGE_BASE_URL + f"run_date={str(process['process_date'])}&project_name={program_name_name_without_space}"
        print("Lineage URL------------------>>>>>>>>>>", lineage_url)

        expected_datetime_utc = str(process['expected_datetime'])[:19]
        schedule_datetime_utc = str(process['schedule_datetime'])[:19]

        if process['end_time']:
            end_time = str(process['end_time'])[:19]
        else:
            end_time = schedule_datetime_utc

        process_details = {"program_name": process['program_name'],
                           "application_name": process['application_name'],
                           "process_name": process['sub_process'],
                           "parent_process_name": process['process_name'],
                           "process_status": process['process_status'],
                           "process_date": str(process['process_date']),
                           "sla_time": f"{expected_datetime_utc} UTC",
                           "schedule_datetime": f"{schedule_datetime_utc} UTC",
                           "completion_time": f"{end_time} UTC",
                           "failure_time": f"{end_time} UTC",
                           "Pipeline Lineage": lineage_url
                           }

        if process['tag_value']:
            process_details['tags'] = ",".join(process['tag_value'])

        # Add additional columns from reporting table to notification.
        required_columns = process['notification_required_columns']
        if required_columns:
            try:
                # Remove white spaces.
                required_columns = required_columns.replace(" ", "")
                # Split the value using comma.
                required_columns = required_columns.split(",")
                for column in required_columns:
                    try:
                        # Add the fields into process details dict.
                        process_details[column] = process[column]
                    except KeyError:
                        print(f"Error: {column} is not found inside reporting table.")
            except Exception as e:
                print(f"Error:{e}")

        print("======================================================")
        print("Process details----------->>>>>>>>>>>>>>>>>", process_details)
        print("======================================================")

        # Sent success notification.
        if process['process_status'] and process['process_status'].lower() == 'success' \
                and process[
            'notify_success'].lower() == 'y' and f"{process['unique_id']}|success" not in generated_alert_id_and_type:
            print("Inside success notification.")

            to_email_list = list()
            if process['success_notification_email_list']:
                to_email = process['success_notification_email_list'].replace(' ', '')
                to_email = to_email.replace('\n', '')
                # Convert the string to list.
                to_email_list = to_email.split(",")
                # Remove null character from list.
                to_email_list = list(filter(None, to_email_list))

            if to_email_list:
                notification_type = "success"
                email_subject = f"TraceX | {environment_value} | Notification | {process['sub_process']} succeeded on {end_time} UTC"
                row_to_insert = evaluate_notifications(process_details, notification_type, email_subject,
                                                       to_email_list,
                                                       process['unique_id'], [],
                                                       process_notification_uid_list)
                if row_to_insert:
                    process_notification_to_insert.append(row_to_insert)
                    process_notification_uid_list.append(f"{process['unique_id']}|{notification_type}")
            else:
                print("Success notification email list is empty.")

        # Add additional info to notification(Only for SLA miss and failure.).
        notification_additional_info = process['notification_additional_info']
        if notification_additional_info:
            try:
                # Convert the data to json
                notification_additional_info = json.loads(notification_additional_info)
                for key, value in notification_additional_info.items():
                    process_details[key] = value
            except Exception as e:
                print(f"Error:{e}")

        # Sent failure notification.
        if process['process_status'] and process['process_status'].lower() == 'failure' \
                and process[
            'notify_failure'].lower() == 'y' and f"{process['unique_id']}|failure" not in generated_alert_id_and_type:
            print("Inside failure notification.")

            to_email_list = list()
            if process['failure_notification_email_list']:
                to_email = process['failure_notification_email_list'].replace(' ', '')
                to_email = to_email.replace('\n', '')
                # Convert the string to list.
                to_email_list = to_email.split(",")
                # Remove null character from list.
                to_email_list = list(filter(None, to_email_list))

            if to_email_list:
                process_details['error_message'] = process['error_message']
                process_details['Log URL'] = process['log_file']
                notification_type = "failure"
                email_subject = f"TraceX | {environment_value} | Failure | {process['sub_process']} | {process['program_name']} | {process['application_name']} | {end_time} UTC"
                row_to_insert = evaluate_notifications(process_details, notification_type, email_subject,
                                                       to_email_list,
                                                       process['unique_id'], [],
                                                       process_notification_uid_list)
                if row_to_insert:
                    process_notification_to_insert.append(row_to_insert)
                    process_notification_uid_list.append(f"{process['unique_id']}|{notification_type}")
            else:
                print("Failure notification email list is empty.")
        # If status is still in pending and SLA cross ----------->>>> Overdue
        # If status is in progress and SLA cross then --------->>>> SLA miss
        # Sent SLA miss notification
        if process['sla_met'] and process['sla_met'].lower() == 'no' and process['notify_sla_miss'].lower() == 'y' and \
                process['process_status'].lower() in ["pending",
                                                      "progress"] and f"{process['unique_id']}|sla_miss" not in generated_alert_id_and_type:
            # Inside SLA miss notification.
            to_email_list = list()
            # This flag will decide whether we need to send email or not, when there is a sla buffer.
            send_email_flag = False
            if process['sla_miss_notification_email_list']:
                to_email = process['sla_miss_notification_email_list'].replace(' ', '')
                to_email = to_email.replace('\n', '')
                # Convert the string to list.
                to_email_list = to_email.split(",")
                # Remove null character from list.
                to_email_list = list(filter(None, to_email_list))
            # Add process duration information in the email if process duration exceeded the average duration.

            try:
                if process['duration_alert'].lower() == 'y':
                    process_details['Sub-process Execution Duration'] = str(
                        timedelta(seconds=int(process['duration_in_seconds'])))
                    process_details['Sub-process Average Execution Duration'] = str(
                        timedelta(seconds=int(process['avg_duration_in_seconds'])))
                    process_details['Sub-process Threshold Limit'] = str(
                        timedelta(seconds=int(process['duration_threshold_high_limit'])))
            except Exception as e:
                print(e)

            if process['sla_buffer']:
                # Removes spaces.
                time_component = process['sla_buffer'].replace(" ", "")
                if re.match(r"^\d\d:\d\d:\d\d$", time_component):
                    time_component = process['sla_buffer'].split(":")
                    sla_date_time = datetime.strptime(str(process['expected_datetime'])[0:19], '%Y-%m-%d %H:%M:%S')

                    sla_buffer_added_date_time = sla_date_time + timedelta(hours=int(time_component[0]),
                                                                           minutes=int(time_component[1]),
                                                                           seconds=int(time_component[2]))
                    print("Current Date and Time:", datetime.utcnow())
                    print("sla_buffer_added_date_time:", sla_buffer_added_date_time)
                    # Check whether the current time is greater than buffer added sla time
                    if datetime.utcnow() > sla_buffer_added_date_time and process[
                        'process_status'].lower() == 'pending':
                        send_email_flag = True
                else:
                    print("Value in SLA buffer column is not a time format. Proceeding without SLA buffer.")
                    send_email_flag = True
            else:
                send_email_flag = True

            if to_email_list and send_email_flag:
                notification_type = "sla_miss"

                email_subject = f"TraceX | {environment_value} | SLA Breach | {process['sub_process']} | {process['program_name']} | {process['application_name']} | {expected_datetime_utc} UTC"
                row_to_insert = evaluate_notifications(process_details, notification_type, email_subject,
                                                       to_email_list,
                                                       process['unique_id'], [],
                                                       process_notification_uid_list)
                if row_to_insert:
                    process_notification_to_insert.append(row_to_insert)
                    process_notification_uid_list.append(f"{process['unique_id']}|{notification_type}")
            else:
                print("SLA miss notification email list is empty.")

    if process_notification_to_insert:
        errors = bigquery_client.insert_rows_json(f"{PROJECT_ID}.{DATASET_ID}.{PROCESS_NOTIFICATION_STATUS_TABLE_ID}",
                                                  process_notification_to_insert)
        if errors:
            print("Encountered errors while inserting rows: {}".format(errors))
    return True

