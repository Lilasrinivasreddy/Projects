import pandas as pd
import json
import re
import pytz
import pandas_gbq
import numpy as np
from croniter import croniter
from datetime import datetime, timedelta

data_bytes = '/Users/reddyvu/Desktop/Test/DataX_Master_Metadata.xlsx'

print("Script Started")

files_meta = pd.read_excel(data_bytes, sheet_name="DataX_File_Meta", dtype=str, keep_default_na=False, index_col=None)

# Drop first column
files_meta = files_meta.iloc[:, 1:]

# Drop row which contains description
files_meta = files_meta.reset_index(drop=True)
files_meta = files_meta.drop(index=[0, 1])

files_column_list = ['dag_id', 'step_id', 'file_name', 'file_path', 'landing_server_name', 
                     'environment_name', 'file_middle_pattern', 'file_prefix', 'file_suffix', 
                     'schedule_interval', 'frequency', 'timezone', 'is_mandatory', 'is_active', 
                     'file_arrival_custom_interval', 'sla', 'source_system', 'source_poc', 
                     'sla_notification_buffer', 'notify_source', 'notify_sla_misses', 
                     'alert_channel', 'alert_target_team_name', 'notify_file_size', 
                     'size_threshold_low_limit', 'size_threshold_high_limit', 
                     'size_threshold_limit_unit', 'interval', 'archive_locaton', 
                     'failure_location', 'sourceFormat', 'sourceUris', 'fixedWidth', 
                     'fieldDelimiter', 'skipLeadingRows', 'maxBadRecords', 'nullMarker', 
                     'allowQuotedNewlines', 'allowJaggedRows', 'ignoreUnknownValues', 
                     'filesensorretries', 'filesensorretrydelay', 'timeout', 'fileSensor', 
                     'gcsBucket', 'object']

files_mandatory_column_list = ['dag_id', 'file_name', 'file_path', 'landing_server_name', 
                               'environment_name', 'file_prefix', 'file_suffix', 
                               'schedule_interval', 'frequency', 'sla', 'source_system', 
                               'source_poc', 'alert_channel', 'alert_target_team_name']

files_integer_column = ['step_id', 'file_arrival_custom_interval', 'size_threshold_low_limit', 
                        'size_threshold_high_limit', 'interval']

files_boolean_column = ['is_mandatory', 'is_active', 'notify_source', 'notify_sla_misses', 
                        'notify_file_size']

frequency_list = ["real_time", "hourly_custom", "weekly", "monthly", "fixed", "hourly", "daily"]
flags = [True, False]
env_list = ["gcp_bq", "unix", "java", "edw_td", "gcp", "edl_hdfs", "gcp_gcs", "onprem_hive", "gcp_hive"]
time_zone_list = ["UTC", "EST"]

# Column Validation
metadata_files_columns = list(files_meta.columns.values)
missed_fle_col = set(files_column_list).difference(metadata_files_columns)
add_fle_col = set(metadata_files_columns).difference(files_column_list)

if len(missed_fle_col) == 0:
    missed_fle_col = 'NA'
if len(add_fle_col) == 0:
    add_fle_col = 'NA'

print(f"Missing columns: {missed_fle_col}")
print(f"Additional columns: {add_fle_col}")

# Mandatory Columns Validation
file_col_check = []
for mandatory_col in files_mandatory_column_list:
    if (files_meta[mandatory_col].isna().any() or 
        (files_meta[mandatory_col].astype(str).str.strip() == '').any() or 
        (files_meta[mandatory_col].astype(str) == '0').any()):
        file_col_check.append(mandatory_col)

if len(file_col_check) > 0:
    raise ValueError("FILE META VALIDATION: Following mandatory columns can't be left empty: " + str(file_col_check))

# Integer Columns Validation
for col in files_integer_column:
    files_meta[col] = pd.to_numeric(files_meta[col], errors='coerce').fillna(0).astype(int)

# Boolean Columns Validation
for col in files_boolean_column:
    files_meta[col] = files_meta[col].replace('', np.nan).map({'Y': True, 'N': False}).fillna(False)

# Frequency Validation
invalid_frequencies = files_meta[~files_meta['frequency'].str.lower().isin(frequency_list)]
if not invalid_frequencies.empty:
    print(f"Invalid frequencies found: {invalid_frequencies['frequency'].unique()}")

# Email Validation
invalid_emails = files_meta[~files_meta['source_poc'].str.contains(r'[^@]+@[^@]+\.[^@]+', na=False, case=False)]
if not invalid_emails.empty:
    print(f"Invalid email addresses found in source_poc: {invalid_emails['source_poc'].unique()}")

print("Validation completed.")
