import pandas as pd
import sys
import os
import re
import json
import numpy as np
import pytz
import pandas_gbq
import smtplib
from google.cloud import bigquery
from email.message import EmailMessage
from google.cloud import storage
# from croniter import croniter
from datetime import datetime, timedelta
import config
from metadata_functions import *

# def load_table(tables_meta,table_id):
#     job_config=bigquery.LoadJobConfig (autodetect=False,write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE)
#     job=client.load_table_from_dataframe(tables_meta,table_id,job_config=job_config)
#     return job.result()

client=bigquery.Client()
storage_client = storage.Client()
project_id= config.project_id
dataset=config.dataset
bucket_name = config.bucket_name
process_table_id=f"{project_id}.{dataset}.datax_process_meta_stg"
task_table_id=f"{project_id}.{dataset}.datax_task_meta_stg"
files_table_id=f"{project_id}.{dataset}.datax_file_meta_stg"
tables_table_id=f"{project_id}.{dataset}.datax_table_meta_stg"
finops_table_id=f"{project_id}.{dataset}.datax_label_meta_stg"


def dof_master_meta_gcs_bq_trigger_api(event,context):
    try:
        env= "dev"
        # env = os.environ.get("env")
        print(env)
        # name of the file which triggers the function
        file_name = event["name"]
        print("File Name is: " + file_name)
        blob_name = file_name
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        data_bytes = blob.download_as_bytes()
        process_count = 0

        tables_meta = pd.read_excel(data_bytes, sheet_name="DataX_Table_Meta",keep_default_na=False,dtype=str,index_col=None)
        files_meta = pd.read_excel(data_bytes, sheet_name="DataX_File_Meta",keep_default_na=False,dtype=str,index_col=None)
        process_meta = pd.read_excel(data_bytes, sheet_name="DataX_Process_Meta",keep_default_na=False,dtype=str,index_col=None)
        task_meta = pd.read_excel(data_bytes, sheet_name="DataX_Task_Meta",keep_default_na=False,dtype=str,index_col=None)
        finops_meta = pd.read_excel(data_bytes, sheet_name="DataX_Tags_Meta",keep_default_na=False,dtype=str,index_col=None)

        # Drop first column
        tables_meta=tables_meta.iloc[:,1:]
        files_meta=files_meta.iloc[:,1:]
        process_meta=process_meta.iloc[:,1:]
        task_meta=task_meta.iloc[:,1:]
        finops_meta=finops_meta.iloc[:,1:]

        # Drop row which contains description
        process_meta=process_meta.reset_index(drop=True)
        process_meta=process_meta.drop(index=[0,1])
        task_meta=task_meta.reset_index(drop=True)
        task_meta=task_meta.drop(index=[0,1])
        files_meta=files_meta.reset_index(drop=True)
        files_meta=files_meta.drop(index=[0,1])
        tables_meta=tables_meta.reset_index(drop=True)
        tables_meta=tables_meta.drop(index=[0,1])
        finops_meta=finops_meta.reset_index(drop=True)
        finops_meta=finops_meta.drop(index=[0,1])

        tables_column_list=['dag_id','task_id','composer_instance_name','step_id','table_name','db_name','server_name','platform_name','environment_name','is_active','notify_table_size','threshold_unit','threshold_low','threshold_up','volume_incident_team_name','table_size_notification_email_list','alert_table_size','volume_alert_channel','volume_query','montior_volume']
        files_column_list=['dag_id','composer_instance_name','task_id','step_id','file_name','file_path','landing_server_name','direction','environment_name','file_middle_pattern','file_prefix','file_suffix','schedule_interval','frequency','timezone','is_mandatory','is_active','file_arrival_custom_interval','sla','source_system','source_poc','sla_notification_buffer','notify_source','notify_sla_misses','alert_channel','alert_target_team_name','notify_file_size','size_threshold_low_limit','size_threshold_high_limit','size_threshold_limit_unit','interval','archive_locaton','failure_location','sourceFormat','sourceUris','fixedWidth','fieldDelimiter','skipLeadingRows','maxBadRecords','nullMarker','allowQuotedNewlines','allowJaggedRows','ignoreUnknownValues','filesensorretries','filesensorretrydelay','timeout','fileSensor','gcsBucket','object']
        process_column_list= ['process_name','process_description','dag_id','project_id','composer_instance_name','vsad','project_space','job_type','data_stream','load_type','platform_name','job_url','scheduler_name','timezone','schedule_interval','frequency','max_active_runs','sla','interval','tracex_tags','run_date_buffer_interval','sla_alert_buffer','duration_threshold_high_value','alert_channel','alert_channal_target_name','alert_additional_info','alert_failure_by','alert_priority','alert_failure','alert_slamiss','alert_longrunning','alert_startoverdue','is_critical','logs_trace_id_enabled','sla_type','code_gcs_location','code_git_link','prod_request_number','production_date','notify_email_success','notify_email_failure','notify_email_slamiss','notify_email_longrunning','notify_email_startoverdue','notify_slack_success','notify_slack_failure','notify_slack_slamiss','notify_slack_longrunning','notify_slack_startoverdue','dev_poc','dev_poc_manager','devgroup_email','mod_poc','legacy_esp_application_name','legacy_esp_process_name','runbook_link','logs_enabled','upstream','downstream','additional_details_json']
        task_column_list=['dag_id','composer_instance_name','task_id','description','step_id','task_type','platform_name','env_name','source_type','source_environment','source_server_name','source','rt_subscription','target_type','target_environment','target_server_name','target','retries','retry_delay','is_active','is_mandatory']
        finops_column_list=['job_id','instance_name','job_engine','vsad','project_id','lob','sub_lob','portfolio','product_area','product_name','program_name','data_product_name','data_domain','data_sub_domain','use_case_name','is_active','updated_timestamp','inserted_timestamp','modified_by']
        config__column_list  = ["variable","dev","test","prod"]

        process_integer_column=['duration_threshold_high_value','max_active_runs','run_date_buffer_interval','interval']
        task_integer_column=['retries','step_id']
        files_integer_column=['step_id','file_arrival_custom_interval','size_threshold_low_limit','size_threshold_high_limit','interval']
        table_integer_column=['step_id','threshold_up','threshold_low']
        finops_integer_column=[]
        process_boolean_column=['alert_failure','alert_slamiss','alert_longrunning','alert_startoverdue',
'is_critical','logs_enabled','logs_trace_id_enabled']
        table_boolean_column=['notify_table_size','is_active','alert_table_size']
        task_boolean_column=['is_mandatory','is_active']
        finops_boolean_column=['is_active']
        files_boolean_column=['is_mandatory','is_active','notify_source','notify_sla_misses','notify_file_size']

        tables_mandatory_column_list=["dag_id","task_id","step_id","table_name","db_name","server_name","platform_name","is_active"]
        task_mandatory_column_list=["dag_id","composer_instance_name","task_id","description","task_type","platform_name","env_name","source_type","source_environment","source_server_name"]
        process_mandatory_column_list=['dag_id','project_id','composer_instance_name','vsad','project_space','job_type','data_stream','load_type','platform_name','job_url','scheduler_name','timezone','schedule_interval','frequency','sla','is_critical','sla_type','code_gcs_location','code_git_link','prod_request_number','production_date','dev_poc','dev_poc_manager','devgroup_email','mod_poc','runbook_link']
        files_mandatory_column_list=['dag_id','composer_instance_name','task_id','file_name','file_path','landing_server_name','environment_name','file_prefix','file_suffix','schedule_interval','frequency','direction','sla','source_system','source_poc','alert_channel','alert_target_team_name']
        finops_mandatory_column_list=['job_id','instance_name','job_engine', 'vsad','project_id','lob','sub_lob','portfolio','product_area','product_name','program_name']



        frequency_list = ["real_time", "hourly_custom", "weekly", "monthly", "fixed", "hourly", "daily"]
        flags = [True,False]
        valid_alerts = ['application', 'tracex', 'beamx']
        bool_flag = ["true","false"]
        platform_list = ["gcp","unix","aws","teradata","edl"]
        env_list = ["gcp_bq","unix","java","edw_td","gcp","edl_hdfs","gcp_gcs","onprem_hive","gcp_hive"]
        scheduler_list = ["airflow","composer","espx","cron","oozie","trigger"]
        businessunit_list = ["vbg","network","vcg","corporate"]
        source_type_list = ["pubsub","table","esp","file","kafka"]
        target_type_list = ["file","kafka","pubsub","table","topic"]
        config_column_list  = ["variable","dev","test","prod"]
        time_zone_list = ["UTC","EST"]
        priority_list = ["P1","P2","P3","P4","P5"]
        true_flag = ["y","Y"]
        env="dev"

        config_df = pd.read_excel(data_bytes, sheet_name="Variables", dtype=str)
        config_df.dropna(axis=0, how="all", inplace=True)
        config_df = config_df.applymap(str)
        variable_list = list(config_df["variable"])
        dev_list = list(config_df["dev"])
        test_list = list(config_df["test"])
        prod_list = list(config_df["prod"])
        ple_list =  list(config_df["ple"])
        print(variable_list)
        print("-------------------------------------------------")
        print(dev_list)
        print("-------------------------------------------------")
        print(test_list)
        print("-------------------------------------------------")
        print(prod_list)
        print("-------------------------------------------------")
        print(ple_list)
        print("-------------------------------------------------")

        #Check for mismatch between variable list and values provided in columns
        if len(variable_list) != len(dev_list):
            raise ValueError(
                "Variable column and dev column values are not mapping correctly. Length of either of them has issues. Kindly check the config tab"
            )
        if len(variable_list) != len(test_list):
            raise ValueError(
                "Variable column and test column values are not mapping correctly. Length of either of them has issues. Kindly check the config tab"
            )
        if len(variable_list) != len(prod_list):
            raise ValueError(
                "Variable column and prod column values are not mapping correctly. Length of either of them has issues. Kindly check the config tab"
            )
        if len(variable_list) != len(ple_list):
            raise ValueError(
                "Variable column and prod column values are not mapping correctly. Length of either of them has issues. Kindly check the config tab"
            )


        #----------------------------------------column_validation-----------------------------------------------
        metadata_tables_columns = list(tables_meta.columns.values)
        metadata_files_columns = list(files_meta.columns.values)
        metadata_process_columns = list(process_meta.columns.values)
        metadata_task_columns = list(task_meta.columns.values)
        metadata_finops_columns = list(finops_meta.columns.values)

        missed_tbl_col = (set(tables_column_list).difference(metadata_tables_columns))
        missed_fle_col = (set(files_column_list).difference(metadata_files_columns))
        missed_prcs_col = (set(process_column_list).difference(metadata_process_columns))
        missed_tsk_col = (set(task_column_list).difference(metadata_task_columns))
        missed_fnps_col = (set(finops_column_list).difference(metadata_finops_columns))

        add_tbl_col = (set(metadata_tables_columns).difference(metadata_tables_columns))
        add_fle_col = (set(metadata_files_columns).difference(metadata_files_columns))
        add_prcs_col = (set(metadata_process_columns).difference(metadata_process_columns))
        add_tsk_col = (set(metadata_task_columns).difference(metadata_task_columns))
        add_fnps_col = (set(metadata_finops_columns).difference(metadata_finops_columns))

        missing_column_cnt = len(missed_tbl_col) + len(missed_fle_col) + len(missed_prcs_col) + len(missed_tsk_col) + len(missed_fnps_col)
        additional_column_cnt = len(add_tbl_col) + len(add_fle_col) + len(add_prcs_col) + len(add_tsk_col) + len(add_fnps_col)
        if len(missed_tbl_col)== 0:
            missed_tbl_col = 'NA'
        if len(missed_fle_col)== 0:
            missed_fle_col = 'NA'
        if len(missed_prcs_col)== 0:
            missed_prcs_col = 'NA'
        if len(missed_tsk_col)== 0:
            missed_tsk_col = 'NA'
        if len(missed_fnps_col)== 0:
            missed_fnps_col = 'NA'
        if len(add_tbl_col)== 0:
            add_tbl_col = 'NA'
        if len(add_fle_col)== 0:
            add_fle_col = 'NA'
        if len(add_prcs_col)== 0:
            add_prcs_col = 'NA'
        if len(add_tsk_col)== 0:
            add_tsk_col = 'NA'
        if len(add_fnps_col)== 0:
            add_fnps_col = 'NA'

        #-----------------------TABLE_VALIDATION_COLUMNS----------------------------------------------------------------
        if not tables_meta.empty:
            # Converting empty strings to null values
            tables_meta = tables_meta.infer_objects(copy=False).replace(r'^\s*$', np.nan, regex=True)

            # Remove whitespaces
            tables_meta = tables_meta.map(lambda x: x.strip() if isinstance(x, str) else x)
            table_env_validation_cnt = table_platform_validation_cnt=table_is_active_validation_cnt=table_name_validation_cnt=table_threshold_low_validation_cnt=table_threshold_high_validation_cnt=notify_table_size_validation_cnt=table_col_check=0
            total_tables_count = tables_meta.shape[0]
            #Updating the Variables
            if "dev" in env:
                tables_meta.infer_objects(copy=False).replace(variable_list, dev_list,inplace=True,regex=True)
                print(tables_meta)
            elif "test" in env:
                tables_meta.infer_objects(copy=False).replace(variable_list, test_list, inplace=True,regex=True)
                print(tables_meta)
            elif "ple" in env:
                tables_meta.infer_objects(copy=False).replace(variable_list, ple_list, inplace=True,regex=True)
                print(tables_meta)
            else:
                tables_meta.infer_objects(copy=False).replace(variable_list, prod_list, inplace=True,regex=True)
                print(tables_meta)

            tables_meta = tables_meta.astype("string")
                
            # Environment Name Validation
            table_env_validation_cnt = total_tables_count - tables_meta['environment_name'].str.lower().isin(env_list).sum()

            # Platform Name Validation
            table_platform_validation_cnt = total_tables_count - tables_meta['platform_name'].str.lower().isin(platform_list).sum()

            # Is Active Validation
            tables_meta['is_active']=tables_meta['is_active'].fillna('y')
            table_is_active_validation_cnt = total_tables_count - tables_meta['is_active'].str.upper().isin(['Y', 'N']).sum()


            # Threshold Low Validation (Must be Numeric)
            low_null_count = tables_meta['threshold_low'].replace('', pd.NA).isna().sum()
            low_numeric_count = tables_meta['threshold_low'].fillna('').astype(str).apply(lambda x: x.strip().isdigit()).sum()
            table_threshold_low_validation_cnt = total_tables_count - low_null_count - low_numeric_count

            # Threshold High Validation (Must be Numeric)
            high_null_count = tables_meta['threshold_up'].replace('', pd.NA).isna().sum()
            high_numeric_count = tables_meta['threshold_up'].fillna('').astype(str).apply(lambda x: x.strip().isdigit()).sum()
            table_threshold_high_validation_cnt = total_tables_count - high_null_count - high_numeric_count

            # Notify Table Size Validation (Boolean Y/N)
            notify_table_size_validation_cnt = total_tables_count - tables_meta['notify_table_size'].fillna('n').str.upper().isin(['Y', 'N']).sum()

            #Table name validation
            table_name_validation_cnt = tables_meta['table_name'].isna().sum() +tables_meta.loc[tables_meta['table_name'].str.contains(r'\s',case=False), :].shape[0]
            
            # Checking the null values in Mandatory Columns
            table_col_check=[]
            for mandatory_col in tables_mandatory_column_list:
                if(tables_meta[mandatory_col].isna().any() or (tables_meta[mandatory_col].astype(str).str.strip()=="").any()):
                    print("checking for mandatory columns")
                    table_col_check.append(mandatory_col)
            
            #Updating the datatype for Integer Column
            for col in table_integer_column:
                tables_meta[col]=tables_meta[col].replace('',np.nan).fillna('0').astype(int)

            #Updating the datatype for Boolean Column    
            for col in table_boolean_column:
                tables_meta[col]=tables_meta[col].replace('',np.nan).str.upper().map({'Y': True,'N':False}).fillna(False)
            
            table_stg = 1

            # Raising Validation Errors
            if table_env_validation_cnt > 0 or \
            table_platform_validation_cnt > 0 or \
            table_is_active_validation_cnt > 0 or \
            table_name_validation_cnt > 0 or \
            table_threshold_low_validation_cnt > 0 or \
            table_threshold_high_validation_cnt > 0 or \
            notify_table_size_validation_cnt > 0 or \
            len(table_col_check) > 0:
                raise ValueError(
                    f"TABLE META VALIDATION FAILED:\n\n"
                    f"Environment name validation failed count: {table_env_validation_cnt if table_env_validation_cnt > 0 else 'None'}\n"
                    f"Platform name validation failed count: {table_platform_validation_cnt if table_platform_validation_cnt > 0 else 'None'}\n"
                    f"Is_active validation failed count: {table_is_active_validation_cnt if table_is_active_validation_cnt > 0 else 'None'}\n"
                    f"Table name validation failed count: {table_name_validation_cnt if table_name_validation_cnt > 0 else 'None'}\n"
                    f"Threshold low validation failed count: {table_threshold_low_validation_cnt if table_threshold_low_validation_cnt > 0 else 'None'}\n"
                    f"Threshold high validation failed count: {table_threshold_high_validation_cnt if table_threshold_high_validation_cnt > 0 else 'None'}\n"
                    f"Notify table size validation failed count: {notify_table_size_validation_cnt if notify_table_size_validation_cnt > 0 else 'None'}\n"
                    f"Mandatory columns missing: {table_col_check if len(table_col_check) > 0 else 'None'}"
                )
            else:
                load_table(tables_meta, tables_table_id)
                print("Data Loaded Table staging Table")

        else:
            print("Tables Tab is Empty")
            table_stg = 0


        #-----------------------FILE_VALIDATION_COLUMNS----------------------------------------------------------------

        if not files_meta.empty:
            # Converting empty strings to null values
            files_meta = files_meta.infer_objects(copy=False).replace(r'^\s*$', np.nan, regex=True)

            # Remove whitespaces
            files_meta = files_meta.applymap(lambda x: x.strip() if isinstance(x, str) else x)
            file_name_validation_cnt=f_frequency_validation_cnt=f_env_name_validation_cnt=f_is_active_validation_cnt=f_timezone_validation_cnt=source_poc_validation_cnt=file_prefix_cnt=file_suffix_cnt=notify_sla_misses_cnt=notify_file_size_cnt=file_size_limit_cnt=f_threshold_low_cnt=f_threshold_up_cnt = 0


            files_meta.dropna(axis=0, how="all", inplace=True)
            total_files_count = files_meta.shape[0]

            # Updating the Variables
            if "dev" in env:
                files_meta.replace(variable_list, dev_list, inplace=True, regex=True)
            elif "test" in env:
                files_meta.replace(variable_list, test_list, inplace=True, regex=True)
            elif "ple" in env:
                files_meta.replace(variable_list, ple_list, inplace=True, regex=True)
            else:
                files_meta.replace(variable_list, prod_list, inplace=True, regex=True)

            files_meta = files_meta.astype("string")

            # Validations 
            # total_files_count = len(files_meta)
            

            #frequency validation
            f_frequency_validation_cnt = total_files_count - files_meta['frequency'].str.lower().isin(frequency_list).sum()
            # f_platform_name_validation_cnt = total_files_count - files_meta['platform_name'].str.lower().isin(platform_list).sum()
            #env_name validation -[GCP_BQ,Unix,JAVA,EDW_TD,GCP,EDL_HDFS,GCP_GCS,OnPrem_Hive,GCP_HIVE)]
            f_env_name_validation_cnt = total_files_count - files_meta['environment_name'].str.lower().isin(env_list).sum()
            #timezone validation [utc/est]
            f_timezone_validation_cnt = total_files_count - files_meta['timezone'].str.lower().isin(["utc", "est"]).sum()
            #is_active validation
            files_meta['is_active']=files_meta['is_active'].fillna('y')
            f_is_active_validation_cnt = total_files_count - files_meta['is_active'].str.upper().isin(['Y', 'N']).sum()
            # files_meta['is_active'] = files_meta['is_active'].fillna('n').astype(str)
            # f_is_active_validation_cnt = total_files_count - files_meta['is_active'].str.lower().isin(flags).sum()
            # Source POC validation
            null_source_poc_cnt = files_meta['source_poc'].isnull().sum()
            source_poc_validation_cnt = total_files_count - files_meta.loc[
                files_meta['source_poc'].fillna('na').str.contains(r'[^@]+@[^@]+\.[^@]+', case=False), :
            ].shape[0]
            source_poc_validation_cnt -= null_source_poc_cnt
            # File prefix, middle pattern, and suffix validation
            file_prefix_cnt = ((files_meta['file_middle_pattern'].notna()) & 
                            (files_meta['file_name'] != files_meta['file_prefix'] + 
                                                        files_meta['file_middle_pattern'] + 
                                                        files_meta['file_suffix'])).sum() + \
                            ((files_meta['file_middle_pattern'].isna()) & 
                            (files_meta['file_name'] != files_meta['file_prefix'] + 
                                                        files_meta['file_suffix'])).sum()
            file_suffix_cnt = file_middle_pattern_cnt = file_prefix_cnt

            # Notify SLA misses validation
            notify_sla_misses_cnt = len(files_meta['notify_sla_misses'].fillna('n')
                                        .replace('', 'n', inplace=False)
                                        .str.lower().loc[~files_meta['notify_sla_misses']
                                                        .fillna('n')
                                                        .replace('', 'n', inplace=False)
                                                        .str.upper().isin(['Y', 'N'])])
            # Notify file size validation
            notify_file_size_cnt = len(files_meta['notify_file_size'].fillna('n')
                                    .replace('', 'n', inplace=False)
                                    .str.lower().loc[~files_meta['notify_file_size']
                                                        .fillna('n')
                                                        .replace('', 'n', inplace=False)
                                                        .str.upper().isin(['Y', 'N'])])

        
            # Threshold low validation
            low_null_count = files_meta['size_threshold_low_limit'].replace('', pd.NA, inplace=False).isna().sum()
            low_numeric_count = files_meta['size_threshold_low_limit'].fillna('').astype(str).apply(lambda x: x.strip().isdigit()).sum()
            f_threshold_low_cnt = total_files_count - low_null_count - low_numeric_count

            # Threshold up validation
            up_null_count = files_meta['size_threshold_high_limit'].replace('', pd.NA, inplace=False).isna().sum()
            up_numeric_count = files_meta['size_threshold_high_limit'].fillna('').astype(str).apply(lambda x: x.strip().isdigit()).sum()
            f_threshold_up_cnt = total_files_count - up_null_count - up_numeric_count
            
            # Checking for mandatory columns
            file_col_check = []
            for mandatory_col in files_mandatory_column_list:
                if files_meta[mandatory_col].isna().any() or (files_meta[mandatory_col].astype(str).str.strip() == "").any():
                    file_col_check.append(mandatory_col)
            
            # Updating the datatype for Integer Columns
            for col in files_integer_column:
                files_meta[col] = files_meta[col].replace('', np.nan).fillna('0').astype(int)

            # Updating the datatype for Boolean Columns
            for col in files_boolean_column:
                files_meta[col] = files_meta[col].replace('', np.nan).str.upper().map({'Y': True, 'N': False}).fillna(False)

            file_stg = 1

            # Raising validation errors
            if file_name_validation_cnt > 0 or f_frequency_validation_cnt > 0 or \
            f_env_name_validation_cnt > 0 or f_timezone_validation_cnt > 0 or \
            f_is_active_validation_cnt > 0 or len(file_col_check) > 0 or \
            source_poc_validation_cnt > 0 or file_prefix_cnt > 0 or notify_sla_misses_cnt > 0 or \
            notify_file_size_cnt > 0 or f_threshold_low_cnt > 0 or f_threshold_up_cnt > 0:
                raise ValueError(
                    f"FILE META VALIDATION FAILED:\n\n"
                    f"File name validation failed count: {file_name_validation_cnt}\n"
                    f"Frequency validation failed count: {f_frequency_validation_cnt}\n"
                    f"Environment name validation failed count: {f_env_name_validation_cnt}\n"
                    f"Timezone validation failed count: {f_timezone_validation_cnt}\n"
                    f"Is_active validation failed count: {f_is_active_validation_cnt}\n"
                    f"Source POC validation failed count: {source_poc_validation_cnt}\n"
                    f"File prefix validation failed count: {file_prefix_cnt}\n"
                    f"Notify SLA misses validation failed count: {notify_sla_misses_cnt}\n"
                    f"Notify file size validation failed count: {notify_file_size_cnt}\n"
                    f"Threshold low validation failed count: {f_threshold_low_cnt}\n"
                    f"Threshold up validation failed count: {f_threshold_up_cnt}\n"
                    f"Mandatory columns missing: {file_col_check}"
                )
            else:
                load_table(files_meta, files_table_id)
                print("Data loaded to File staging Table")
        else:
            print("Files Tab is Empty")
            file_stg = 0
            
        print("end")

       #------------------TASK_VALIDATION_COLUMNS---------------------
        if not task_meta.empty:
            # Converting empty string to null values
            task_meta= task_meta.infer_objects(copy=False).replace(r'^\s*$',np.nan,regex=True)

            total_task_count = task_meta.shape[0]
            task_platform_validation_cnt=task_env_validation_cnt=task_is_active_validation_cnt=task_id_validation_cnt=dag_id_validation_cnt=source_type_validation_cnt=target_type_validation_cnt=task_col_check=0
            #Remove whitespaces
            task_meta=task_meta.map(lambda x:x.strip() if isinstance(x,str) else x)
            dictionary = dict(zip(variable_list, dev_list))
            if "dev" in env:
                task_meta.infer_objects(copy=False).replace(dictionary, inplace=True,regex=True)
            elif "test" in env:
                task_meta.infer_objects(copy=False).replace(variable_list, test_list, inplace=True,regex=True)
            elif "ple" in env:
                task_meta.infer_objects(copy=False).replace(variable_list, ple_list, inplace=True,regex=True)
            else:
                task_meta.infer_objects(copy=False).replace(variable_list, prod_list, inplace=True,regex=True)

            task_meta = task_meta.astype("string")
           
            # Platform Name Validation
            task_platform_validation_cnt = total_task_count - task_meta['platform_name'].str.lower().isin(platform_list).sum()

            # Environment Name Validation
            task_env_validation_cnt = total_task_count - task_meta['env_name'].str.lower().isin(env_list).sum()

            # Is Active Validation
            task_meta['is_active']=task_meta['is_active'].fillna('y')
            task_is_active_validation_cnt = total_task_count - task_meta['is_active'].str.upper().isin(['Y', 'N']).sum()

            # Task ID Validation (Mandatory and Non-Empty)
            task_id_validation_cnt = task_meta['task_id'].isna().sum() + \
                                    task_meta['task_id'].astype(str).str.strip().eq('').sum()

            # DAG ID Validation (Mandatory and Non-Empty)
            dag_id_validation_cnt = task_meta['dag_id'].isna().sum() + \
                                    task_meta['dag_id'].astype(str).str.strip().eq('').sum()

            # Source Type Validation (Must Match Predefined List)
            source_type_validation_cnt = total_task_count - task_meta['source_type'].str.lower().isin(source_type_list).sum()

            # Target Type Validation (Must Match Predefined List)
            target_type_validation_cnt = total_task_count - task_meta['target_type'].fillna('').str.lower().isin(target_type_list+['']).sum()

            # Checking the null values in Mandatory Columns
            task_col_check = []
            for mandatory_col in task_mandatory_column_list:
                if task_meta[mandatory_col].isna().any() or (task_meta[mandatory_col].astype(str).str.strip() == "").any():
                    print("Checking for mandatory columns")
                    task_col_check.append(mandatory_col)

            #Updating the datatype for Integer Column 
            for col in task_integer_column:
                task_meta[col]=task_meta[col].replace('',np.nan).fillna('0').astype(int)

            #Updating the datatype for Boolean Column 
            for col in task_boolean_column:
                task_meta[col]=task_meta[col].replace('',np.nan).str.upper().map({'Y': True,'N':False}).fillna(False)

            task_stg = 1

            # Raising Validation Errors
            if task_platform_validation_cnt > 0 or \
            task_env_validation_cnt > 0 or \
            task_is_active_validation_cnt > 0 or \
            task_id_validation_cnt > 0 or \
            dag_id_validation_cnt > 0 or \
            source_type_validation_cnt > 0 or \
            target_type_validation_cnt > 0 or \
            len(task_col_check) > 0:
                raise ValueError(
                    f"TASK META VALIDATION FAILED:\n\n"
                    f"Platform name validation failed count: {task_platform_validation_cnt if task_platform_validation_cnt > 0 else 'None'}\n"
                    f"Environment name validation failed count: {task_env_validation_cnt if task_env_validation_cnt > 0 else 'None'}\n"
                    f"Is_active validation failed count: {task_is_active_validation_cnt if task_is_active_validation_cnt > 0 else 'None'}\n"
                    f"Task ID validation failed count: {task_id_validation_cnt if task_id_validation_cnt > 0 else 'None'}\n"
                    f"DAG ID validation failed count: {dag_id_validation_cnt if dag_id_validation_cnt > 0 else 'None'}\n"
                    f"Source type validation failed count: {source_type_validation_cnt if source_type_validation_cnt > 0 else 'None'}\n"
                    f"Target type validation failed count: {target_type_validation_cnt if target_type_validation_cnt > 0 else 'None'}\n"
                    f"Mandatory columns missing: {task_col_check if len(task_col_check) > 0 else 'None'}"
                )
            else:
                load_table(task_meta, task_table_id)
                print("Data Loaded Task staging Table")

        else:
            print("Task Meta Tab is Empty")
            task_stg = 0

        #------------------PROCESS_VALIDATION_COLUMNS---------------------
        if not process_meta.empty:
            # Converting empty string to null values
            process_meta= process_meta.infer_objects(copy=False).replace(r'^\s*$',np.nan,regex=True)
            
            #Remove whitespaces
            process_meta=process_meta.map(lambda x:x.strip() if isinstance(x,str) else x)

            total_process_count = process_meta.shape[0]
            # Updating the Variable
            dictionary = dict(zip(variable_list, dev_list))
            if "dev" in env:
                process_meta.infer_objects(copy=False).replace(dictionary,inplace=True,regex=True)
            elif "test" in env:
                process_meta.infer_objects(copy=False).replace(variable_list, test_list,  inplace=True,regex=True)
            elif "ple" in env:
                process_meta.infer_objects(copy=False).replace(variable_list, ple_list, inplace=True,regex=True)
            else:
                process_meta.infer_objects(copy=False).replace(variable_list, prod_list, inplace=True,regex=True)

            process_meta = process_meta.astype("string")
           
            # Process Name Validation
            process_name_validation_cnt = process_meta['process_name'].isna().sum() + process_meta.loc[
                process_meta['process_name'].str.strip().str.contains(r'\s', case=False), :].shape[0]

            # Frequency Validation
            process_meta["frequency"] = process_meta["frequency"].str.lower()
            frequency_validation_cnt = process_meta['frequency'].isna().sum() + (
                total_process_count - process_meta['frequency'].str.strip().str.lower().isin(frequency_list).sum())

            # Is Critical Validation
            # process_meta['is_critical'] = process_meta['is_critical'].replace('', "N")
            is_critical_validation_cnt = total_process_count - process_meta['is_critical'].fillna('n').str.upper().isin(['Y', 'N']).sum()

            # Platform Name Validation
            platform_name_validation_cnt = total_process_count - process_meta['platform_name'].fillna('na').str.lower().isin(platform_list).sum()

        
            # Process POC Validation (Email Format)
            dev_poc_validation_cnt = total_process_count - process_meta.loc[
                process_meta['dev_poc'].fillna('na').str.contains(r'[^@]+@[^@]+\.[^@]+', case=False), :].shape[0]

            # Scheduler Name Validation
            process_meta['scheduler_name']=process_meta['scheduler_name'].fillna('airflow')
            scheduler_name_validation_cnt = total_process_count - process_meta['scheduler_name'].str.lower().isin(scheduler_list).sum()

            # Logs Enabled Validation
            process_meta['logs_enabled']=process_meta['logs_enabled'].fillna('y')
            logs_enabled_validation_cnt = total_process_count - process_meta['logs_enabled'].str.upper().isin(['Y', 'N']).sum()

            # Logs Trace Id  Validation
            process_meta['logs_trace_id_enabled ']=process_meta['logs_trace_id_enabled '].fillna('y')
            logs_enabled_validation_cnt = total_process_count - process_meta['logs_trace_id_enabled '].str.upper().isin(['Y', 'N']).sum()

            # VSAD Validation
            vsad_validation_cnt = process_meta['vsad'].isna().sum()

            # Timezone Validation
            process_meta["timezone"] = process_meta["timezone"].fillna('EST')
            timezone_validation_cnt = total_process_count - process_meta['timezone'].isin(time_zone_list).sum()

            
            # Run Date Buffer Validation
            process_meta['run_date_buffer_interval'] = process_meta['run_date_buffer_interval'].replace('', "0")
            run_date_buffer_interval_validation_cnt = total_process_count - process_meta.loc[
                process_meta['run_date_buffer_interval'].fillna('0').str.contains(r'[0-9]', case=False), :].shape[0]

        
            # Duration Threshold Validation
            # process_meta['duration_threshold_low_value'] = process_meta['duration_threshold_low_value'].replace('', "20")
            process_meta['duration_threshold_high_value'] = process_meta['duration_threshold_high_value'].fillna("20")
            # duration_threshold_low_value_validation_cnt = total_process_count - process_meta.loc[
            #     process_meta['duration_threshold_low_value'].fillna('20').str.contains(r'[0-9]', case=False), :].shape[0]
            duration_threshold_high_value_validation_cnt = total_process_count - process_meta.loc[
                process_meta['duration_threshold_high_value'].fillna('20').str.contains(r'[0-9]', case=False), :].shape[0]

            alert_failure_validation_count_a = (total_process_count - process_meta['alert_failure_by'] .fillna(' ').str.lower().isin(valid_alerts +['']).sum())


        #Checking for mandatory columns
            process_col_check=[]
            for mandatory_col in process_mandatory_column_list:
                if(process_meta[mandatory_col].isna().any() or (process_meta[mandatory_col].astype(str).str.strip()=="").any()):
                    print("checking for mandatory columns")
                    process_col_check.append(mandatory_col)

            process_meta['schedule_interval']=process_meta['schedule_interval'].fillna('').astype(str)
            process_meta['frequency']=process_meta['frequency'].fillna('').astype(str)

            

            # -----------------------------------Cron Validation-------------------------------------------------------------

            for index,row in process_meta.iterrows():
                dly_cron_vld,wkly_cron_vld,mntly_cron_vld,hrly_cron_vld,chrly_cron_vld,uknwn_cron_vld=0,0,0,0,0,0
                schedule=row['schedule_interval']
                if row['frequency']== 'daily':
                # Valid daily schedule should match: '0 HH  ' or similar (e.g., "30 14  ")
                    daily_pattern = r'^\d{1,2} \d{1,2} \* \* \*$'

                    if not re.match(daily_pattern, schedule):
                        dly_cron_vld+=1
                        raise ValueError(schedule + " : Invalid cron for daily frequency. Expected format: 'MINUTE HOUR  *'.",{schedule})   
                
                elif row['frequency'] == 'weekly':

                # Valid weekly schedule should match: '0 HH  DAY' (e.g., "0 0  MON")
                    weekly_pattern = weekly_pattern = r'^\d{1,2} \d{1,2} \* \* (([0-6](,[0-6])*)|((SUN|MON|TUE|WED|THU|FRI|SAT)(,(SUN|MON|TUE|WED|THU|FRI|SAT))*))$'
                    if not re.match(weekly_pattern, schedule):
                        wkly_cron_vld+=1
                        raise ValueError(schedule + " :Invalid cron for weekly frequency. Expected format: 'MINUTE HOUR  DAY'.")

            
                elif row['frequency'] == 'hourly':

                # Valid hourly schedule should match: '0  ' or 'MINUTE  '
                    hourly_pattern = r'^\d{1,2} \* \* \* \*$'
                    if not re.match(hourly_pattern, schedule):
                        hrly_cron_vld+=1
                        raise ValueError(schedule + " :Invalid cron for hourly frequency. Expected format: 'MINUTE  '.")
                
                elif row['frequency'] == 'monthly':

                # Valid monthly schedule should match: '0 HH DD ' (e.g., "0 0 1 " for the 1st of the month)
                    monthly_pattern = r'^\d{1,2} \d{1,2} ((\d{1,2})(,\d{1,2})*) \* \*$'
                    if not re.match(monthly_pattern, schedule):
                        mntly_cron_vld+=1
                        raise ValueError(schedule + " :Invalid cron for monthly frequency. Expected format: 'MINUTE HOUR DAY '.")

                elif row['frequency'] == 'hourly_custom':

                # Valid custom hourly schedule should match: '*/N  ' or 'MINUTE HOUR1,HOUR2,...  *'
                    custom_hourly_pattern = r'^\d{1,2} (?:\*/\d{1,2}|\d{1,2}(?:,\d{1,2})*) \* \* \*$'
                    if not re.match(custom_hourly_pattern, schedule):
                        chrly_cron_vld+=1
                        raise ValueError(schedule + " :Invalid cron for custom hourly frequency. Expected format: 'MINUTE HOUR1,HOUR2,...  ' or '/N  '.")
                else:
                    uknwn_cron_vld+=1
                    raise ValueError( f"Unknown frequency: {row['frequency' ]}")
                
                 #Updating the datatype for Integer Column
            for col in process_integer_column:
                process_meta[col]=process_meta[col].replace('',np.nan).fillna('0').astype(int)

                #Updating the datatype for Boolean Column
            for col in process_boolean_column:
                process_meta[col]=process_meta[col].replace('',np.nan).str.upper().map({'Y': True,'N':False}).fillna(False)

            process_stg = 1

            #Raise validation errors

            if process_name_validation_cnt > 0 or\
            frequency_validation_cnt > 0 or\
            is_critical_validation_cnt > 0 or \
            platform_name_validation_cnt > 0 or\
            dev_poc_validation_cnt > 0 or\
            scheduler_name_validation_cnt > 0 or\
            logs_enabled_validation_cnt > 0 or\
            vsad_validation_cnt > 0 or\
            timezone_validation_cnt > 0 or\
            run_date_buffer_interval_validation_cnt > 0 or\
            duration_threshold_high_value_validation_cnt > 0 or\
            alert_failure_validation_count_a > 0 or\
            len(process_col_check) > 0:
                raise ValueError(
                    f"Validation failed with the following counts:\n"
                    f"Invalid Process Names: {process_name_validation_cnt}\n"
                    f"Invalid Frequency Values: {frequency_validation_cnt}\n"
                    f"Invalid Is Critical Values: {is_critical_validation_cnt}\n"
                    f"Invalid Platform Names: {platform_name_validation_cnt}\n"
                    f"Invalid Dev POC Emails: {dev_poc_validation_cnt}\n"
                    f"Invalid Scheduler Names: {scheduler_name_validation_cnt}\n"
                    f"Invalid Logs Enabled Values: {logs_enabled_validation_cnt}\n"
                    f"Missing VSAD Values: {vsad_validation_cnt}\n"
                    f"Invalid Timezone Values: {timezone_validation_cnt}\n"
                    f"Invalid Run Date Buffer Interval Values: {run_date_buffer_interval_validation_cnt}\n"
                    f"Invalid Duration Threshold High Values: {duration_threshold_high_value_validation_cnt}\n"
                    f"Invalid Alert Failure By Values: {alert_failure_validation_count_a}\n"
                    f"Mandatory columns missing:{process_col_check}\n"
                    )
            
        
            else:
                load_table(process_meta,process_table_id)
                print("Data loaded to Process staging Table")
        else:
            print("Process Tab is Empty")
            process_stg = 0

       #-----------------------Finops_VALIDATION_COLUMNS----------------------------------------------------------------
        if not finops_meta.empty:
            # Converting empty string to null values
            finops_meta=finops_meta.infer_objects(copy=False).replace(r'^\s*$',np.nan,regex=True)

            #Remove whitespaces
            finops_meta=finops_meta.map(lambda x:x.strip() if isinstance(x,str) else x)
            total_finops_count = finops_meta.shape[0]
            job_id_validation_cnt=instance_name_validation_cnt=lob_validation_cnt=label_is_active_validation_cnt=label_col_check=0

            #Updating the Variables
            if "dev" in env:
                finops_meta.infer_objects(copy=False).replace(variable_list, dev_list, inplace=True,regex=True)
                print(finops_meta)
            elif "test" in env:
                finops_meta.infer_objects(copy=False).replace(variable_list, test_list, inplace=True,regex=True)
                print(finops_meta)
            elif "ple" in env:
                finops_meta.infer_objects(copy=False).replace(variable_list, ple_list, inplace=True,regex=True)
                print(finops_meta)
            else:
                finops_meta.infer_objects(copy=False).replace(variable_list, prod_list, inplace=True,regex=True)
                print(finops_meta)
            
            finops_meta = finops_meta.astype("string")
            

            # Job ID Validation (Mandatory and Non-Empty)
            job_id_validation_cnt = finops_meta['job_id'].isna().sum() + \
                                    finops_meta['job_id'].astype(str).str.strip().eq('').sum()

            # Instance Name Validation (Mandatory and Non-Empty)
            instance_name_validation_cnt = finops_meta['instance_name'].isna().sum() + \
                                        finops_meta['instance_name'].astype(str).str.strip().eq('').sum()


            # LOB Validation (Mandatory and Non-Empty)
            lob_validation_cnt = finops_meta['lob'].isna().sum() + \
                                finops_meta['lob'].astype(str).str.strip().eq('').sum()

            # Is Active Validation
            finops_meta['is_active']=finops_meta['is_active'].fillna('y')
            label_is_active_validation_cnt = total_finops_count - finops_meta['is_active'].fillna('y').str.upper().isin(['Y', 'N']).sum()

            # Checking the null values in Mandatory Columns
            finops_col_check = []
            for mandatory_col in finops_mandatory_column_list:
                if finops_meta[mandatory_col].isna().any() or (finops_meta[mandatory_col].astype(str).str.strip() == "").any():
                    print("Checking for mandatory columns")
                    finops_col_check.append(mandatory_col)
            
            #Updating the datatype for Integer Column
            for col in finops_integer_column:
                finops_integer_column[col]=finops_meta[col].replace('',np.nan).fillna('0').astype(int)

            #Updating the datatype for Boolean Column           
            for col in finops_boolean_column:
                finops_meta[col]=finops_meta[col].replace('',np.nan).str.upper().map({'Y': True,'N':False}).fillna(False)

            finops_stg  = 1

            # Raising Validation Errors
            if job_id_validation_cnt > 0 or \
            instance_name_validation_cnt > 0 or \
            lob_validation_cnt > 0 or \
            label_is_active_validation_cnt > 0 or \
            len(finops_col_check) > 0:
                raise ValueError(
                    f"LABEL META VALIDATION FAILED:\n\n"
                    f"Job ID validation failed count: {job_id_validation_cnt if job_id_validation_cnt > 0 else 'None'}\n"
                    f"Instance Name validation failed count: {instance_name_validation_cnt if instance_name_validation_cnt > 0 else 'None'}\n"
                    f"LOB validation failed count: {lob_validation_cnt if lob_validation_cnt > 0 else 'None'}\n"
                    f"Is_active validation failed count: {label_is_active_validation_cnt if label_is_active_validation_cnt > 0 else 'None'}\n"
                    f"Mandatory columns missing: {finops_col_check if len(finops_col_check) > 0 else 'None'}"
                )
            else:
                load_table(finops_meta, finops_table_id)
                print("Data Loaded Label staging Table")

        else:
            print("Finops Meta Tab is Empty")
            finops_stg  = 0

        # Additional or Missing Columns in metadata
        
        if missing_column_cnt > 0 or additional_column_cnt > 0:
                    raise ValueError("COMMON VALIDATION:\n\n Missing columns in metadata :\n\tFiles:{0} \n\tTables:{1} \n\tProcess:{2} \n\tTask:{3} \n\tTask:{4} \nAdditional columns in metadata :\n\tFiles:{5} \n\tTables:{6} \n\tProcess:{7} \n\tTask:{8} \n\tTask:{9} ".format(missed_fle_col,missed_tbl_col,missed_prcs_col,missed_tsk_col,missed_fnps_col,add_fle_col,add_tbl_col,add_prcs_col,add_tsk_col,add_fnps_col))

        # --------------------Vaiables Still Present in Metadata------------------------------

        t_var=[]
        f_var=[]
        p_var=[]
        tsk_var=[]
        fnps_var=[]
        for col in tables_column_list:
            if pd.notna(tables_meta[col]).all() and tables_meta[col].astype(str).str.contains('<').any():
                t_var.append(col)
        for col in files_column_list:
            if pd.notna(files_meta[col]).all() and files_meta[col].astype(str).str.contains('<').any():
                f_var.append(col)
        for col in process_column_list:
            if pd.notna(process_meta[col]).all() and process_meta[col].astype(str).str.contains('<').any():
                p_var.append(col)
        for col in task_column_list:
            if pd.notna(task_meta[col]).all() and task_meta[col].astype(str).str.contains('<').any():
                tsk_var.append(col)
        for col in finops_column_list:
            if pd.notna(finops_meta[col]).all() and finops_meta[col].astype(str).str.contains('<').any():
                fnps_var.append(col)

        
        file_sql = """CALL `aid_epdo_prd_tbls.datax_upsert_into_file_meta` ()"""
        table_sql = """CALL `aid_epdo_prd_tbls.datax_upsert_into_table_meta` ()"""
        process_sql = """CALL `aid_epdo_prd_tbls.datax_upsert_into_process_meta` ()"""
        task_sql = """CALL `aid_epdo_prd_tbls.datax_upsert_into_task_meta` ()"""
        finops_sql = """CALL `aid_epdo_prd_tbls.datax_upsert_into_label_meta` ()"""
        lineage_sql = """CALL `aid_epdo_prd_tbls.datax_upsert_into_relation_meta` ()"""

        if missing_column_cnt > 0 or additional_column_cnt > 0  or  len(t_var) > 0  or  len(f_var) > 0  or  len(p_var) > 0  or  len(t_var) > 0 or len(fnps_var) > 0:
                    raise ValueError("COMMON VALIDATION:\n\n Missing columns in metadata :\n\tFiles:{0} \n\tTables:{1} \n\tProcess:{2} \n\tTask:{3} \n\tTask:{4} \nAdditional columns in metadata :\n\tFiles:{5} \n\tTables:{6} \n\tProcess:{7} \n\tTask:{8} \n\tTask:{9} \nVariables still present in metadata: \n\tFiles:{10} \n\tTables:{11} \n\tProcess:{12} \n\tTask:{13} \n\tFinops:{14}".format(missed_fle_col,missed_tbl_col,missed_prcs_col,missed_tsk_col,missed_fnps_col,add_fle_col,add_tbl_col,add_prcs_col,add_tsk_col,add_fnps_col,f_var,t_var,p_var,tsk_var,fnps_var))

# --------------------Loading Data from Stage to Meta tables------------------------------

        print("Starting to load staging data to process metadata.")
        if process_stg == 1:
            query_job=client.query(process_sql)
            query_job.result()
            print("Load to process base table completed.")
            process_count += 1
        else:
            print("Procedure to update Process_meta will not run!")
            process_count += 1
        print("-----------------------------------------------------")
        print("-----------------------------------------------------")

        print("-----------------------------------------------------")
        print("Starting to load staging data to file metadata.")
        if file_stg == 1:
            client.query(file_sql)
            print("Load to files base table completed.")
            process_count += 1
        else:
            print("Procedure to update file_meta will not run!")
            process_count += 1
        print("-----------------------------------------------------")
        print("-----------------------------------------------------")

        print("Starting to load staging data to table metadata.")
        if table_stg == 1:
            client.query(table_sql)
            print("Load to Tables base table completed.")
            process_count += 1
        else:
            print("Procedure to update Table_meta will not run!")
            process_count += 1
        print("-----------------------------------------------------")
        print("-----------------------------------------------------")

        if finops_stg==1:
            print("Starting to load staging data to finops metadata.")
            client.query(finops_sql)
            print("Load to finops base table completed.")
            process_count += 1
        else:
            print("Procedure to update Finops_meta will not run!")
        print("-----------------------------------------------------")
        print("-----------------------------------------------------")

        if task_stg==1:
            print("Starting to load staging data to task metadata.")
            client.query(task_sql)
            print("Load to task base table completed.")
            process_count += 1
        else:
            print("Procedure to update Task_meta will not run!")
        print("-----------------------------------------------------")
        print("-----------------------------------------------------")

        print("Starting to load staging data to Lineage metadata.")
        client.query(lineage_sql)
        print("Load to Lineage base table completed.")
        process_count += 1
        print("-----------------------------------------------------")
        print("-----------------------------------------------------")

         # Send Email for success
        process_count=4
        if process_count < 4:
            raise ValueError(
                "Someting in Pipeline Went Wrong. Some base table hasn't been loaded."
            )
        else:
            msg = EmailMessage()
            success = "FILE NAME: " + file_name + " Load Successful!"
            msg.set_content(success)
            msg["Subject"] = "(" + env + ") Metadata Pipeline - Success!"
            msg["From"] = "do-not-reply@verizon.com"
            msg["To"] = "ayush.singh1@verizon.com"
            smtpObj = smtplib.SMTP("vzsmtp.verizon.com", 25)
            smtpObj.send_message(msg)
            print("Successfully sent email for Success!")
            smtpObj.quit()
    except Exception as e:
        # exc_type,exc_obj,exc_tb=sys.exc_info
        # line_number=exc_tb.tb.lineno
        error = "Error Occurred in FILE: " + file_name + "\n" + "ERROR: " + str(e)
        print(error)
        msg = EmailMessage()
        msg.set_content(error)
        msg["Subject"] = "(" + env + ") Error occured in Metadata Pipeline!"
        msg["From"] = "do-not-reply@verizon.com"
        msg["To"] = "ayush.singh1@verizon.com"

        try:
            smtpObj = smtplib.SMTP("vzsmtp.verizon.com", 25)
            smtpObj.send_message(msg)
            print("Successfully sent email for Error!")
            smtpObj.quit()
        except SMTPException:
            print("Error: unable to send email"
