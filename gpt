@method_decorator(csrf_exempt, name='dispatch')    
class DropdownSubdmnCustomMtd(CredentialsandConnectivity):
    
    def get(self, request, *args, **kwargs):
        query = f"""SELECT distinct(data_sub_dmn) FROM  {self.dq_project_id}.dga_dq_tbls.dqaas_profile_mtd"""
        print(query) 
        self.dq_bigquery_client()
        query_job = self.client.query(query)
        results = query_job.result()
        distinct_db = [dict(row) for row in results]
        return JsonResponse({'distinct_db':distinct_db})
    def post(self, request,*args, **kwargs):  
        if request.content_type == 'application/json':
            data = json.loads(request.body.decode('utf-8'))
            print(data)
        else:
            data = request.POST
            print(data)
        data_sub_dmn = data.get("sub_domain")
        query = f"""select *  from  {self.dq_project_id}.dga_dq_tbls.dqaas_profile_mtd where data_sub_dmn = '{data_sub_dmn}'"""
        print(query)
        self.dq_bigquery_client()
        query_job = self.client.query(query)
        results = query_job.result()
        
        rows_list = []
        for row in results:
            print("results", row)
            row_dict = dict(row)
            rows_list.append(row_dict)
        return JsonResponse(rows_list, safe=False) 

@method_decorator(csrf_exempt, name='dispatch')    
class SaveCustomMtd(CredentialsandConnectivity): 
    def post(self, request,*args, **kwargs):  
        if request.content_type == 'application/json':
            data = json.loads(request.body.decode('utf-8'))
            print("post data",data)
        else:
            data = request.POST
            print("post data",data)   
        for key in data:
            if data[key] == None:
                data[key] ='Null'
        prfl_id = data.get("prfl_id")
        prfl_type = data.get("prfl_type")
        data_lob = data.get("data_lob")
        data_bus_elem = data.get("data_bus_elem")
        data_dmn = data.get("data_dmn")
        data_sub_dmn = data.get("data_sub_dmn")
        data_src = data.get("data_src")
        dq_pillar = data.get("dq_pillar")
        feature_name = data.get("feature_name")
        db_name = data.get("db_name")
        src_tbl = data.get("src_tbl")
        src_col = data.get("src_col")
        incr_dt_col = data.get("incr_dt_col")
        incr_dt_cond = data.get("incr_dt_cond")
        meas_name = data.get("meas_name")
        meas_rule_sql = data.get("meas_rule_sql")
        micro_segment_columns = data.get("micro_segment_columns")
        aggregated_column = data.get("aggregated_column")
        comparison_type = data.get("comparison_type")
        micro_segment_threshold = data.get("micro_segment_threshold")
        max_threshold = data.get("max_threshold")
        min_threshold = data.get("min_threshold")
        is_hourly_flg = data.get("is_hourly_flg")
        run_hour = data.get("run_hour")
        add_param_clause = data.get("add_param_clause")
        pass_param_1 = data.get("pass_param_1")
        pass_param_2 = data.get("pass_param_2")
        pass_param_3 = data.get("pass_param_3")
        pass_param_4 = data.get("pass_param_4")
        pass_param_5 = data.get("pass_param_5")
        prfl_schd_ts = data.get("prfl_schd_ts")
        is_active_flg = data.get("is_active_flg")
        is_critical_flg = data.get("is_critical_flg")
        is_opsgenie_flg = data.get("is_opsgenie_flg")
        opsgenie_api_key = data.get("opsgenie_api_key")
        record_ind = data.get("record_ind")
        hourly_col = data.get("hourly_col")
        jira_assignee = data.get("jira_assignee")

        query = f"""update {self.dq_project_id}.dga_dq_tbls.dqaas_profile_mtd set data_sub_dmn = '{data_sub_dmn}',  data_lob='{data_lob}',data_bus_elem='{data_bus_elem}',data_dmn='{data_dmn}',dq_pillar ='{dq_pillar}',feature_name ='{feature_name}'  ,data_src='{data_src}',src_col='{src_col}',incr_dt_col='{incr_dt_col}',incr_dt_cond={incr_dt_cond},meas_name='{meas_name}',meas_rule_sql='''{meas_rule_sql}''',micro_segment_columns='{micro_segment_columns}',aggregated_column='{aggregated_column}',is_active_flg='{is_active_flg}',comparison_type='{comparison_type}',micro_segment_threshold={micro_segment_threshold},max_threshold={max_threshold},min_threshold={min_threshold},is_hourly_flg='{is_hourly_flg}',run_hour={run_hour},add_param_clause='{add_param_clause}',pass_param_1={pass_param_1},pass_param_2={pass_param_2},pass_param_3={pass_param_3},pass_param_4={pass_param_4},pass_param_5={pass_param_5},update_made_ts=current_timestamp(),is_opsgenie_flg='{is_opsgenie_flg}',opsgenie_api_key='{opsgenie_api_key}',prfl_schd_ts='{prfl_schd_ts}' where prfl_id = {prfl_id} and db_name = '{db_name}' and src_tbl='{src_tbl}' """
        print(query)
        self.dq_bigquery_client()
        query_job = self.client.query(query)
        return JsonResponse({'status':'success','message':'Data Updated succesfully'}, safe=False)




@method_decorator(csrf_exempt, name='dispatch')    
class ExecuteHistorySQL(CredentialsandConnectivity): 
# Handling file upload and processing SQL queries

    def dispatch(self, request, *args, **kwargs):
        # logger 
        if not hasattr(self, 'logger'):
            self.logger = logging.getLogger(__name__)
            logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

        return super().dispatch(request, *args, **kwargs)

    def post(self, request, *args, **kwargs):
        try:
            if not hasattr(self, 'logger'):
                self.logger = logging.getLogger(__name__)
            if 'file' not in request.FILES:
                self.logger.info("file uploaded.")
                return HttpResponse("file uploaded.", status=400)

            file = request.FILES['file']
            self.logger.info(f"Received file: {file.name}")

            queries = self.read_queries_from_uploaded_file(file)
            if not queries:
                self.logger.error("No valid queries found.")
                return HttpResponse("No valid queries found.", status=400)

            self.execute_queries(queries)
            return HttpResponse("File uploaded and queries executed successfully.", status=200)

        except Exception as e:
            self.logger.error(f"Error processing request: {e}")
            return HttpResponse("Error processing request.", status=500)

    # Execute queries in BQ
    def execute_queries(self, queries):
        self.dq_bigquery_client()
        for query in queries:
            try:
                query_job = self.client.query(query)
                query_job.result()
                self.logger.info("Query executed successfully.")
            except Exception as e:
                self.logger.error(f"Query execution error: {e}")

    # Read SQL queries from text file
    def read_queries_from_uploaded_file(self, file):
        try:
            content = file.read().decode('utf-8')
            queries = [q.strip() for q in re.split(r';\s*\n', content) if q.strip()]
            self.logger.info(f"Read {len(queries)} queries from file.")
            return queries
        except Exception as e:
            self.logger.error(f"Error reading file: {e}")
            return []
SELECT * 
FROM `vz-it-np-izcv-dev-idmcdo-0.dga_dq_tbls.dqaas_profile_rpt`
WHERE rpt_seq_num = 1000005
ORDER BY prfl_run_ts DESC
LIMIT 10;

-----------------------------------------------
import os
import re
from django.shortcuts import render
from django.http import JsonResponse
from django.views import View
from django.conf import settings
from google.cloud import bigquery
import base64
import time
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
from rest_framework.views import APIView
import subprocess
from django.http import HttpResponse
from django.contrib import messages
from configparser import ConfigParser
from requests.exceptions import HTTPError
from utils.mtd_logger import Logger
from datetime import datetime
import requests
import json
import google.auth
from pathlib import Path
from sqlalchemy import create_engine
import pandas as pd
from sqlalchemy import text
import logging

# __path=os.path.join(settings.BASE_DIR, 'self_serve', 'config','config_self_serve.ini')
# config =ConfigParser()
# config.read(filenames=__path)
# logger_file_path = config.get('dir','logs') + 'meta_dt_upload_'+datetime.now().strftime('%Y-%m-%d-%H-%M-%S')+'.log'
# logger = Logger(name='MTDT',path=logger_file_path).log



class CredentialsandConnectivity(APIView):
    def __init__(self):
        __path=os.path.join(settings.BASE_DIR, 'self_serve', 'config','config_self_serve.ini')
        self.config =ConfigParser()
        self.config.read(filenames=__path)
        logger_file_path = self.config.get('dir','logs') + 'meta_dt_upload_'+datetime.now().strftime('%Y-%m-%d-%H-%M-%S')+'.log'
        logger = Logger(name='MTDT',path=logger_file_path).log

        #getting dataset credentials config.json
        self.dq_sa_json_file = self.config.get('dir','dq_sa_path')

        # Set Credentials
        http_proxy = self.config.get('proxy','http_proxy')
        https_proxy = self.config.get('proxy','https_proxy')
        no_proxy = self.config.get('proxy','no_proxy')

        self.proxies = {
                "http_proxy": http_proxy,
                "https_proxy": https_proxy,
                "no_proxy": no_proxy
    }

        # os.environ['http_proxy'] = 'http://proxy.ebiz.verizon.com:80/'
        # os.environ['https_proxy'] = 'http://proxy.ebiz.verizon.com:80/'
        # os.environ['no_proxy'] = 'http://proxy.ebiz.verizon.com:80/'
        #Setting GCP Credentials
        self.dq_client_id = self.config.get('gcp','client_id')
        self.dq_client_secret = self.config.get('gcp','client_secret')
        self.dq_project_id = self.config.get('gcp','project_id')
        self.dq_dataset = self.config.get('gcp','db_name')
        self.token_url = self.config.get('gcp','token_url')

        self.python_path = self.config.get('dir','python_path')

        self.auto_header_cols = self.config.get('cols_chk','auto_header_chk')
        self.rule_header_cols = self.config.get('cols_chk','rule_header_chk')
        #TeraData Connection Details
        self.uid = self.config.get('td','uid')
        self.pwd = self.config.get('td','pwd')
        self.hostname = self.config.get('td','hostname')
        self.td_dbname = self.config.get('td','td_dbname')

    def isTokenExpired(self,path):
        try:
            print(f"The Token File Path is {path}")
            if(os.path.exists(path)):
                print("Token File Available")
                with open(path,'r') as f:
                    old_access_token = json.load(f)['access_token'].split('.')[1]
                    old_access_token += '=' * (-len(old_access_token) % 4)
                    old_token_json_decoded = json.loads(base64.b64decode(old_access_token).decode('utf8').replace("'",'"'))
                    auth_time = old_token_json_decoded['auth_time']
                    expires_in = old_token_json_decoded['expires_in']
                    curr_epoch_time = int(time.time())
                    if curr_epoch_time - auth_time < expires_in - 120:
                        print("Access Token is Valid")
                        return False
                    else:
                        print("Access Token is Invalid")
            return True
        except Exception as e:
            raise e
        
    def exchange_and_save_oidc_token_for_jwt(self,client_id,client_secret,token_path) -> None:
        print(f'Retrieving JWT from OIDC provider...')
        payload = {"grant_type": "client_credentials", "client_id": client_id,"client_secret": client_secret, "scope": "read"}
        try:
            response = requests.post(url=self.token_url, params=payload,proxies=self.proxies)
            response.raise_for_status()
            token = response.json()
            print(f"Saving token...")
            # Serializing json
            oidc_token_file_name = token_path
            if os.path.isfile(oidc_token_file_name):
                os.remove(oidc_token_file_name)
                # time.sleep(15)

            print(f"path: {oidc_token_file_name}")
            with open(oidc_token_file_name, "w") as f:
                json.dump(token, f)
        except HTTPError as e:
            raise e

    def dq_bigquery_client(self):
        token_path = os.path.join(self.config.get('dir','config_dir'),self.config.get('gcp','dq_sa_token_json'))
        if self.isTokenExpired(token_path):
            self.exchange_and_save_oidc_token_for_jwt(self.dq_client_id,self.dq_client_secret,token_path)
        print(f"Setting environment variable...")
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = self.dq_sa_json_file
        os.environ["GOOGLE_CLOUD_PROJECT"] = self.dq_project_id
        self.credentials, _ = google.auth.default()
        print(f"project_id={self.dq_project_id}, credentials={self.credentials}")
        self.client = bigquery.Client(credentials=self.credentials, project=self.dq_project_id)
        return self.client, self.credentials
    def td_client(self):
        try:
            self.td_engine = create_engine(f"teradatasql://{self.uid}:{self.pwd}@{self.hostname}/{self.td_dbname}?encryptdata=true")
            self.td_conn = self.td_engine.connect()
            return self.td_engine, self.td_conn
        except Exception as e:
            print(f"Teradata Connection Error. Error:{e}")
            

# Create your views here.

def selfserve_mle_form(request):
    return render(request, "selfservemle_form.html")




@method_decorator(csrf_exempt, name='dispatch')
class MLESelfServe(CredentialsandConnectivity):
    def post(self, request, *args, **kwargs):
        try:
            print("File submission started")
            file_info ={key:value for key, value in request.FILES.items()}

            for csvname, csv_file in file_info.items():
                print("CSV Name ::",csvname)
                print("CSV File ::", csv_file)
                print(type(csv_file))
                #Validation Starts
                if not csv_file.name.endswith('.csv'):
                    return JsonResponse({'status':'error', 'message':'Invalid File Format'}, safe=False)
                
                lines = csv_file.read().decode('utf-8', 'ignore').split('\n')
                header_received = lines[0].lower()

                if not (header_received.strip() == self.auto_header_cols.strip() or header_received.strip() == self.rule_header_cols.strip()):

                    return JsonResponse({'status':'error', 'message':'Invalid Column Format'}, safe=False)
                #Validation Ends

                #upload_to_gcs_from_ui(csv_file, csv_file.name)
                input_path = os.path.join(settings.BASE_DIR, 'MLE', 'input_files', 'add_metadata')
                #Create Directory if not exists
                Path(input_path).mkdir(parents=True, exist_ok=True)
                input_file = os.path.join(input_path,csv_file.name)
                with open (input_file,'w+') as f:
                    if "file1" in csvname:
                        print("in auto")
                        final_data = self.auto_header_cols.upper()+'\n'+'\n'.join(x for x in lines[1:])
                        f.write(final_data)
                    else:
                        print("in rule")
                        final_data = self.rule_header_cols.upper()+'\n'+'\n'.join(x for x in lines[1:])
                        f.write(final_data)

            script_path = os.path.join(settings.BASE_DIR, 'self_serve', 'self_serve.py')
            print("Script Path ::",script_path)
            result = subprocess.run([self.python_path, script_path], capture_output = True)

            if result.returncode == 0:
                    error_lines = ''
                    response_data = {'status':'success','message':'Data inserted succesfully'}
                    stderr=result.stderr.split(b'\n')
                    for line in stderr:
                        if line.find(b'ERROR') > 0:
                            error_lines = error_lines + str(line).strip()[35:]
                            response_data = {'status':'error','message':error_lines}
            else:
                error_message = result.stderr.strip() if result.stderr else 'Something went wrong'
                response_data = {'status':'error', 'message':error_message}
        except Exception as e:
            response_data = {'status':'error', 'message':str(e)}

        return JsonResponse(response_data, safe=False)
        
        
        
def auto_prfl_template_download(request):
    file_path = os.path.join(settings.BASE_DIR, 'templates','dqaas_auto_prfl_mtd_tf.csv')
    with open(file_path, 'rb') as file:
        response = HttpResponse(file.read(), content_type = 'text/csv')
        response['Content-Disposition'] = 'attachment; filename=dqaas_auto_prfl_mtd_tf.csv'
    return response

def rule_prfl_template_download(request):
    file_path = os.path.join(settings.BASE_DIR, 'templates','dqaas_rule_prfl_mtd_MI.csv')
    with open(file_path, 'rb') as file:
        response = HttpResponse(file.read(), content_type = 'text/csv')
        response['Content-Disposition'] = 'attachment; filename=dqaas_rule_prfl_mtd_MI.csv'
    return response
    

    
@method_decorator(csrf_exempt, name='dispatch')    
class ViewEditAutoMLEMtd(CredentialsandConnectivity):
    def post(self, request,*args, **kwargs):  
        if request.content_type == 'application/json':
            data = json.loads(request.body.decode('utf-8'))
            print(data)
        else:
            data = request.POST
            print(data)
        DB_NAME = data.get("db_name")
        TBL_NAME = data.get("table_name")
        query = f"""select *  from  {self.dq_project_id}.dga_dq_tbls.dqaas_auto_prfl_mtd where db_name = '{DB_NAME}' and src_tbl='{TBL_NAME}'"""
        print(query)
        self.dq_bigquery_client()
        query_job = self.client.query(query)
        results = query_job.result()
        rows_list = []
        for row in results:
            row_dict = dict(row)
        rows_list.append(row_dict)
        return JsonResponse(rows_list, safe=False)
        
@method_decorator(csrf_exempt, name='dispatch')    
class ViewEditRuleMLEMtd(CredentialsandConnectivity):
    def post(self, request,*args, **kwargs):  
        if request.content_type == 'application/json':
            data = json.loads(request.body.decode('utf-8'))
            print(data)
        else:
            data = request.POST
            print(data)
        DB_NAME = data.get("db_name")
        TBL_NAME = data.get("table_name")
        query = f"""select *  from  {self.dq_project_id}.dga_dq_tbls.dqaas_rule_prfl_mtd where db_name = '{DB_NAME}' and src_tbl='{TBL_NAME}'"""
        print(query)
        self.dq_bigquery_client()
        query_job = self.client.query(query)
        results = query_job.result()
        rows_list = []
        for row in results:
            row_dict = dict(row)
        rows_list.append(row_dict)
        return JsonResponse(rows_list, safe=False)
        





@method_decorator(csrf_exempt, name='dispatch')    
class DropdownDbAutoMLEMtd(CredentialsandConnectivity):
    
    def get(self, request, *args, **kwargs):
        query = f"""SELECT distinct(db_name) FROM  {self.dq_project_id}.dga_dq_tbls.dqaas_auto_prfl_mtd"""
        print(query) 
        self.dq_bigquery_client()
        query_job = self.client.query(query)
        results = query_job.result()
        distinct_db = [dict(row) for row in results]
        return JsonResponse({'distinct_db':distinct_db})
    def post(self, request,*args, **kwargs):  
        if request.content_type == 'application/json':
            data = json.loads(request.body.decode('utf-8'))
            print(data)
        else:
            data = request.POST
            print(data)
        self.dq_bigquery_client()
        DB_NAME = data.get("db_name")
        query = f"""SELECT DISTINCT src_tbl FROM  {self.dq_project_id}.dga_dq_tbls.dqaas_auto_prfl_mtd where db_name ='{DB_NAME}'"""
        print(query) 
        query_job = self.client.query(query)
        results = query_job.result()
        distinct_tbl = [row.src_tbl for row in results]
        return JsonResponse({'distinct_tbl':distinct_tbl})   
        
@method_decorator(csrf_exempt, name='dispatch')    
class DropdownDbRuleMLEMtd(CredentialsandConnectivity):
    
    def get(self, request, *args, **kwargs):
        query = f"""SELECT distinct(db_name) FROM  {self.dq_project_id}.dga_dq_tbls.dqaas_rule_prfl_mtd"""
        print(query) 
        self.dq_bigquery_client()
        query_job = self.client.query(query)
        results = query_job.result()
        distinct_db = [dict(row) for row in results]
        return JsonResponse({'distinct_db':distinct_db})
    def post(self, request,*args, **kwargs):  
        if request.content_type == 'application/json':
            data = json.loads(request.body.decode('utf-8'))
            print(data)
        else:
            data = request.POST
            print(data)
        self.dq_bigquery_client()
        DB_NAME = data.get("db_name")
        query = f"""SELECT DISTINCT src_tbl FROM  {self.dq_project_id}.dga_dq_tbls.dqaas_rule_prfl_mtd where db_name ='{DB_NAME}'"""
        print(query) 
        query_job = self.client.query(query)
        results = query_job.result()
        distinct_tbl = [row.src_tbl for row in results]
        return JsonResponse({'distinct_tbl':distinct_tbl}) 

@method_decorator(csrf_exempt, name='dispatch')    
class SaveRuleMLEMtd(CredentialsandConnectivity): 
    def post(self, request,*args, **kwargs):  
        if request.content_type == 'application/json':
            data = json.loads(request.body.decode('utf-8'))
            print(data)
        else:
            data = request.POST
            print(data)   
         
        rule_id = data.get("rule_id")
        data_lob = data.get("data_lob")
        data_bus_elem = data.get("data_bus_elem")
        data_dmn = data.get("data_dmn")
        data_sub_dmn = data.get("data_sub_dmn")
        data_src = data.get("data_src")
        db_name = data.get("db_name")
        src_tbl = data.get("src_tbl")
        src_col = data.get("src_col")
        dq_pillar = data.get("dq_pillar")
        meas_name = data.get("meas_name")
        meas_rule_desc = data.get("meas_rule_desc")
        meas_rule_sql = str(data.get("meas_rule_sql")).replace("'",'"')
        is_active_flg = data.get("is_active_flg")
        rule_prfl_schd_ts = data.get("rule_prfl_schd_ts")
        is_critical_flg = data.get("is_critical_flg")
        mail_sub = data.get("mail_sub")
        rule_min_thrsd = data.get("rule_min_thrsd")
        rule_max_thrsd = data.get("rule_max_thrsd")
        email_distro=data.get("email_distro")
        invalid_rec_flg = data.get("invalid_rec_flg")
        invalid_rec_sql = str(data.get("invalid_rec_sql")).replace("'",'"')
        is_opsgenie_flg = data.get("is_opsgenie_flg")
        opsgenie_api_key = data.get("opsgenie_api_key")
        label_tags = data.get("label_tags")
        query = f"""update {self.dq_project_id}.dga_dq_tbls.dqaas_rule_prfl_mtd set data_lob='{data_lob}',data_bus_elem='{data_bus_elem}',data_dmn='{data_dmn}',data_sub_dmn='{data_sub_dmn}',data_src='{data_src}',dq_pillar='{dq_pillar}',meas_rule_desc='{meas_rule_desc}',meas_rule_sql='{meas_rule_sql}',is_active_flg='{is_active_flg}',rule_prfl_schd_ts='{rule_prfl_schd_ts}',is_critical_flg='{is_critical_flg}',mail_sub='{mail_sub}',rule_min_thrsd={rule_min_thrsd},rule_max_thrsd={rule_max_thrsd},email_distro='{email_distro}',invalid_rec_flg='{invalid_rec_flg}',invalid_rec_sql='{invalid_rec_sql}',is_opsgenie_flg='{is_opsgenie_flg}',opsgenie_api_key='{opsgenie_api_key}',label_tags='{label_tags}',update_made_ts=current_timestamp() where rule_id = {rule_id} and db_name = '{db_name}' and src_tbl='{src_tbl}' and src_col='{src_col}' and meas_name='{meas_name}'"""
        print(query)
        self.dq_bigquery_client()
        query_job = self.client.query(query)
        return JsonResponse({'status':'success','message':'Data Updated succesfully'}, safe=False)
        
@method_decorator(csrf_exempt, name='dispatch')    
class SaveAutoMLEMtd(CredentialsandConnectivity): 
    def post(self, request,*args, **kwargs):  
        if request.content_type == 'application/json':
            data = json.loads(request.body.decode('utf-8'))
            print(data)
        else:
            data = request.POST
            print(data)   
         
        prfl_tbl_id = data.get("prfl_tbl_id")
        data_lob = data.get("data_lob")
        data_bus_elem = data.get("data_bus_elem")
        data_dmn = data.get("data_dmn")
        data_sub_dmn = data.get("data_sub_dmn")
        data_src = data.get("data_src")
        db_name = data.get("db_name")
        src_tbl = data.get("src_tbl")
        incr_dt_col = data.get("incr_dt_col")
        incr_dt_cond = data.get("incr_dt_cond")
        unq_index_cols = data.get("unq_index_cols")
        auto_prfl_freq = data.get("auto_prfl_freq")
        auto_prfl_schd_ts = data.get("auto_prfl_schd_ts")
        email_dist = data.get("email_dist")
        is_active_flg = data.get("is_active_flg")
        is_complete_flg = data.get("is_complete_flg")
        is_critical_flg = data.get("is_critical_flg")
        tbl_cde = data.get("tbl_cde")
        score_min_threshold = data.get("score_min_threshold")
        score_max_threshold = data.get("score_max_threshold")
        is_opsgenie_flg = data.get("is_opsgenie_flg")
        opsgenie_api_key = data.get("opsgenie_api_key")
        label_tags = data.get("label_tags")

        query = f"""update {self.dq_project_id}.dga_dq_tbls.dqaas_auto_prfl_mtd set data_lob='{data_lob}',data_bus_elem='{data_bus_elem}',data_dmn='{data_dmn}' ,data_src='{data_src}',incr_dt_col='{incr_dt_col}',incr_dt_cond={incr_dt_cond},unq_index_cols='{unq_index_cols}',auto_prfl_freq='{auto_prfl_freq}',auto_prfl_schd_ts='{auto_prfl_schd_ts}',email_dist='{email_dist}',is_active_flg='{is_active_flg}',is_complete_flg='{is_complete_flg}',is_critical_flg='{is_critical_flg}',tbl_cde='{tbl_cde}',update_made_ts=current_timestamp(),is_opsgenie_flg='{is_opsgenie_flg}',opsgenie_api_key='{opsgenie_api_key}',label_tags='{label_tags}' where prfl_tbl_id = {prfl_tbl_id} and db_name = '{db_name}' and src_tbl='{src_tbl}' and data_sub_dmn = '{data_sub_dmn}'"""
        print(query)
        self.dq_bigquery_client()
        query_job = self.client.query(query)
        return JsonResponse({'status':'success','message':'Data Updated succesfully'}, safe=False)
        
@method_decorator(csrf_exempt, name='dispatch')    
class DropdownSubdmnCustomMtd(CredentialsandConnectivity):
    
    def get(self, request, *args, **kwargs):
        query = f"""SELECT distinct(data_sub_dmn) FROM  {self.dq_project_id}.dga_dq_tbls.dqaas_profile_mtd"""
        print(query) 
        self.dq_bigquery_client()
        query_job = self.client.query(query)
        results = query_job.result()
        distinct_db = [dict(row) for row in results]
        return JsonResponse({'distinct_db':distinct_db})
    def post(self, request,*args, **kwargs):  
        if request.content_type == 'application/json':
            data = json.loads(request.body.decode('utf-8'))
            print(data)
        else:
            data = request.POST
            print(data)
        data_sub_dmn = data.get("sub_domain")
        query = f"""select *  from  {self.dq_project_id}.dga_dq_tbls.dqaas_profile_mtd where data_sub_dmn = '{data_sub_dmn}'"""
        print(query)
        self.dq_bigquery_client()
        query_job = self.client.query(query)
        results = query_job.result()
        
        rows_list = []
        for row in results:
            print("results", row)
            row_dict = dict(row)
            rows_list.append(row_dict)
        return JsonResponse(rows_list, safe=False) 

@method_decorator(csrf_exempt, name='dispatch')    
class SaveCustomMtd(CredentialsandConnectivity): 
    def post(self, request,*args, **kwargs):  
        if request.content_type == 'application/json':
            data = json.loads(request.body.decode('utf-8'))
            print("post data",data)
        else:
            data = request.POST
            print("post data",data)   
        for key in data:
            if data[key] == None:
                data[key] ='Null'
        prfl_id = data.get("prfl_id")
        prfl_type = data.get("prfl_type")
        data_lob = data.get("data_lob")
        data_bus_elem = data.get("data_bus_elem")
        data_dmn = data.get("data_dmn")
        data_sub_dmn = data.get("data_sub_dmn")
        data_src = data.get("data_src")
        dq_pillar = data.get("dq_pillar")
        feature_name = data.get("feature_name")
        db_name = data.get("db_name")
        src_tbl = data.get("src_tbl")
        src_col = data.get("src_col")
        incr_dt_col = data.get("incr_dt_col")
        incr_dt_cond = data.get("incr_dt_cond")
        meas_name = data.get("meas_name")
        meas_rule_sql = data.get("meas_rule_sql")
        micro_segment_columns = data.get("micro_segment_columns")
        aggregated_column = data.get("aggregated_column")
        comparison_type = data.get("comparison_type")
        micro_segment_threshold = data.get("micro_segment_threshold")
        max_threshold = data.get("max_threshold")
        min_threshold = data.get("min_threshold")
        is_hourly_flg = data.get("is_hourly_flg")
        run_hour = data.get("run_hour")
        add_param_clause = data.get("add_param_clause")
        pass_param_1 = data.get("pass_param_1")
        pass_param_2 = data.get("pass_param_2")
        pass_param_3 = data.get("pass_param_3")
        pass_param_4 = data.get("pass_param_4")
        pass_param_5 = data.get("pass_param_5")
        prfl_schd_ts = data.get("prfl_schd_ts")
        is_active_flg = data.get("is_active_flg")
        is_critical_flg = data.get("is_critical_flg")
        is_opsgenie_flg = data.get("is_opsgenie_flg")
        opsgenie_api_key = data.get("opsgenie_api_key")
        record_ind = data.get("record_ind")
        hourly_col = data.get("hourly_col")
        jira_assignee = data.get("jira_assignee")

        query = f"""update {self.dq_project_id}.dga_dq_tbls.dqaas_profile_mtd set data_sub_dmn = '{data_sub_dmn}',  data_lob='{data_lob}',data_bus_elem='{data_bus_elem}',data_dmn='{data_dmn}',dq_pillar ='{dq_pillar}',feature_name ='{feature_name}'  ,data_src='{data_src}',src_col='{src_col}',incr_dt_col='{incr_dt_col}',incr_dt_cond={incr_dt_cond},meas_name='{meas_name}',meas_rule_sql='''{meas_rule_sql}''',micro_segment_columns='{micro_segment_columns}',aggregated_column='{aggregated_column}',is_active_flg='{is_active_flg}',comparison_type='{comparison_type}',micro_segment_threshold={micro_segment_threshold},max_threshold={max_threshold},min_threshold={min_threshold},is_hourly_flg='{is_hourly_flg}',run_hour={run_hour},add_param_clause='{add_param_clause}',pass_param_1={pass_param_1},pass_param_2={pass_param_2},pass_param_3={pass_param_3},pass_param_4={pass_param_4},pass_param_5={pass_param_5},update_made_ts=current_timestamp(),is_opsgenie_flg='{is_opsgenie_flg}',opsgenie_api_key='{opsgenie_api_key}',prfl_schd_ts='{prfl_schd_ts}' where prfl_id = {prfl_id} and db_name = '{db_name}' and src_tbl='{src_tbl}' """
        print(query)
        self.dq_bigquery_client()
        query_job = self.client.query(query)
        return JsonResponse({'status':'success','message':'Data Updated succesfully'}, safe=False)




@method_decorator(csrf_exempt, name='dispatch')    
class ExecuteHistorySQL(CredentialsandConnectivity): 
# Handling file upload and processing SQL queries

    def dispatch(self, request, *args, **kwargs):
        # logger 
        if not hasattr(self, 'logger'):
            self.logger = logging.getLogger(__name__)
            logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

        return super().dispatch(request, *args, **kwargs)

    def post(self, request, *args, **kwargs):
        try:
            if not hasattr(self, 'logger'):
                self.logger = logging.getLogger(__name__)
            if 'file' not in request.FILES:
                self.logger.info("file uploaded.")
                return HttpResponse("file uploaded.", status=400)

            file = request.FILES['file']
            self.logger.info(f"Received file: {file.name}")

            queries = self.read_queries_from_uploaded_file(file)
            if not queries:
                self.logger.error("No valid queries found.")
                return HttpResponse("No valid queries found.", status=400)

            self.execute_queries(queries)
            return HttpResponse("File uploaded and queries executed successfully.", status=200)

        except Exception as e:
            self.logger.error(f"Error processing request: {e}")
            return HttpResponse("Error processing request.", status=500)

    # Execute queries in BQ
    def execute_queries(self, queries):
        self.dq_bigquery_client()
        for query in queries:
            try:
                query_job = self.client.query(query)
                query_job.result()
                self.logger.info("Query executed successfully.")
            except Exception as e:
                self.logger.error(f"Query execution error: {e}")

    # Read SQL queries from text file
    def read_queries_from_uploaded_file(self, file):
        try:
            content = file.read().decode('utf-8')
            queries = [q.strip() for q in re.split(r';\s*\n', content) if q.strip()]
            self.logger.info(f"Read {len(queries)} queries from file.")
            return queries
        except Exception as e:
            self.logger.error(f"Error reading file: {e}")
            return []
        
-------------------------------------------------------------------------
insert into vz-it-np-izcv-dev-idmcdo-0.dga_dq_tbls.dqaas_profile_rpt (rpt_seq_num, prfl_id, prfl_type, dq_pillar, src_tbl, meas_name, data_dt,feature_name,grouped_columns,count_curr,prfl_run_ts,weekday)
(select 1000005 as rpt_seq_num, 1403 as prfl_id, 'CUSTOM_RULES' as prfl_type, 'Consistency' as dq_pillar, 'cust_acct_addr' as src_tbl,'LAST_UPD_DT' as meas_name, cast(LAST_UPD_DT as date)as profile_date,
'Tier1 Models' as feature_name,
null as dimension,
count (*) as value,
current_timestamp as insert_date,
extract(dayofweek from LAST_UPD_DT) as weekday
from vz-it-pr-gk1v-cwlsdo-0.vzw_uda_prd_tbls_rd_v.cust_acct_addr where cast(LAST_UPD_DT as date)>= current_date -90 group by 1,2,3,4,5,6,7,8,11,12)
