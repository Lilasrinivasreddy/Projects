import pandas as pd
import logging
import argparse
import sys
import os
from datetime import datetime, timedelta, time
from configparser import ConfigParser
import traceback

## Importing User Defined Modules
sys.path.insert(1,os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))
import scripts.config_params as config
from scripts.common_handlers import CommonUtils, set_logger
from scripts.auto_profile import AutoProfileEngine
from scripts.sql_rule_profile_bkp import RuleProfileEngine
from scripts.sql_rule_profile import RuleProfile
from scripts.source_chk_avail import SourceCheckAvailability




## Validating all the crons
def cron_validation(logger: logging, utils: CommonUtils, df_val: pd.DataFrame):
    cron_scheduler_list = df_val["PROFILE_SCHEDULE_TS"][~df_val["PROFILE_SCHEDULE_TS"].isna()].unique().tolist()
    logger.info(f"Scheduler List: {cron_scheduler_list}")

    start_min, end_min = utils.get_minute_range()
    logger.info('-------------------------------------------------------------')
    cron_schd_for_curr_run: list = []
    for cron_schd in cron_scheduler_list:

        cron_trigger_yn = utils.validate_croniter(
            # logger=logger,
            cron_schd_format=cron_schd,
            start_min_range=start_min
        )
        
        if cron_trigger_yn == 'Y':
            cron_schd_for_curr_run.append(cron_schd)
        
        logger.info('-------------------------------------------------------------')
    
    return cron_schd_for_curr_run           






## Argument Parser - For getting the Data Source
def get_profile_input_details():
    message = None
    try:
        if len(sys.argv[1:]) > 0:
          parser_args = argparse.ArgumentParser()
          parser_args.add_argument('--data_src', dest='data_src', type=str, required=True, help="Data Source is Mandatory")
          parser_args.add_argument('--profile_type', dest='profile_type', type=str, required=True, help="Profile Type is Mandatory")
          args = parser_args.parse_args()
          
          data_src = args.data_src
          data_src = data_src.upper()

          profile_type = args.profile_type
          profile_type = profile_type.upper()
          
          if data_src in config.APPL_DATA_SRC and profile_type in config.APPL_PRFL_TYPE:
            return data_src, profile_type.lower()
         
        message = f"""\n
        Data Source and Profile Type Not Found for Auto/Rule Profile Scheduled Tables
        Flag                    : --data_src
        Applicable Data Source  : {config.APPL_DATA_SRC}
        Example for Teradata    : python3.9 table_watcher_auto_profile_cron --data_src=TD
        Example for GCP         : python3.9 table_watcher_auto_profile_cron --data_src=GCP
        
        ** Data Source and Profile Type are Mandatory
        """
    except Exception as err:
        message = f"Error Occurred in  Argument Flag Validation. Error: {err}"
        
    raise Exception(message)


# @staticmethod
def get_run_process_mtd_condition(run_type: str, schd_type:str):
    if run_type in ("DR", "RR") and schd_type == "DAILY":
        return f" and upper(is_daily_flg) = 'Y' "
    elif run_type in ("MR", "RR") and schd_type == "MONTHLY":
        return f" and upper(is_daily_flg) = 'Y'  and upper(is_monthly_flg) = 'Y' "
    elif run_type == "AR" and schd_type == "ADHOC":
        return " and upper(is_adhoc_flg) = 'Y' "      
    return None

# @staticmethod
def get_run_process_details(run_type: str, schd_type:str):
    if run_type == "DR" and schd_type == "DAILY":
        return f"Daily Run Profiling Process"
    if run_type == "MR" and schd_type == "MONTHLY":
        return f"Monthly Run Profiling Process"
    if run_type == "AR" and schd_type == "ADHOC":
        return "Adhoc Run Profiling Process" 
    if run_type == "RR" and schd_type == "DAILY":
        return f"Rerun for Daily Profiling Process"
    if run_type == "RR" and schd_type == "MONTHLY":
        return f"Rerun for Monthly Profiling Process"
        
    return None

# @staticmethod
def data_avail_retry_time_range(current):
    retry_flag = False
    start = current.replace(hour=0, minute=0, second=0)
    end = start + timedelta(days=1)
    intervals = pd.date_range(start, end, 24//4+1).tolist()  #Hardcoded to run every 4 Hours
    for interval in intervals:
        if interval <= current <= interval+timedelta(minutes=29):
            retry_flag = True
    return retry_flag

## Main Function
def main(run_type:str, schd_type:str):
    try:
    
        data_src, profile_type = get_profile_input_details()
        
        ## Creating Logger File and Object
        logger: logging = set_logger(
            logger_path=config.LOGS_DIR,
            log_filename=f'{data_src}_time_based_{profile_type}_profile_cron',
            process_name=f'{profile_type}-Cron',
            # date_with_minutes_yn='Y'
        )
        utils: CommonUtils = CommonUtils(logObj=logger)
        
        #metadata_where_condition: str = get_run_process_mtd_condition(run_type=run_type, schd_type=schd_type)
        query = f"""
        select *
        from {config.dqaas_mtd} T1 join 
        {config.dqaas_taxonomy} T2 on
        T1.product_name = T2.product_name and T1.database_name=T2.database_name and T1.table_name=T2.table_name
        WHERE  active_flag = 'Y'
        AND data_src = '{data_src}'
        AND profile_type = '{profile_type}'
        AND (profile_schedule_ts IS NOT NULL OR profile_schedule_ts <> '')
        ORDER BY profile_id;
        """

        df_val = utils.run_bq_sql(
            bq_auth=config.dq_gcp_auth_payload,
            select_query=query
        )
        logger.info(f"Records Found: {len(df_val)}")
            
        cron_schd_for_curr_run = []
        if len(df_val) > 0:
            df_val = df_val.rename(columns={col: str(col).upper() for col in df_val.columns.tolist()})
            cron_schd_for_curr_run = cron_validation(
                logger=logger,
                utils=utils,
                df_val=df_val
            )

            if len(cron_schd_for_curr_run) > 0:
                df_val = df_val[df_val["PROFILE_SCHEDULE_TS"].isin(cron_schd_for_curr_run)]
                logger.info(f"Records Found for this hour: {len(df_val)}")
                logger.info(f"Original df val: {df_val}")
                

            if len(cron_schd_for_curr_run) > 0:
                df_val = df_val[df_val["PROFILE_SCHEDULE_TS"].isin(cron_schd_for_curr_run)]


        if len(df_val) == 0 or len(cron_schd_for_curr_run) == 0:
            logger.warning("No Tables Scheduled for Current Hour")
            return
        
        logger.info(f"Records Count Before Source Check: {len(df_val)}")
        
        #Rerun Source Check every 4 Hrs for all tables where is_available='N'
        if data_avail_retry_time_range(datetime.now()) :
             logger.info("Rerun all today's table with source check indicator 'N' and current time - 4 Hrs")
             src_chk_not_avail_query = f"""select a.* from {config.dqaas_mtd} a join {config.dqaas_src_chk_avail} b on a.DATABASE_NAME=b.DATABASE_NAME and a.TABLE_NAME=b.TABLE_NAME where cast(run_dt as DATE) = date(current_timestamp(),'US/Eastern') and data_availability_indicator='N' and b.profile_type='{profile_type}' and cast(b.update_made_ts AS datetime) <= TIMESTAMP_SUB(datetime(current_timestamp(),'US/Eastern'),INTERVAL 4 HOUR)"""
             src_chk_not_avail_mtd = utils.run_bq_sql(
                 bq_auth=config.dq_gcp_auth_payload,
                 select_query=src_chk_not_avail_query
             )
             df_val=pd.concat([df_val,src_chk_not_avail_mtd])
             df_val = df_val.drop_duplicates()
        src_chk = SourceCheckAvailability(data_src,df_val)
        src_chk.main(data_src, profile_type)
    
        #Execute only the tables which has data in source
        src_chk_avail_query = f"""WITH CTE AS(select DATABASE_NAME,TABLE_NAME,ROW_NUMBER() over (partition by DATABASE_NAME,TABLE_NAME order by insert_made_ts desc) as rn from {config.dqaas_src_chk_avail} where CAST(run_dt AS DATE) = date(current_timestamp(),'US/Eastern') and profile_type='{profile_type}' and data_availability_indicator='Y') select DATABASE_NAME,TABLE_NAME from CTE where rn=1"""
        src_chk_avail_y = utils.run_bq_sql(
             bq_auth=config.dq_gcp_auth_payload,
             select_query=src_chk_avail_query
         )
        src_chk_avail_y = src_chk_avail_y.rename(columns={col: str(col).upper() for col in src_chk_avail_y.columns.tolist()})
        df_val = pd.merge(df_val, src_chk_avail_y, how='inner',on=['DATABASE_NAME','TABLE_NAME'])

        #Load balacer code starts from here
        server_name = config.LINUX_SERVERS.split(',')
        load_balancer_query = f"""WITH
            subdomain_table_counts AS (
            SELECT
                data_sub_dmn,
                profile_type,
                COUNT(table_name) AS table_count
            FROM
            `{config.dqaas_src_chk_avail}` where data_availability_indicator = 'Y'
            GROUP BY
                data_sub_dmn,profile_type ),
            total_tables AS (
            SELECT
            profile_type,
                SUM(table_count) AS total_table_count
            FROM
                subdomain_table_counts GROUP BY profile_type),
            server_check AS (
            SELECT
                stc.data_sub_dmn,
                stc.profile_type,
                stc.table_count,
                SUM(stc.table_count) over (partition by stc.profile_type order by stc.table_count) as check_table_count from subdomain_table_counts stc),
            final_check as (
                select sc.data_sub_dmn, sc.profile_type, sc.table_count,
                CASE 
                    WHEN sc.profile_type='auto' and sum(sc.table_count)  over (PARTITION BY sc.profile_type order by sc.table_count)  <= ( SELECT total_table_count / 3 FROM total_tables ts where profile_type='auto') THEN '{server_name[0]}' 
                    WHEN sc.profile_type='auto' and sum(sc.table_count)  over (PARTITION BY sc.profile_type order by sc.table_count)  <= ( SELECT total_table_count / 3 *2 FROM total_tables ts where profile_type='auto') THEN '{server_name[1]}'
                    WHEN sc.profile_type='auto' then '{server_name[2]}'
                
                    WHEN sc.profile_type='rule' and sum(sc.table_count) over (PARTITION BY sc.profile_type order by sc.table_count)  <= ( SELECT total_table_count / 2 FROM total_tables ts where profile_type='rule') THEN '{server_name[3]}' 
                    WHEN sc.profile_type='rule' then '{server_name[4]}'

                    WHEN sc.profile_type='rule_custom' and sum(sc.table_count) over (PARTITION BY sc.profile_type order by sc.table_count)  <= ( SELECT total_table_count / 2 FROM total_tables ts where profile_type='rule_custom') THEN '{server_name[5]}' 
                    WHEN sc.profile_type='rule_custom' then '{server_name[6]}'                
                END  AS assigned_server  FROM server_check sc)
            SELECT
            sc.data_sub_dmn,
            sc.profile_type,
            sc.table_count,
            sc.assigned_server
            FROM
            final_check  sc
            ORDER BY
            sc.profile_type,    
            sc.assigned_server,
            sc.table_count DESC"""
        #Using Load balancer query data to update the control table with server status
        try:
            update_query = f"""UPDATE `{config.dqaas_src_chk_avail}` AS CT SET CT.server_name=LB.assigned_server FROM ({load_balancer_query}) AS LB WHERE CT.data_sub_dmn = LB.data_sub_dmn AND CT.profile_type=LB.profile_type AND CT.data_availability_indicator = 'Y'"""
            load_ct_table_with_server_details = utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=update_query
            )
            
            logger.info(f"Server Assignment details updated in control table")
        except Exception as e:
            logger.error(f"Error in running load balancer update query. \nError: {err}")
        
    
    except Exception as err:
        traceback.print_exc()
        logger.error(f"Error in Source Check/Load balancer. \nError: {err}")
            

if __name__ == "__main__":
    main(run_type="DR", schd_type="DAILY")
