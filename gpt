import base64
from pathlib import Path
import requests
import json
import os
import sys
import numpy as np
import pandas as pd
import google.auth
from requests.exceptions import HTTPError
from google.cloud import bigquery
from datetime import datetime, timedelta
import pandas_gbq
import time
import decimal
from functools import reduce
# from logger import Logger
import logging
from dqaas_opsgenie import Alert
import traceback
import math

sys.path.insert(1,os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))
from utils.send_email import SendEmail
from config_data import get_config, set_logger
import scripts.config_params as config
from scripts.common_handlers import CommonUtils
from scripts.dqaas_jira import Jira_ticket
from scripts.invalid_sql_processor import InvalidRecProcessor




"""

Run Types   | Description   
-----------------------------
DR          | Daily Run     
MR          | Monthly Run
AR          | Adhoc Run
RR          | Rerun Request


Scheduled Types | Description
-----------------------------------------
DAILY           | Daily Scheduled Rules
MONTHLY         | Monthly Scheduled Rules
ADHOC           | Rules for Adhoc Run


"""

class RuleProfile(object):
    def __init__(self, data_src: str=None):
        self.config = get_config()
        self.data_src = data_src
        if self.data_src not in config.APPL_DATA_SRC:
            raise Exception(f"Data Source not Provided. Error: Data Source Value is {data_src}")
            
        self.log = self.set_rule_profile_logger(
            process_name="RP-Main",
            data_src=data_src
        )

        self.utils = CommonUtils(logObj=self.log)
        self._set_attributes(self.config)
        self.run_process_details = ""

    # @staticmethod
    def set_rule_profile_logger(self, process_name:str, data_src: str):
        
        timestamp = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
        filename = f'{data_src}_rule_profile_{timestamp}'
        
        process_id = os.getpid()
        log: logging = set_logger(
            logger_path=config.SQL_RULE_PROFILE_LOGS,
            log_filename=filename,
            process_name=f'{process_name}-{process_id}',
            only_date_yn="Y",
        )
        
        return log
    
    def __del__(self):
        self.log.info("Closing the Class Object")
        
    def _set_attributes(self, config):
        bq_cred_dtls = config['gcp_metadata_db']
        profile_dtls = config['sql_rule_profile']
        master_mtd_table = config['master_mtd_table']
        home_path: str = self.config["dir"]["home_dir"]
        config_path = self.config["dir"]["config_dir"]
        keys_folder_path = self.config["keys"]["gcp_creds_keys_path"]
        self.project_config = {
            "jn0v" : {
                "conn_details_folder" : f"{keys_folder_path}/jn0v"
            }
        }
        
        # self.log_path = os.path.abspath(os.path.join(home_path, "logs"))
        # self.log_file_path = os.path.abspath(os.path.join(self.log_path, 'rule_based_1corpdata_' + datetime.now().strftime('%Y-%m-%d-%H-%M-%S')+'.log'))
        # self.log = Logger(name='1corpdata', path=self.log_file_path).log
        # self.log = self._set_logger(logger_path=self.config["dir"]["logs_dir"], filename="rule_based_1corpdata")
        # self.log: logging = set_logger(logger_path=self.config["dir"]["logs_dir"], log_filename="rule_based_1corpdata")
        
        
        self.email_template = os.path.abspath(os.path.join(self.config["dir"]["template_dir"], r'dq_common_message.html'))

        self.dq_opsgenie_client = Alert(
            api_key=config["ops_genie"]["dq_ops_genie_api_key"],
            proxy=bq_cred_dtls['gcp_http_proxy_url']
        )
        self.od_opsgenie_client = Alert(
            api_key=config["ops_genie"]["od_ops_genie_api_key"],
            proxy=bq_cred_dtls['gcp_http_proxy_url']
        )

        ##  Data Quality Service Account
        self.dq_project_id = bq_cred_dtls['dq_project_id']
        self.dq_auth_payload = {
            "client_id": bq_cred_dtls['dq_client_id'],
            "client_secret": bq_cred_dtls['dq_client_secret_key'],
            "token_url": bq_cred_dtls['gcp_token_url'],
            "conn_project_id": self.dq_project_id,
            "sa_json_file_dtls": os.path.abspath(os.path.join(config_path, bq_cred_dtls['dq_sa_json'])),
            "project_space": os.path.join(config_path, "dq_oidc_token.json")
        }

        ##  One Corp Data Service Account
        self.od_conn_project_id = bq_cred_dtls['od_data_project_id']
        self.od_mtd_project_id = bq_cred_dtls['od_mtd_project_id']
        self.one_corp_auth_payload = {
            "client_id": bq_cred_dtls['od_client_id'],
            "client_secret": bq_cred_dtls['od_client_secret_key'], 
            "token_url": bq_cred_dtls['gcp_token_url'],
            "conn_project_id": self.od_conn_project_id,
            "sa_json_file_dtls": os.path.abspath(os.path.join(config_path, bq_cred_dtls['od_sa_json'])),
            "project_space": os.path.join(config_path, "od_oidc_token.json")
            }

        # DQ Space Metadata and Report Table Details
        # dq_dataset_name = self.dq_project_id + "." + profile_dtls['dq_dataset_name']
        self.dq_mtd_table_name =  self.dq_project_id + "." + profile_dtls['dq_dataset_name'] + "." + master_mtd_table['dq_metadata_table']
        self.dq_report_table_name = self.dq_project_id + "." + profile_dtls['dq_dataset_name'] + "." + profile_dtls['dq_rpt_table_name']

        # OneCorp Space Metadata and Report Table Details
        # od_dataset_name = self.od_mtd_project_id + "." + profile_dtls['od_dataset_name']
        self.od_report_table_name = self.od_mtd_project_id + "." + profile_dtls['od_dataset_name'] + "." + profile_dtls['od_rpt_table_name']
        self.od_invalid_table_name = self.od_mtd_project_id + "." + profile_dtls['od_dataset_name'] + "." + profile_dtls['od_invalid_table_name']
        self.n_days_interval = int(profile_dtls["n_days_limit"])

        ##  Proxy ## Commented on 2024-07-25 
        # os.environ['http_proxy'] = bq_cred_dtls['gcp_http_proxy_url']
        # os.environ['https_proxy'] = bq_cred_dtls['gcp_https_proxy_url']
        # os.environ['no_proxy'] = bq_cred_dtls['gcp_no_proxy_urls']

        ## Mail Distro
        self.failure_alert_email_group = str(profile_dtls['default_failure_mail_group']).split(',')
        self.summary_alert_email_group = str(profile_dtls['default_summary_mail_group']).split(',')
        
        self.monthly_process_date, self.monthly_process_yn = self.validate_monthly_process(profile_dtls['monthly_process_day'])
        self.log.info(f'monthly_process_yn:{self.monthly_process_yn}, monthly_process_date:{self.monthly_process_date}')

        self.current_datetime = datetime.now()
        self.current_date = datetime.strftime(self.current_datetime, '%Y-%m-%d')
        self.previous_day_date = datetime.strftime((self.current_datetime - timedelta(days=self.n_days_interval)), '%Y-%m-%d %H:%M:%S')
        
        ##  Email Configuration
        self.email = SendEmail(loggerObj=self.log,
                               smtp=config["email_configuration"]["smtp_server_name"],
                               mail_from=config["email_configuration"]["sender_email_id"])

    # @staticmethod
    # def _set_logger(logger_path, filename: str):
    #     logger_filename = filename+ '_' + datetime.now().strftime('%Y-%m-%d-%H-%M-%S')+'.log'
    #     log_file_path = os.path.abspath(os.path.join(logger_path, logger_filename))
    #     logger_obj  = Logger(name='1corpdata', path=log_file_path).log
    #     return logger_obj
    
    @staticmethod
    def validate_monthly_process(day: int):
        current_date = datetime.strftime(datetime.now(), '%Y-%m-%d')
        monthly_process_date = datetime.strftime(datetime.now(), f'%Y-%m-{day}')
        monthly_process_yn: str =  "Y" if current_date == monthly_process_date else "N"
        return monthly_process_date, monthly_process_yn
    
        
    ## Create Ops Genie Ticket
    def opsgenie_alert(self, alert_type,priority, message, description, details, one_corp_yn: str="N"):
        try:
            current_time_stamp = datetime.now().isoformat()
            env = config.get_config_values('environment', 'env')
            tags = [current_time_stamp,env,alert_type] 
            if one_corp_yn.upper() == "Y":
                # response = f"1Corp Opsgenie Alert.message:{message}, description:{description}, details:{details}, priority:{priority}"
                response = self.od_opsgenie_client.create_alert(message, description, details, priority,tags)
            else:
                # response = f"DQ Opsgenie Alert.message:{message}, description:{description}, details:{details}, priority:{priority}"
                response = self.dq_opsgenie_client.create_alert(message, description, details, priority,tags) 

            self.log.info(f"Opsgenies Response: {response}")
        except Exception as err:
            self.log.error(f"Error Occurred while Ops Genie Alert. Error: {err}")


    ## JWT Token Creation
    @staticmethod
    def exchange_and_save_oidc_token_for_jwt(self, url: str, client_id: str, client_secret: str, oidc_token_file_name:str) -> None:
        try:
            self.log.info('Retrieving JWT from OIDC provider...')
            payload = {'grant_type': 'client_credentials', 'client_id': client_id,
                       'client_secret': client_secret, 'scope': 'read'}
        
            response = requests.post(url=url, params=payload)
            response.raise_for_status()
            token = response.json()
            self.log.info('Saving token...')
            # Serializing json
            # oidc_token_file_name = "oidc_token.json"
            oidc_token_path = oidc_token_file_name
            if os.path.isfile(oidc_token_path):
                os.remove(oidc_token_path)
                time.sleep(7)

            with open(oidc_token_path, 'w') as f:  # don't change the file name
                json.dump(token, f)
        except HTTPError as e:
            raise HTTPError(f"Http Error. Error:{e}")
        except Exception as e:
            raise Exception(f"Error Ocurred in Generating the OIDC Token. Error:{e}")

    def dq_bigquery_client(self, auth: dict):
        self.log.info(f"Default Auth: {auth}")
        self.log.debug(f'url={auth["token_url"]}, client_id={auth["client_id"]}, client_secret={auth["client_secret"]}')
        
        self.exchange_and_save_oidc_token_for_jwt(self,
                                            url=auth["token_url"],
                                            client_id=auth["client_id"],
                                            client_secret=auth["client_secret"], 
                                            oidc_token_file_name=auth["project_space"]
                                            )
        
        self.log.info('Setting environment variable...')
        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = auth["sa_json_file_dtls"]
        os.environ['GOOGLE_CLOUD_PROJECT'] = auth["conn_project_id"]
        
        credentials, _ = google.auth.default()
        
        client = bigquery.Client(credentials=credentials, project=auth["conn_project_id"])
        self.log.info(f'Connected to {auth["conn_project_id"]} project space')
        
        return client, credentials
    #Used to connect to user mentioned project space dynamically using the json file path from dqaas_product_features_meta table
    def dq_bigquery_client_dynamic(self,project_space):
        if project_space not in self.project_config:
            raise Exception(f"Unknown project space details : : {project_space}.Please configure it in config and server if it is a new project space ")
        config = self.project_config[project_space]
        folder = config["folder"]
        if not os.path.exists(folder) or not os.path.isdir(folder):
            raise Exception(f"Invalid folder path for project space : {project_space}")
        
        gcp_creds_file = os.path.join(folder, "gcp_creds.json")
        
        with open(gcp_creds_file, 'r') as file:
            gcp_creds_content = json.load(file)
        oidc_token_file_path = self.find_file_with_suffix(folder, "oidc_token.json")
        sa_config_file_path = self.find_file_with_suffix(folder, "-config.json") 
        auth = {
            "client_id": gcp_creds_content['client_id'],
            "client_secret": gcp_creds_content['client_secret'], 
            "token_url": gcp_creds_content['token_url'],
            "conn_project_id": gcp_creds_content["conn_project_id"],
            "sa_json_file_dtls": sa_config_file_path,
            "project_space": oidc_token_file_path
        }
        self.log.info(f"Dynamic Auth: {auth}")
        
        self.log.debug(f'url={auth["token_url"]}, client_id={auth["client_id"]}, client_secret={auth["client_secret"]}')
        
        self.exchange_and_save_oidc_token_for_jwt(self,
                                            url=auth["token_url"],
                                            client_id=auth["client_id"],
                                            client_secret=auth["client_secret"], 
                                            oidc_token_file_name=auth["project_space"]
                                            )
        
        self.log.info('Setting environment variable...')
        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = auth["sa_json_file_dtls"]
        os.environ['GOOGLE_CLOUD_PROJECT'] = auth["conn_project_id"]
        
        credentials, _ = google.auth.default()
        
        client = bigquery.Client(credentials=credentials, project=auth["conn_project_id"])
        self.log.info(f'Connected to {auth["conn_project_id"]} project space')
        
        return client, credentials
        
    ## Retrieve Max Sequence
    def get_max_sequence(self, bq_client: bigquery.Client, sq_column: str, bq_tablename: str) -> int:
        try:
            if bq_client is None:
                raise Exception('BQ Client not Found')
            
            count = bq_client.query(f"select IFNULL(MAX({sq_column}), 0) from {bq_tablename}").to_dataframe()
            rpt_max_sequence = count.iloc[0, 0]
            self.log.info(f'Max Sequence: {rpt_max_sequence}')
            return rpt_max_sequence
        except Exception as err:
            self.log.error(f'Error While Finding Max Sequence for the Table {bq_tablename}. Error: {err}')

        return -1
        
    @staticmethod
    def round_off(val):  
        d = decimal.Decimal(val)
        return d.quantize(decimal.Decimal('.01'), decimal.ROUND_DOWN)

    def get_date_details(self, dq_bq_client:bigquery.Client, incr_dt_dict:dict,run_type):
        try:
            if run_type == 'RR':
                date_interval_query = f"""
                    select  format_date('%Y-%m-%d', "{incr_dt_dict['start_date']}"),
                            format_date('%Y-%m-%d',"{incr_dt_dict['end_date']}"),
                            format_date('%Y%m', "{incr_dt_dict['start_year_month']}"),
                            format_date('%Y%m', "{incr_dt_dict['end_year_month']}"),
                            parse_date('%Y%m%d', format_date('%Y%m%d', "{incr_dt_dict['start_date']}")),
                            parse_date('%Y%m%d', format_date('%Y%m%d', "{incr_dt_dict['end_date']}")),
                            parse_date('%Y%m%d', concat(format_date('%Y%m', "{incr_dt_dict['start_year_month']}"), '01')),
                            format_timestamp('%Y-%m-%d %T', timestamp({incr_dt_dict['invld_dt']}));
                """
            else: 
                date_interval_query = f"""
                select  format_date('%Y-%m-%d', date({incr_dt_dict['start_date']})),
                        format_date('%Y-%m-%d', date({incr_dt_dict['end_date']})),
                        format_date('%Y%m', date({incr_dt_dict['start_year_month']})),
                        format_date('%Y%m', date({incr_dt_dict['end_year_month']})),
                        parse_date('%Y%m%d', format_date('%Y%m%d', date({incr_dt_dict['start_date']}))),
                        parse_date('%Y%m%d', format_date('%Y%m%d', date({incr_dt_dict['end_date']}))),
                        parse_date('%Y%m%d', concat(format_date('%Y%m', date({incr_dt_dict['start_year_month']})), '01')),
                        format_timestamp('%Y-%m-%d %T', timestamp({incr_dt_dict['invld_dt']}));
            """
            self.log.info(f"date_interval_query: {date_interval_query}")
            
            result= dq_bq_client.query(date_interval_query).to_dataframe()
            self.log.info(result)
            
            return {
                'START_DATE': result.iat[0, 0],
                'END_DATE': result.iat[0, 1],
                'START_YEAR_MONTH': result.iat[0, 2],
                'END_YEAR_MONTH': result.iat[0, 3],
                'START_DATE_SRC_COL_DT_VAL': pd.Timestamp(datetime.strptime(str(result.iat[0, 4]), '%Y-%m-%d')),
                'END_DATE_SRC_COL_DT_VAL': pd.Timestamp(datetime.strptime(str(result.iat[0, 5]), '%Y-%m-%d')),
                'START_YEAR_MONTH_SRC_COL_DT_VAL': pd.Timestamp(datetime.strptime(str(result.iat[0, 6]), '%Y-%m-%d')),
                'INVL_DT': f"timestamp('{result.iat[0, 7]}')"
            }
        except Exception as err:
            raise RuntimeError(f"Error Occurred in Data Interval. Error: {err}")

    @staticmethod
    def get_run_process_mtd_condition(run_type: str, schd_type:str):
        if run_type in ("DR", "RR") and schd_type == "DAILY":
            return f" and upper(is_daily_flg) = 'Y' "
        elif run_type in ("MR", "RR") and schd_type == "MONTHLY":
            return f" and upper(is_daily_flg) = 'Y'  and upper(is_monthly_flg) = 'Y' "
        elif run_type == "AR" and schd_type == "ADHOC":
            return " and upper(is_adhoc_flg) = 'Y' "      
        return None
    
    @staticmethod
    def get_run_process_details(run_type: str, schd_type:str):
        if run_type == "DR" and schd_type == "DAILY":
            return f"Daily Run Profiling Process"
        if run_type == "MR" and schd_type == "MONTHLY":
            return f"Monthly Run Profiling Process"
        if run_type == "AR" and schd_type == "ADHOC":
            return "Adhoc Run Profiling Process" 
        if run_type == "RR" and schd_type == "DAILY":
            return f"Rerun for Daily Profiling Process"
        if run_type == "RR" and schd_type == "MONTHLY":
            return f"Rerun for Monthly Profiling Process"
          
        return None
    
    
    # def get_rule_metadata_details(self, dq_bq_client:bigquery.Client, metadata_condition: str) -> pd.DataFrame:
    # def get_rule_metadata_details(self, dq_bq_client:bigquery.Client) -> pd.DataFrame:
    #     try:
    #         mtd_query = f""" 
    #             select *
    #             from {self.dq_mtd_table_name}
    #             where upper(is_active_flg) = 'Y'
    #             --{metadata_condition}
    #             order by profile_id;
    #         """.replace('[','(').replace(']',')')
            
    #         self.log.info(f"Metadata Query: {mtd_query}")
    #         return dq_bq_client.query(mtd_query).to_dataframe()
        
    #     except Exception as e:
    #         self.log.error(f"Error: {e}")
            
        # return pd.DataFrame()
    def send_failure_status_email(self, table_name: str, rule: str, data_sub_dmn: str='', persona: str=''):
        try:
            receipents_email_addr_list = None
            if len(data_sub_dmn) > 0 and len(persona) > 0:
                receipents_email_addr_list = self.utils.get_mail_distro(df_val=self.mail_list, sub_dmn=data_sub_dmn, persona=persona)
            self.log.info(f"Receipents email details - Table: {receipents_email_addr_list}")

            if receipents_email_addr_list is None:
                receipents_email_addr_list = config.RP_DEFAULT_MAIL_GROUP
            self.log.info(f"Receipents email details - default: {receipents_email_addr_list}")
            
            self.log.info(f'Initiating eMail for failed rule {rule}')
            self.email.send_rules_error_report(
                email_template_filepath=config.rp_error_report_email_template,
                mail_subject=r'Rule Failure Report',
                receipents_email_id=receipents_email_addr_list,
                rule_id=rule,
                table_name=table_name
            )
        except Exception as e:
            self.log.error(f'Error while triggering error status.\n {e}')
    
    def run_td_rule_profile_engine(self, df_rule_data, rules_name):
        df_rule_data['col_vld_cnt'] = ""
        df_rule_data['col_invld_cnt'] = ""
        # df_rule_data['VALID_Prcnt'] = ""
        df_rule_data['col_invld_pct'] = ""
        df_rule_data['col_null_cnt'] = ""
        df_rule_data['col_dist_cnt'] = ""
        df_rule_data['col_min_val'] = ""
        df_rule_data['col_max_val'] = ""
        df_rule_data['col_vld_pct'] = ""
        df_rule_data['col_tot_cnt'] = ""      

        rules_error_list: list = []
        for count, rule in enumerate(rules_name):        
            valid_percent = 0.00
            invalid_percent = 0.00
            valid_count = 0
            invalid_count = 0
            null_records = 0
            unique_records = 0
            min = 0
            max = 0
            tablename = ''
            data_sub_dmn = ''
            rule_sql = ""
            try:
                self.log.info(f'Rule:{rule}')
                rule_id = df_rule_data.loc[count, 'PROFILE_ID']
                df_rule_data["RULE_ID"] = rule_id
                dbname = df_rule_data['DATABASE_NAME'][count]
                rule_sql = df_rule_data['RULE_SQL'][count]  # get the query for respective rule
                tablename = df_rule_data['TABLE_NAME'][count]
                data_sub_dmn = df_rule_data['DATA_SUB_DMN'][count]
                # db_engine = self.get_connection(dbname=dbname, section='profile_database')
                # df = pd.read_sql(query, db_engine)
                
                df = self.utils.run_teradata_sql(
                    db_name=dbname, 
                    query=rule_sql, 
                    td_auth=config.src_tbl_db_config
                )
                self.log.info(f"dataframe: {df}")
                dx = df[rule].value_counts()
                null_count = int(df.iloc[:, 0].replace(r'^\s*$', np.nan, regex=True).isnull().sum())
                unique = len(df.iloc[:, 0].replace(r'^\s*$', np.nan, regex=True).unique())
                try:
                    valid_percent = float(dx.loc['VALID']/len(df))
                    self.log.info(f"valid_percent: {valid_percent}")
                except:
                    valid_percent = 0.00
                try:
                    invalid_percent = float(dx.loc['INVALID']/len(df))
                    self.log.info(f"invalid_percent: {invalid_percent}")
                except:
                    invalid_percent = 0.00
                try:
                    valid_count = dx.loc['VALID']
                    self.log.info(f"valid_count: {valid_count}")
                except:
                    valid_count = 0
                try:
                    invalid_count = dx.loc['INVALID']
                    self.log.info(f"invalid_count: {invalid_count}")
                except:
                    invalid_count = 0
                try:
                    null_records = null_count
                except:
                    null_records = 0
                try:
                    unique_records = unique
                except:
                    unique_records = 0
                try:
                    min_val = df.iloc[:, 0].replace(r'^\s*$', 0, regex=True).min()
                    min = int(min_val)
                    self.log.info(f'min:{min}')
                except:
                    min = 0
                try:
                    max_val = df.iloc[:, 0].replace(r'^\s*$', 0, regex=True).max()
                    max = int(max_val)
                    self.log.info(f'max:{max}')
                except:
                    max = min
            except Exception as e:
                self.log.error(f'Error in Validating the Rule({rule}).\nError info:{e}')
                # eMail Error Report
                self.send_failure_status_email(tablename, rule)
                rules_error_list.append({'table':tablename, 'rules': rule , 'query': rule_sql})

            # df_rule_data.loc[count, 'VALID_Prcnt'] = valid_percent*100
            df_rule_data.loc[count, 'col_vld_pct'] = valid_percent*100  # df_rule_data['VALID_Prcnt']
            df_rule_data.loc[count, 'col_invld_pct'] = invalid_percent*100
            df_rule_data.loc[count, 'col_vld_cnt'] = int(valid_count)
            df_rule_data.loc[count, 'col_invld_cnt'] = int(invalid_count)
            df_rule_data['col_tot_cnt'] = df_rule_data['col_vld_cnt'] + df_rule_data['col_invld_cnt']
            df_rule_data.loc[count, 'col_null_cnt'] = int(null_records)
            df_rule_data.loc[count, 'col_dist_cnt'] = int(unique_records)
            df_rule_data.loc[count, 'col_min_val'] = min
            df_rule_data.loc[count, 'col_max_val'] = max


        return df_rule_data, rules_error_list
    
    def run_gcp_rule_profile_engine(self, df_rules_list: pd.DataFrame, val_to_replace: dict):
        try:
            self.log.info('-------------------------------------------------------------------------')
            self.log.info('Rule Profile Engine - Initiated')
            self.log.info('-------------------------------------------------------------------------')

            error_list = []
            df_rules_list['COL_VLD_CNT'] = ""
            df_rules_list['COL_INVLD_CNT'] = ""
            df_rules_list['COL_INVLD_PCT'] = ""
            df_rules_list['COL_NULL_CNT'] = ""
            df_rules_list['COL_DIST_CNT'] = ""
            df_rules_list['COL_MIN_VAL'] = ""
            df_rules_list['COL_MAX_VAL'] = ""
            df_rules_list['COL_VLD_PCT'] = ""
            df_rules_list['COL_TOT_CNT'] = "" 
            
            remote_clients = {}
            local_bq_client, _ = self.dq_bigquery_client(self.dq_auth_payload)
            for idx in df_rules_list.index:
                self.log.info('-------------------------------------------------------------------------')
                is_remote = df_rules_list.loc[idx,"RUN_QUERIES_ON_REMOTE"].upper()                
                if is_remote.upper() == 'Y':
                    project_space = df_rules_list.loc[idx,"VSAD"]
                    if project_space not in remote_clients:
                        remote_client, _ = self.dq_bigquery_client_dynamic(project_space)
                        remote_clients[project_space] = remote_client
                    else:
                        remote_client = remote_clients[project_space]
                    bq_client = remote_client
                else:
                    bq_client = local_bq_client
                rule = df_rules_list.loc[idx, 'RULE_NAME']
                table_name = df_rules_list.loc[idx, 'TABLE_NAME']
                rule_sql = df_rules_list.loc[idx, 'RULE_SQL']
                valid_count, invalid_count = 0, 0
                try:
                    rule_id = df_rules_list.loc[idx, 'PROFILE_ID']
                    df_rules_list["profile_id"] = rule_id
                    self.log.info(f'index:{idx}, profile_id:{rule_id}, rule_name:{rule}')
                    
                    ##  Replacing the placeholder in SQL Query with actual values.
                    ##  val_to_replace argument is a dict variable which has values to replace in sql
                    rule_sql = reduce(lambda sql, replace_str: sql.replace(*replace_str), [rule_sql, *list(val_to_replace.items())])
                    self.log.info(f"RuleSQL:{rule_sql}")
                    
                    df_result = bq_client.query(rule_sql).to_dataframe()

                    self.log.info(f'\n{df_result}')

                    try:
                        valid_count =  df_result[df_result[rule] == 'VALID']['count']
                        self.log.info(f"valid count1: {valid_count}")
                        valid_count = valid_count.iloc[0] if len(valid_count) > 0 else 0
                        self.log.info(f"valid count2: {valid_count}")
                    except:
                        valid_count = 0

                    try:
                        invalid_count =  df_result[df_result[rule] == 'INVALID']['count']
                        self.log.info(f"invalid count1: {invalid_count}")
                        invalid_count = invalid_count.iloc[0] if len(invalid_count) > 0 else 0
                        self.log.info(f"invalid count2: {invalid_count}")
                    except:
                        invalid_count = 0

                except Exception as e:
                    self.log.error(f'Error in executing Rule SQL. Error:{e}')

                    error_list.append({'table': table_name,
                                        'rules': rule,
                                        'query': rule_sql})
                    
                    self.email.send_common_message(mail_subject='Rule Failure',
                                                    message=f'Rule Failed.<br>Rule: {rule}<br>Query: {rule_sql}',
                                                    receipents_email_id=self.failure_alert_email_group,
                                                    email_template_filepath=self.email_template)

                    if df_rules_list.loc[idx, 'OPSGENIE_FLAG'] == 'Y':
                        self.opsgenie_alert(one_corp_yn="Y",
                                            priority="P2",
                                            message='DQ-2.0 Rule Failure',
                                            description=f'DQ-2.0 Rule Failed.<br>Rule: {rule}<br>Query: {rule_sql}',
                                            details={'Message': 'DQ-2.0 Rule Failure', 'Rule': rule, 'Query': rule_sql},
                                            alert_type="Exception")
                    
                total_record_count, valid_percent, invalid_percent = 0, 0.0, 0.0
                total_record_count = int(valid_count) + int(invalid_count)
                try:
                    valid_percent = float(valid_count/total_record_count) * 100
                except:
                    valid_percent = 0.00
                try:
                    invalid_percent = float(invalid_count/total_record_count) * 100
                except:
                    invalid_percent = 0.00

                self.log.info(f'valid_count:{valid_count}, invalid_count:{invalid_count}, total_record_count:{total_record_count}')
                self.log.info(f'valid_percent:{valid_percent}, invalid_percent:{invalid_percent}')

                if valid_percent < 100:
                    if df_rules_list.loc[idx, 'OPSGENIE_FLAG'] == 'Y':
                        env = config.get_config_values('environment', 'env')
                        prod_name = df_rules_list.loc[idx, 'PRODUCT_NAME']
                        db_name = df_rules_list.loc[idx, 'DATABASE_NAME']
                        alert_type = "SQL_Rule_failed"
                        run_date = datetime.now().isoformat()
                        self.opsgenie_alert(one_corp_yn="Y",
                                    priority="P3",
                                    message=f"Lensx|{env}|{alert_type}|{prod_name}|{db_name}| Rule Score less than 100|{run_date}",
                                    description=f'Rule Score less than 100 <br> Rule: {rule}',
                                    details={'Message': 'DQ-2.0 Score less than 100', 'Rule': rule},
                                    alert_type="Rule failed")
                    elif df_rules_list.loc[idx, 'JIRA_ASSIGNEE'] is not None:  
                        try:
                            jira_assignee = df_rules_list.loc[idx, 'JIRA_ASSIGNEE']
                            lable = "DQaaS"       
                            table_name = df_rules_list.loc[idx, 'TABLE_NAME']                  
                            self.log.info(f"Calling Jira Module for: {table_name}")                        
                            self.log.info(f"No data found for the table: {table_name}")  

                            process_date = f"'{datetime.now().date() - timedelta(days=config.RP_N_DAYS_LIMIT)}'"                                            
                            
                            summary = f"LensX|DQ Failure|Table: {table_name} no data found for profiling"
                            description = f"DQ has failed for Table : {table_name} on Process date : {process_date}."
                            jira_client = Jira_ticket()
                            ticket_id=jira_client.create_jira_ticket(jira_assignee,summary, description,lable)
                            self.log.info(f"Jira Id created: {ticket_id}")
                        except Exception as err:
                            self.log.error(f"Error occured while creating JIRA tickets {err}")

                
                df_rules_list.loc[idx, 'COL_VLD_PCT'] = self.round_off(float(valid_percent))
                df_rules_list.loc[idx, 'COL_INVLD_PCT'] =  self.round_off(float(invalid_percent))
                df_rules_list.loc[idx, 'COL_VLD_CNT'] = int(valid_count)
                df_rules_list.loc[idx, 'COL_INVLD_CNT'] = int(invalid_count)
                df_rules_list.loc[idx, 'COL_TOT_CNT'] = total_record_count
                df_rules_list.loc[idx, 'COL_NULL_CNT'] = 0
                df_rules_list.loc[idx, 'COL_DIST_CNT'] = 0
                df_rules_list.loc[idx, 'COL_MIN_VAL'] = 0
                df_rules_list.loc[idx, 'COL_MAX_VAL'] = 0   
            return df_rules_list, error_list
        except Exception as err:
            self.log.error(f"Error Occurred in Rules Profiling. Error: {err}")
            print(traceback.format_exc())
        return pd.DataFrame(), []

    @staticmethod
    def critical_type_header_name(flag: str = None):
        if flag is None:
            return ""
        if flag == 'Y':
            return 'Critical '
        if flag == 'N':
            return 'Non Critical '
        return ""
    
    def send_summary_sub_dmn_level_mail(self, sub_domain: str, rules_data=pd.DataFrame(), df_rules_error_list=pd.DataFrame(), critical_flag=None):
        try:
            self.log.info('Preparing Overall Summary for Persona Group ------------------')
            email_cols_list = config.RP_REQD_SUMMARY_COLS
            df_email_rules_data = rules_data.loc[:, rules_data.columns.isin(email_cols_list)]
            df_email_rules_data = df_email_rules_data.sort_values(by=['col_vld_pct'])
            df_email_rules_data['col_vld_pct'] = round(df_email_rules_data['col_vld_pct'].astype(float), 2)
            # df_email_rules_data['RULE_RUN_DT'] = pd.to_datetime(df_email_rules_data['RULE_RUN_DT']).dt.date
            self.log.info(f'SQL Profile eMail Columns: {df_email_rules_data.columns.tolist()}')
            df_email_rules_data = df_email_rules_data[email_cols_list]
            df_email_rules_data = df_email_rules_data.rename(columns=config.RP_EMAIL_SUMMARY_COL_RENAME)
            df_email_rules_data = df_email_rules_data.reset_index(drop=True)

            report_header_name = self.critical_type_header_name(flag=critical_flag)
            data_sub_dmn=sub_domain
            self.log.info(f'Sub Domain:{data_sub_dmn}')
            self.mail_list = self.utils.get_email_distros_from_table(data_sub_dmn_list=[sub_domain])
            
            email_group = self.utils.get_mail_distro(df_val=self.mail_list, sub_dmn=data_sub_dmn, persona='EMAIL_DISTRO')
            self.log.info(f'Email Group:{email_group}')
            
            if len(email_group ) == 0:
                email_group = config.RP_DEFAULT_MAIL_GROUP ## receipents_email_id
                
            self.email.send_rules_email_message(
                df_val=df_email_rules_data,
                df_err_val=df_rules_error_list,
                email_template_filepath=config.rp_summary_report_email_template,
                receipents_email_id=email_group, #receipents_email_id,
                mail_subject=report_header_name + 'Rule Summary for '+ data_sub_dmn,
                report_header_name=report_header_name + 'Rule Summary for '+ data_sub_dmn
            )
            
        except Exception as e:
            self.log.error('Error While Initiating Overall Summary Email.\n Error Info:', e)

    def send_summary_table_level_mail(self, df_mail_summary: pd.DataFrame, error_rules_list: list, rule_run_dt,schd_type):
        try:

            self.log.info(f'Summary Result Len: {len(df_mail_summary)}, Error list : {error_rules_list}')
            if len(df_mail_summary) > 0:
                src_table_list = df_mail_summary["table_name"].unique().tolist()
                self.log.info(f'Source Table List: {src_table_list}')
                
                df_rules_error_list = pd.DataFrame.from_records(error_rules_list)
                df_rules_error_list = df_rules_error_list.reset_index(drop=True)
                for tbl in src_table_list:
                    product_name = df_mail_summary[df_mail_summary['table_name']==tbl]['product_name'][0]
                    data_sub_dmn =df_mail_summary[df_mail_summary['table_name']==tbl]['data_sub_dmn'][0]
                    self.log.info(f'Product Name: {product_name}')
                    self.log.info(f'Data Sub Domain: {data_sub_dmn}')
                    self.log.info(f'Source Table: {tbl}')
                    try:
                        self.log.info(f'{schd_type} run: {tbl} Summary Email - DQ-2.0')
                        if schd_type == "DAILY":
                            subject = f'Daily run: DQ-2.0 - {product_name}|{data_sub_dmn}|{tbl} Summary Mail - {datetime.strftime(rule_run_dt, "%Y-%m-%d")}'
                        elif schd_type== "MONTHLY":
                            subject = f'Monthly run: DQ-2.0 - {product_name}|{data_sub_dmn}|{tbl} Summary Mail - {datetime.strftime(rule_run_dt, "%Y-%m-%d")}'
                        else:
                            subject = f'Ad-hoc run: DQ-2.0 - {product_name}|{data_sub_dmn}|{tbl} Summary Mail - {datetime.strftime(rule_run_dt, "%Y-%m-%d")}'

                        message = 'Please find the below summary.<br>'
                        #here removed data_dmn from the below email_cols_list
                        email_cols_list = ['rule_run_dt', 'db_name', 'table_name', 'src_col', 'dq_pillar',
                                        'rule_name', 'col_vld_cnt', 'col_invld_cnt', 'col_vld_pct', 'dq_status']

                        df_email_rules_data: pd.DataFrame = df_mail_summary[email_cols_list][df_mail_summary["table_name"]==tbl]
                        if len(df_email_rules_data) > 0:
                            df_email_rules_data = df_email_rules_data.sort_values(by=['col_vld_pct'])
                            df_email_rules_data['col_vld_pct'] = df_email_rules_data['col_vld_pct'].astype(float).map(self.round_off)

                            self.log.info(f'SQL Profile eMail Columns: {df_email_rules_data.columns.tolist()}')
                            df_email_rules_data = df_email_rules_data.rename(columns={  
                                                                                        'db_name': 'Database',
                                                                                        'table_name': 'Table',
                                                                                        'src_col': 'Column',
                                                                                        'dq_pillar': 'DQ Pillar',
                                                                                        'rule_name': 'Measure',
                                                                                        'col_vld_cnt': 'Valid Count',
                                                                                        'col_invld_cnt': 'Invalid Count',
                                                                                        'col_vld_pct': 'DQ Score',
                                                                                        'dq_status': 'Indicator',
                                                                                        # 'data_dmn': 'Domain',
                                                                                        'rule_run_dt': 'Date'})
                            df_email_rules_data = df_email_rules_data.reset_index(drop=True)

                        if len(df_rules_error_list) > 0:
                            df_rules_error_list_email = df_rules_error_list[df_rules_error_list["table"]==tbl]
                            df_rules_error_list_email = df_rules_error_list_email.reset_index(drop=True)
                            if len(df_rules_error_list_email) > 0:
                                addl_msg = f'<br><b>Rule Profile Error List:</b>{df_rules_error_list_email.to_html()}'
                                message += addl_msg

                        self.email.send_common_message(email_template_filepath=self.email_template,
                                                       mail_subject=subject,
                                                       message=message,
                                                       receipents_email_id=self.summary_alert_email_group,
                                                       df_val=df_email_rules_data)
                        self.log.info(f'Successfully Triggered {tbl} Summary Email for DQ-2.0')
                    except Exception as e:
                        self.log.error( f'Error Occurred in {tbl} Summary Email for DQ-2.0. Error:{e}')
                    continue

        except Exception as e:
            self.log.error( f'Error Occurred in Summary Email for  DQ-2.0. Error:{e}')
            self.opsgenie_alert(priority="P1",
                           message='DQ-2.0 - Summary Email Error',
                           description=f'Failure Occurred while sending summary email',
                           details={'Message':'DQ-2.0 - Summary Email Error'},
                           alert_type="Exception")


    def execute_invalid_sqls(self, df_invalid_rec:pd.DataFrame, val_to_replace: dict, date_interval: dict, rule_run_dt):
        try:

            self.log.info('-------------------------------------------------------------------------')
            self.log.info('Invalid SQL Query Execution - Initiated')
            self.log.info('-------------------------------------------------------------------------')
            
            df_invalid_rec = df_invalid_rec[(df_invalid_rec['invalid_records_flag']=='Y') & (df_invalid_rec['col_invld_cnt'] > 0)]
            df_invalid_rec = df_invalid_rec.reset_index(drop=True)
            self.log.info(f'Invalid data rules length: {len(df_invalid_rec)}')

            if len(df_invalid_rec) ==  0:
                self.log.warning("No Records Found for Populating Invalid Records")
            else:
                import re
                od_bq_client, _ = self.dq_bigquery_client(auth=self.one_corp_auth_payload)
                for idx in df_invalid_rec.index:
                    self.log.info('-------------------------------------------------------------------------')
                    rule = df_invalid_rec.loc[idx, 'rule_name']
                    rule_id = df_invalid_rec.loc[idx, 'profile_id']
                    #invalid_rec_sql ## Changed
                    invalid_sql_query = df_invalid_rec.loc[idx, 'invld_rec_sql']
                    try:

                        invalid_sql_query = reduce(lambda sql, replace_str: sql.replace(*replace_str), [invalid_sql_query, *list(val_to_replace.items())])
                        self.log.info(f"RuleSQL:{invalid_sql_query}")
                        
                        invalid_sql_query = re.sub("CURRENT_TIMESTAMP()", date_interval['INVL_DT'], invalid_sql_query, flags=re.IGNORECASE)
                        invalid_sql_query = re.sub("CURRENT_TIMESTAMP", date_interval['INVL_DT'], invalid_sql_query, flags=re.IGNORECASE)
                        
                        self.log.info(f'Invalid SQL index: {idx}, profile_id: {rule_id}, rule: {rule}')
                        self.log.info(f"Invalid Records Query: {invalid_sql_query}")   
                        result = od_bq_client.query(invalid_sql_query)
                        result.result()
                        self.log.info(f'result: {result.num_dml_affected_rows}')


                    except Exception as e:
                        self.log.error(f'Error Occurred while validating invalid_rec_sql: {e}')
                        self.email.send_common_message(mail_subject=f'Invalid SQL Failed',
                                                message=f'Invalid SQL Failed.<br>Rule: {rule}<br>Invalid SQL: {invalid_sql_query}',
                                                receipents_email_id=self.failure_alert_email_group,
                                                email_template_filepath=self.email_template)
                        self.opsgenie_alert(one_corp_yn="Y",
                                    priority="P2",
                                    message='DQ-2.0 Invalid SQL Failure',
                                    description=f'Invalid SQL Failed.<br>Rule: {rule}<br>Invalid SQL: {invalid_sql_query}',
                                    details={'Message': 'DQ-2.0 Invalid SQL Failure', "Rule":rule, "Invalid SQL": invalid_sql_query},
                                    alert_type="Exception")
                        
                        continue

        except Exception as err:
            raise RuntimeError(f"Error Occurred in Invalid SQLs execution block. Error:{err} ")

    def check_threshold_create_opsgenie_JIRA_alert(self,rules_data=pd.DataFrame()):
        try:
            opsgenie_alert_info: list = [] 
            #rules_data  = rules_data[rules_data['IS_OPSGENIE_FLG'] == "Y"]
            rules_data['max_threshold_limit'] = rules_data['max_threshold_limit'].fillna(config.RP_DEFAULT_MAX_THRSD)
            rules_data = rules_data.reset_index(drop=True)
            for idx in rules_data.index:                
                self.log.info(f"col_vld_pct: {rules_data.loc[idx,'col_vld_pct']}")
                self.log.info(f"max_threshold_limit: {rules_data.loc[idx,'max_threshold_limit']}")
                if rules_data.loc[idx,'col_vld_pct'] < rules_data.loc[idx,'max_threshold_limit']:                
                    if rules_data.get('opsgenie_flag') == 'Y':              
                        priority = "p3"
                        alert_type = 'SQL_Rule_failed' 
                        profile_type = "sql_rule"        
                        env = config.get_config_values('environment', 'env')                                
                        api_key = rules_data.loc[idx,'opsgenie_api']
                        #if not api_key:
                        if api_key in config.EMPTY_STR_LIST or (isinstance(api_key,float) and math.isnan(api_key)) :
                                # Opsgenie Api Key
                                api_key = config.get_config_values('opsgenie', 'api_key')

                        # response,request_id,message = self.create_opsgenie_alert(rules_data,idx,alert_type,priority,api_key)
                        gcp_http_proxy_url = config.GCP_HTTP_PROXY_URL          
                        opsgenie_client = Alert(api_key=api_key,proxy=gcp_http_proxy_url)
                        rules_data = rules_data.rename(columns={col: str(col).upper() for col in rules_data.columns.tolist()})
                        response,request_id,message = opsgenie_client.create_opsgenie_alert(rules_data, 0,alert_type,priority,env ,profile_type)
                        self.log.info(f"Opsgenie response code: {response}")
                        self.log.info('Opsgenie alert sent successfully')
                        self.log.info(f"Alert Message: {message}")
                    elif rules_data.get('jira_assignee') is not None:  
                        try:
                            jira_assignee = rules_data.get('jira_assignee')
                            lable = "DQaaS"       
                            table_name = rules_data.get('table_name','')                  
                            self.log.info(f"Calling Jira Module for: {table_name}")                        
                            self.log.info(f"No data found for the table: {table_name}")  

                            process_date = f"'{datetime.now().date() - timedelta(days=config.RP_N_DAYS_LIMIT)}'"                                            
                            
                            summary = f"LensX|DQ Failure|Table: {table_name} no data found for profiling"
                            description = f"DQ has failed for Table : {table_name} on Process date : {process_date}."
                            jira_client = Jira_ticket()
                            ticket_id=jira_client.create_jira_ticket(jira_assignee,summary, description,lable)
                            self.log.info(f"Jira Id created: {ticket_id}")
                        except Exception as err:
                            self.log.error(f"Error occured while creating JIRA tickets {err}")

                    try:
                        data_lob = rules_data.loc[idx,'data_lob']
                        data_bus_elem = rules_data.loc[idx,'data_bus_elem']
                        data_dmn = rules_data.loc[idx,'data_dmn']
                        data_sub_dmn = rules_data.loc[idx,'data_sub_dmn']

                        prod_info = self.get_prod_details(data_lob,data_bus_elem,data_dmn,data_sub_dmn)
                    except Exception as err:
                        self.log.error(f"Error occured while fetching product details {err}")

                    
        except Exception as err:
            
            self.log.info(f"Error in check_threshold_create_opsgenie_JIRA_alert while rule profile: {err}")
    def get_prod_details(self,data_lob,data_bus_elem,data_dmn,data_sub_dmn):
        query = f''' select distinct product_type,product_area,product_name,business_program 
                    from {config.dqaas_rule_prfl_mtd} mtd
                    JOIN {config.dqaas_auto_rule_prod_mtd} prd 
                    ON 
                    mtd.data_lob = prd.data_lob
                    AND mtd.data_bus_elem = prd.data_bus_elem
                    AND mtd.data_dmn = prd.data_dmn
                    AND mtd.data_sub_dmn = prd.data_sub_dmn
                    where
                    mtd.data_lob = '{data_lob}'
                    AND mtd.data_bus_elem = '{data_bus_elem}'
                    AND mtd.data_dmn = '{data_dmn}'
                    AND mtd.data_sub_dmn = '{data_sub_dmn}';
                '''    
        try:
            # metadata_query = f"""
            #     SELECT * FROM {config.dqaas_rule_prfl_mtd}
            #     WHERE IS_ACTIVE_FLG = 'Y'    
            #     {add_condition}
            #     ORDER BY rule_id;
            # """
            ## AND DATA_SRC = 'TD'
            df_val = self.utils.run_bq_sql(
                bq_auth=config.dq_gcp_auth_payload,
                select_query=query
            )
            # df_val = df_val.rename(columns={col: str(col).upper() for col in df_val.columns.tolist()})
            return df_val
        except Exception as err:
            raise Exception(f'Error Occured While Executing the Query to fetch prod details. Error: {err} ')
    
    def load_result_to_bq_tables(self, df_rules_list: pd.DataFrame, rules_execution_process_details:dict):
        try:
            self.log.info('-------------------------------------------------------------------------')
            self.log.info('Loading Result to Report Table - Initiated')
            self.log.info('-------------------------------------------------------------------------')
            self.log.info(f"rules_execution_process_details:{rules_execution_process_details}")
            # df_rules_list['rule_prfl_num'] = ''
            df_rules_list['prfl_run_dt'] = rules_execution_process_details['rules_execution_date']

            # df_rules_list['src_col_dt_val'] = rules_execution_process_details['business_date']
            report_reference_key = datetime.now().strftime("%Y%m%d%H%M%S%f")
            df_rules_list['rpt_ref_key'] = report_reference_key

            df_rules_list['threshold_limit'] = df_rules_list['threshold_limit'].fillna(float(self.config['sql_rule_profile']["default_min_thrsd"])).astype('float64')
            df_rules_list['max_threshold_limit'] = df_rules_list['max_threshold_limit'].fillna(float(self.config['sql_rule_profile']["default_max_thrsd"])).astype('float64')

            df_rules_list['dq_status'] = ''           
            df_rules_list['dq_status'] = np.where((df_rules_list['col_vld_pct'] >= df_rules_list['max_threshold_limit']),
                                               'Good', df_rules_list['dq_status'])
            df_rules_list['dq_status'] = np.where((df_rules_list['col_vld_pct'] <= df_rules_list['threshold_limit']),
                                               'Bad', df_rules_list['dq_status'])
            df_rules_list['dq_status'] = np.where((df_rules_list['col_vld_pct'] < df_rules_list['max_threshold_limit']) &
                                               (df_rules_list['col_vld_pct'] > df_rules_list['threshold_limit']),
                                               'Average', df_rules_list['dq_status'])
            df_rules_list['dq_status'] = np.where((df_rules_list['col_vld_cnt'] == 0) & (df_rules_list['col_invld_cnt'] == 0),
                                               'No Data', df_rules_list['dq_status'])
            df_rules_list['col_vld_pct'] = np.where((df_rules_list['col_vld_cnt'] == 0) & (df_rules_list['col_invld_cnt'] == 0),
                                               np.nan, df_rules_list['col_vld_pct'])

            # df_rules_list['run_type'] = rules_execution_process_details['run_type']
            # df_rules_list['schd_type'] = rules_execution_process_details['schd_type']
            
            ##  Column Details for loading data into Reporting Tables
            col_list = ['rpt_ref_key', 'profile_id', 'prfl_run_dt', 'dq_pillar','table_name','col_name','col_completeness','col_uniqueness','col_conformity','col_validity','col_integrity', 'col_tot_cnt',
                        'col_vld_cnt', 'col_invld_cnt', 'col_vld_pct', 'col_invld_pct','profile_type','dq_status']
            decimal_cols_list = ['col_vld_pct', 'col_invld_pct']
            int_cols_list = [  'col_tot_cnt', 'col_vld_cnt', 'col_invld_cnt']
            str_cols_list = [ 'dq_pillar' ,'dq_status']

            try:
                self.check_threshold_create_opsgenie_JIRA_alert(df_rules_list)
            except Exception as err:
                self.log.error(f"Error occured while creating opsgenie alert during rule profiling {err}")
            
            self.log.info('-------------------------------------------------------------------------')
            self.log.info('Loading Results into DQ Report Table')
            self.log.info('-------------------------------------------------------------------------')

            try:
                
                
                # rpt_max_sequence = self.get_max_sequence(bq_client=dq_bq_client,
                #                                          bq_tablename=self.dq_report_table_name,
                #                                          sq_column='rule_prfl_num'
                #                                         )
                
                # self.log.info(f"Max Sequence in DQ Space:{rpt_max_sequence}")
                # if rpt_max_sequence == -1:
                #     raise RuntimeError("Error Occurred while Identifying the Max Sequence in DQ Space")
                
                # dq_df_rules_list = df_rules_list.assign(rule_prfl_num=[rpt_max_sequence +i for i in range(1, len(df_rules_list)+1)])
                dq_df_rules_list = df_rules_list.copy()
                for col in decimal_cols_list:
                    dq_df_rules_list[col] = dq_df_rules_list[col].astype(str).map(decimal.Decimal)
                for col in int_cols_list:
                    dq_df_rules_list[col] = dq_df_rules_list[col].astype('int64')
                for col in str_cols_list:
                    dq_df_rules_list[col] = dq_df_rules_list[col].astype(str)
                load_rules_data = dq_df_rules_list.loc[:, dq_df_rules_list.columns.isin(col_list)]
                load_rules_data = load_rules_data.reset_index(drop=True)
                load_rules_data = load_rules_data.drop_duplicates()
                load_rules_data = load_rules_data.loc[:, ~load_rules_data.columns.duplicated()]
                
                self.log.info(f'Report Length:{len(load_rules_data)}, Table: {self.dq_report_table_name} \n{load_rules_data}')
                dq_bq_client, dq_credentials = self.dq_bigquery_client(self.dq_auth_payload)
                pandas_gbq.to_gbq(dataframe=load_rules_data,
                                destination_table=self.dq_report_table_name,
                                if_exists='append',
                                credentials=dq_credentials,
                                project_id=self.dq_project_id,
                                )
                self.log.info(f'Data Loaded Successfully to the table({self.dq_report_table_name})')
            except Exception as e:
                print(traceback.format_exc())
                self.log.error(f'Error Occurred While Loading Data into DQ Dataset. Error Info: {e}')
                self.opsgenie_alert(priority="P1",
                               message='DQaaS - Failure when Loading summary',
                               description=f'Rule Summary Loading Failed in DQaaS Project Space.<br>Table Name: {self.dq_report_table_name}',
                               details={'Message':'Rule Summary Loading Failed in DQaaS Project Space', "Table Name": self.dq_report_table_name},
                               alert_type="Exception")
                
            # Check and load to respective project space if the remote flag is enabled.   
            self.log.info(f"df_rules_lits :: {df_rules_list.columns}")
            self.log.info(f"df_rules_lits :: {df_rules_list.to_string()}")
            df_rules_list_remote = df_rules_list[(df_rules_list["run_queries_on_remote"] == 'Y') & (df_rules_list["vsad"] == 'jn0v')]
            if len(df_rules_list_remote) > 0:
                self.log.info('-------------------------------------------------------------------------')
                self.log.info('Loading Results into One Corp Data Dataset')
                self.log.info('-------------------------------------------------------------------------')
                try:
                    project_space = df_rules_list_remote.iloc["vsad"]
                    bq_client, dq_credentials = self.dq_bigquery_client_dynamic(project_space)
                    rpt_max_sequence = self.get_max_sequence(bq_client=bq_client,
                                                            bq_tablename=self.od_report_table_name,
                                                            sq_column='rule_prfl_num'
                                                            )

                    self.log.info(f"Max Sequence in OneCorp Space:{rpt_max_sequence}")
                    if rpt_max_sequence == -1:
                        raise RuntimeError("Error Occurred while Identifying the Max Sequence in Onecorp Space")
                    
                    od_df_rules_list = df_rules_list.assign(rule_prfl_num=[rpt_max_sequence +i for i in range(1, len(df_rules_list)+1)])
                    for col in decimal_cols_list:
                        od_df_rules_list[col] = od_df_rules_list[col].astype(str).map(decimal.Decimal)
                    for col in int_cols_list:
                        od_df_rules_list[col] = od_df_rules_list[col].astype('int64')
                    for col in str_cols_list:
                        od_df_rules_list[col] = od_df_rules_list[col].astype(str)
                    load_rules_data = od_df_rules_list.loc[:, od_df_rules_list.columns.isin(col_list)]
                    load_rules_data = load_rules_data.reset_index(drop=True)

                    self.log.info(f'Report Length:{len(load_rules_data)}, Table: {self.od_report_table_name} \n{load_rules_data}')

                    pandas_gbq.to_gbq(dataframe=load_rules_data,
                                    destination_table=self.od_report_table_name,
                                    if_exists='append',
                                    credentials=dq_credentials,
                                    project_id=self.od_conn_project_id,
                                    )
                    self.log.info(f'Data Loaded Successfully to the table({self.od_report_table_name})')
                except Exception as e:
                    print(traceback.format_exc())
                    self.log.error(f'Error Occurred While Loading Data into One Corp Data Dataset. Error Info: {e}')     
                    self.opsgenie_alert(priority="P1",
                                message='DQ-2.0 - Failure when Loading summary',
                                description=f'Rule Summary Loading Failed in One Corp Project Space.<br>Table Name: {self.od_report_table_name}',
                                details={'Message':'Rule Summary Loading Failed in One Corp Project Space', "Table Name": self.od_report_table_name},
                                alert_type="Exception")

        except Exception as e:
            print(traceback.format_exc())
            self.log.error( f'Error Occurred in loading Data. Error:{e}')

        
    # def get_rule_metrics_details(self, rule_run_dt, metadata_condition:str):
    def get_rule_metrics_details(self, rule_run_dt):
        try:
            summary_query = f"""select  a.*,c.*, col_vld_cnt, col_invld_cnt, col_vld_pct,dq_status, prfl_run_dt
                from    `{self.dq_mtd_table_name}` a,
                        `{self.dq_report_table_name}` b,
                        {config.dqaas_taxonomy} c
                where a.profile_id = b.profile_id and a.product_name=c.product_name
                and upper(a.active_flag) = 'Y'
                and b.prfl_run_dt = '{rule_run_dt}'
                order by a.profile_id ;"""
            
            self.log.info(f"Metrics Query: {summary_query}")
            dq_bq_client, _ = self.dq_bigquery_client(self.dq_auth_payload)
            return dq_bq_client.query(summary_query).to_dataframe()
        except Exception as err:
            raise RuntimeError(f"Error Occurred while Identifying the Rule Metrics. Error: {err}")

    def profile_engine(self, df_rules_list:pd.DataFrame, incr_dt_dict:dict, run_type:str, schd_type:str, src_tablename:str = None):
        exec_status, exec_msg = None, None
        try:
            ##  DQ Space BigQuery Client
            dq_bq_client, _ = self.dq_bigquery_client(self.dq_auth_payload)
            
            ##  Get Date Details for Profiling 
            date_interval = self.get_date_details(dq_bq_client=dq_bq_client,
                                                  incr_dt_dict=incr_dt_dict,run_type=run_type)
            self.log.info(f'Data Interval for Queries: {date_interval}')
            
            ## Values for replacing the placeholders in SQL Query
            val_to_replace_str = {
                "$start_dt": date_interval['START_DATE'],
                "$end_dt": date_interval['END_DATE'],
                "$start_yr_mnth": date_interval['START_YEAR_MONTH'],
                "$end_yr_mnth": date_interval['END_YEAR_MONTH'],
                "$incr_col1": self.config['sql_rule_profile']["incr_col1"],
                "$incr_col2": self.config['sql_rule_profile']["incr_col2"],
                "$incr_col3": self.config['sql_rule_profile']["incr_col3"],
                "$incr_col4": self.config['sql_rule_profile']["incr_col4"],
                "$incr_col5": self.config['sql_rule_profile']["incr_col5"],
                "$incr_col6": self.config['sql_rule_profile']["incr_col6"],
            }
            self.log.info(f'Values to replace in SQL: {val_to_replace_str}')
            
            ## Additional Details Like Run Type, Schedule Type, Execution Date and Business Date For Profiling
            rules_execution_process_details: dict = {
                "run_type": run_type,
                "schd_type": schd_type,
                "rules_execution_date": pd.Timestamp(datetime.now() - timedelta(days=self.n_days_interval)),
                "business_date": date_interval['START_DATE_SRC_COL_DT_VAL'] if run_type in ('DR','RR') else date_interval['END_DATE_SRC_COL_DT_VAL']
            }
            self.log.info(f"Rules Execution Process details :{rules_execution_process_details}")
            if src_tablename is not None and run_type in ("AR", "RR"):
                metadata_where_condition += f" and upper(table_name) = upper('{src_tablename}') "
                
            ## Get Metadata Details for Rules Execution
            # df_rules_list = self.get_rule_metadata_details(dq_bq_client=dq_bq_client,
            #                                                metadata_condition=metadata_where_condition)
            
            self.log.info(f"Rules Length: {len(df_rules_list)}")
            if len(df_rules_list) == 0:
                raise RuntimeError("No Rules found for profiling")
            data_src = df_rules_list.loc[0, 'DATA_SRC']
            # df_rules_list = df_rules_list.head(10)
            rules_name = df_rules_list['RULE_NAME'].to_list()
            self.log.info(f"rules_name: {rules_name}")
            self.log.info(f"data_src : {data_src}")
            ##  Run Rule Profile Engine based on data source
            if data_src in config.RP_AGG_RULES_APPL_DATA_SRC:
                self.log.info(f"Inside GCP Rule Engine")
                df_rules_list, error_list = self.run_gcp_rule_profile_engine(df_rules_list=df_rules_list,
                                                                     val_to_replace=val_to_replace_str)
            
            if data_src in config.RP_NON_AGG_RULES_APPL_DATA_SRC:
                self.log.info(f"Inside TD Rule Engine")
                df_rules_list, error_list = self.run_td_rule_profile_engine(df_rules_list, rules_name)
            ##  Loading Results to BQ tables in DQ and OneCorp Space  
            df_rules_list.columns = df_rules_list.columns.str.lower()  
            self.load_result_to_bq_tables(df_rules_list,
                                          rules_execution_process_details)
            
            ##  Get Rules Metric Report for Send Summary mail and Run Invalid Rec SQls
            df_rules_result = self.get_rule_metrics_details(rule_run_dt=rules_execution_process_details["rules_execution_date"])
                                                            # metadata_condition=metadata_where_condition)
            
            self.log.info(f"Rule Summary Length: {len(df_rules_result)}")
            if len(df_rules_result) == 0:
                raise Exception(" No Results found for Summary Mail and Run Invalid SQLs.")
            df_rules_result_tbl_level = df_rules_result[df_rules_result['email_type']== 'TABLE']
            df_rules_result_sub_dmn_level = df_rules_result[(df_rules_result['email_type']== 'email_type') | (df_rules_result['email_type']== '') | (df_rules_result['email_type'].isna())]
            ##  Send Summary Report Table Level
            if len(df_rules_result_tbl_level) > 0:
                self.send_summary_table_level_mail(df_mail_summary=df_rules_result_tbl_level,
                                       error_rules_list=error_list,
                                       rule_run_dt=rules_execution_process_details["rules_execution_date"],
                                       schd_type = schd_type)

            ##  Send Summary Report Sub Domain Level
            if len(df_rules_result_sub_dmn_level) > 0:
                sub_domain_list = df_rules_result_sub_dmn_level['data_sub_dmn'].unique().tolist()
                for domain in sub_domain_list:    
                    self.send_summary_sub_dmn_level_mail(
                        sub_domain=domain,
                        rules_data=df_rules_result_sub_dmn_level,
                        df_rules_error_list=error_list,
                        critical_flag=''
                    )

            if src_tablename is not None and run_type in ("RR"):
                self.del_invalid_records(tablename=src_tablename,
                                         start_dt=date_interval['START_DATE'],
                                         end_dt=date_interval['END_DATE'],
                                         schd_type=schd_type)

            ##  Invalid Rec SQLs Execution Block
            # self.execute_invalid_sqls(df_invalid_rec=df_rules_result,
            #                           rule_run_dt=rules_execution_process_details["rules_execution_date"],
            #                           date_interval=date_interval,
            #                           val_to_replace=val_to_replace_str)   
            try: 
                invld_process_obj = InvalidRecProcessor()
                invld_process_obj.execute_invalid_sqls_onecorp(df_invalid_rec=df_rules_result,
                                        rule_run_dt=rules_execution_process_details["rules_execution_date"],
                                        date_interval=date_interval,
                                        val_to_replace=val_to_replace_str)
            except Exception as e:
               self.log.error(f"Error in invalid sql function") 
               print(traceback.format_exc())
            exec_status, exec_msg = "SUCCESS", f"{self.run_process_details} Completed"
            self.log.info(exec_msg)
        except RuntimeError as err:
            exec_status, exec_msg = "ERROR", f"Run Time Error. Error:{err}"
            self.log.error(exec_msg)
        except ValueError as err:
            exec_status, exec_msg = "ERROR", f"Value Error. Error:{err}"
            self.log.error(exec_msg)
        except HTTPError as err:
            exec_status, exec_msg = "ERROR", f"HTTP Error. Error:{err}"
            self.log.error(exec_msg)
        except Exception as err:
            exec_status, exec_msg = "ERROR", f"Exception Occurred in Main Block. Error:{err}"
            self.log.error(exec_msg)

        #if schd_type == "DAILY"  :
        #        mail_subject_msg = f"DQ-2.0 Rule Profiling completed for the daily run on ({self.current_date})"
        #elif schd_type == "MONTHLY":
        #    mail_subject_msg = f"DQ-2.0 Rule Profiling completed for the monthly run on ({self.current_date})"
        #else:
        #    mail_subject_msg = f"DQ-2.0 Rule Profiling completed for the adhoc run on ({self.current_date})"
        #    
        ## MJ:
        #self.email.send_common_message(email_template_filepath=self.email_template,
        #                               #mail_subject=f"1CorpData Rule Profiling Completed",
        #                               mail_subject = mail_subject_msg,
        #                               message="DQ-2.0 rule Profiling Initiation completed",
        #                               receipents_email_id=self.summary_alert_email_group)
        
        return exec_status, exec_msg
    
    def del_invalid_records(self, tablename:str, start_dt:str, end_dt: str, schd_type: str):
        try:
            del_query = f"""
                delete
                from {self.od_invalid_table_name}
                where incrm_dt_val is not null
                and upper(db_name) = upper('{tablename}')
                and incrm_dt_val = '{start_dt}';
            """
            ##  and date(dq_rule_run_dt) between '{start_dt}' and '{end_dt}' 
            od_bq_client, _ = self.dq_bigquery_client(auth=self.one_corp_auth_payload)
            del_rec_count = od_bq_client.query(del_query)
            del_rec_count.result()
            self.log.info(f"Invalid records del count : {del_rec_count.num_dml_affected_rows}")
        except Exception as err:
            self.log.info(f"Error occurred while deleting the invalid records for rerun. Error: {err}")

    # Method to Initiate Daily Process 
    def daily_run_process(self,df_rules_list,run_type=None):
        if run_type == "RR":
            try:
                self.log.info('-------------------------------------------------------------------------')
                self.log.info('Rerun Process - Initiated')
                self.log.info('-------------------------------------------------------------------------')
                business_date = df_rules_list.loc[0,"RUN_DT"]
                incr_dt_dict_val = {'start_date': business_date,
                                    'end_date':business_date,
                                    'start_year_month': business_date,
                                    'end_year_month': business_date,
                                    'invld_dt': self.config['sql_rule_profile']['invld_dt']
                                    }
                
                exec_status, exec_msg = self.profile_engine(df_rules_list,incr_dt_dict=incr_dt_dict_val,
                                                            run_type="RR",
                                                            schd_type="DAILY")

                self.log.info('-------------------------------------------------------------------------')
                self.log.info(f"Rerun. Execution Status:{exec_status}, Message:{exec_msg}")
                self.log.info('-------------------------------------------------------------------------')
                return exec_status, exec_msg
            except Exception as err:
                exec_msg = f"Error Occurred in Rerun Process Main block. Error: {err}"
                self.log.error(exec_msg)
                return "ERROR", exec_msg
        
        else:
            try:
                self.log.info('-------------------------------------------------------------------------')
                self.log.info('Daily Run Process - Initiated')
                self.log.info('-------------------------------------------------------------------------')
                
                incr_dt_dict_val = {'start_date': self.config['sql_rule_profile']['start_date'],
                                    'end_date': self.config['sql_rule_profile']['end_date'],
                                    'start_year_month': self.config['sql_rule_profile']['start_year_month'],
                                    'end_year_month': self.config['sql_rule_profile']['end_year_month'],
                                    'invld_dt': self.config['sql_rule_profile']['invld_dt']
                                    }
                
                exec_status, exec_msg = self.profile_engine(df_rules_list,incr_dt_dict=incr_dt_dict_val,
                                                            run_type="DR",
                                                            schd_type="DAILY")

                self.log.info('-------------------------------------------------------------------------')
                self.log.info(f"Daily Run. Execution Status:{exec_status}, Message:{exec_msg}")
                self.log.info('-------------------------------------------------------------------------')
                return exec_status, exec_msg
            except Exception as err:
                exec_msg = f"Error Occurred in Daily Run Process Main block. Error: {err}"
                self.log.error(exec_msg)
                return "ERROR", exec_msg
            

    # Method to Initiate Monthly Process 
    def monthly_run_process(self,df_rules_list):
        try:
            self.log.info('-------------------------------------------------------------------------')
            self.log.info('Monthly Run Process - Initiated')
            self.log.info('-------------------------------------------------------------------------')
            incr_dt_dict_val = {'start_date': f"date_trunc(date_sub('{self.monthly_process_date}', interval 1 month), month)",
                                'end_date': f"last_day(date_sub('{self.monthly_process_date}', interval 1 month), month)",
                                'start_year_month': f"date_trunc(date_sub('{self.monthly_process_date}', interval 1 month), month)",
                                'end_year_month': f"last_day(date_sub('{self.monthly_process_date}', interval 1 month), month)",
                                'invld_dt': self.config['sql_rule_profile']['invld_dt']
                                }

            exec_status, exec_msg = self.profile_engine(df_rules_list,incr_dt_dict=incr_dt_dict_val,
                                                        run_type="MR",
                                                        schd_type="MONTHLY")
            
            self.log.info('-------------------------------------------------------------------------')
            self.log.info(f"Monthly Run. Execution Status:{exec_status}, Message:{exec_msg}")
            self.log.info('-------------------------------------------------------------------------')
            return exec_status, exec_msg
        except Exception as err:
            exec_msg = f"Error Occurred in Monthly Run Process Main block. Error: {err}"
            self.log.error(exec_msg)
            return "ERROR", exec_msg

    ##  Adhoc and Rerun Execution Process - Called while receiving the Trigger Files 
    def adhoc_rerun_process(self, tablename:str, start_dt:str, end_dt: str, schd_type: str, trigger_file: str):
        try:
            self.log.info('-------------------------------------------------------------------------')
            self.log.info('Adhoc / Rerun Process - Initiated')
            self.log.info('-------------------------------------------------------------------------')
            self.log.info(f"Arrived Inputs for Initiating the Process. tablename:{tablename}, start date:{start_dt}, end date: {end_dt}, schedule type: {schd_type}, Trigger File: {trigger_file}")
            
            run_type = ""
            if schd_type == "ADHOC":
                run_type = "AR"
            elif schd_type in ("DAILY", "MONTHLY"):
                run_type = "RR"
            
            if len(run_type) == 0:
                raise ValueError("Rerun or Adhoc Process Not identified")
            else:
                incr_dt_dict_val: dict = {'start_date': f"'{start_dt}'",
                                          'end_date': f"'{end_dt}'",
                                          'start_year_month': f"'{start_dt}'",
                                          'end_year_month': f"'{end_dt}'",
                                          'invld_dt': self.config['sql_rule_profile']['invld_dt']
                                          }

                exec_status, exec_msg = self.profile_engine(incr_dt_dict=incr_dt_dict_val,
                                                            run_type=run_type,
                                                            schd_type=schd_type.upper(),
                                                            src_tablename=tablename)
                
                
                self.log.info('-------------------------------------------------------------------------')
                self.log.info(f"Adhoc/Rerun Process. Execution Status:{exec_status}, Message:{exec_msg}")
                self.log.info('-------------------------------------------------------------------------')
            
                return exec_status, exec_msg
       
        except ValueError as verr:
            exec_msg = f"Value Error. Error:{verr}"
            self.log.error(exec_msg)
            return "ERROR", exec_msg
            
        except Exception as err:
            exec_msg = f"Error Occurred in Adhoc / Rerun Process Main block. Error: {err}"
            self.log.error(exec_msg)
            return "ERROR", exec_msg
        
        
    ## Main Block for Running Daily and Monthly Process
    def run_regular_process(self,df_rules_list,run_type=None):
        exec_status, exec_msg, sched_type = None, None, 'DAILY'
        try:
            #mail_subject_msg = f"DQ-2.0 Rule Profiling started for the daily run on ({self.current_date})"
            #if self.monthly_process_yn == "MONTHLY":
            #    mail_subject_msg = f"DQ-2.0 Rule Profiling started for the monthly run on ({self.current_date})"
#
            #self.email.send_common_message(email_template_filepath=self.email_template,
            #                               mail_subject = mail_subject_msg,
            #                               message="DQ-2.0 rule profiling have started",
            #                               receipents_email_id=self.summary_alert_email_group)
            
            exec_status, exec_msg = self.daily_run_process(df_rules_list,run_type)

            if exec_status == "ERROR":
                return "ERROR", f"Error in {sched_type} Run Process: {exec_msg}"
            
            #Monthly Process Starts
            if self.monthly_process_yn == "Y":
                sched_type = 'MONTHLY'
                exec_status, exec_msg = self.monthly_run_process(df_rules_list)
                if exec_status == "ERROR":
                    return "ERROR", f"Error in {sched_type} Run Process: {exec_msg}"
            
            #Send Profile Completed Alert
            #mail_subject_msg = f"DQ-2.0 Rule Profiling ended for the daily run on ({self.current_date})"
            #if self.monthly_process_yn == "MONTHLY":
            #    mail_subject_msg = f"DQ-2.0 Rule Profiling ended for the monthly run on ({self.current_date})"
#
            #self.email.send_common_message(email_template_filepath=self.email_template,
            #                               mail_subject = mail_subject_msg,
            #                               message="DQ-2.0 rule profiling have ended",
            #                               receipents_email_id=self.summary_alert_email_group)
            
            return "SUCCESS", "Rule Profiling Completed"    
        except Exception as err:
            exec_msg = f"Error Occurred in Regular Execution Block: {err}"
            self.log.error(exec_msg)
            return "ERROR", exec_msg

## Method to initiate the Execution Process for Daily and Monthly
def daily_monthly_normal_execution_process():

    config = get_config()
    if config is not None:
        ruleprofile = RuleProfile()
        exec_status, exec_msg = ruleprofile.run_regular_process()
        del ruleprofile
        
        
        
"""
def daily_monthly_normal_execution_process():
    
    import config_data
    config = config_data.get_config()
    
    if config is not None:
        ruleprofile = RuleProfile(config_data=config)
        ruleprofile.run_regular_process()
        
        del ruleprofile
""" 
    
if __name__ == "__main__":
    daily_monthly_normal_execution_process()
