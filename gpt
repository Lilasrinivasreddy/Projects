import sys
from google.cloud import bigquery
from google.cloud import storage
import pandas as pd
import numpy as np
import json
import pytz
import pandas_gbq
import smtplib
import os
from email.message import EmailMessage
from process_activation_status_notification import *
from reporting_functions import *
from metadata_functions import *

frequency_list = ["real_time", "hourly_custom", "weekly", "monthly", "fixed", "hourly", "daily"]
tables_column_list = ["application_name","db_name","table_name","servername","platform_name","env_name","frequency","is_active","run_day","run_hour","sla","table_process_day","threshold_low","threshold_unit","threshold_up","tool_name","create_dt","notify_table_size","table_size_limit","opsgenie_team","table_size_notification_email_list","alert_table_size","variable_list"]
files_column_list  = ["file_name","file_path","server_name","application_name","platform_name","env_name","comment_text","file_middle_pattern","file_prefix","file_suffix","frequency","instance_ind","is_active","notify_source","run_day","process_owner","sla","source_poc","threshold_low","threshold_unit","threshold_up","create_dt","notify_downstream","downstream_email","interval","file_name_regexp","run_hour","file_arrival_custom_interval","source_system","sla_notification_buffer","notify_sla_misses","opsgenie_team","notify_file_size","file_size_limit","alert_info","timezone","is_mandatory"]
process_column_list  = ["program_name","application_name","process_name","subprocess_name","frequency","is_active","is_critical","platform_name","env_name","process_poc","run_day","run_hour","scheduler_name","sla","create_dt","source","target","source_type","target_type","source_servername","target_servername","process_seq_num","sub_process_seq_num","source_environment","target_environment","interval","step_id","logs_enabled","is_mandatory","run_date_buffer_interval","timezone","logs_trace_id_enabled","volume_query","reason_is_active_change","duration_threshold_low_value","duration_threshold_high_value","collect_volume","businessunit","vsad","portfolio","application","additional_info","gcp_project_name","obs_poc"]
realtime_process_column_list  = ["program_name","application_name","process_name","subprocess_name","cloud_service_type","failure_email","frequency","is_active","is_critical","notify_fails","notify_success","platform_name","env_name","process_poc","run_day","run_hour","scheduler_name","sla","success_email","create_dt","source","target","source_type","target_type","source_servername","target_servername","source_environment","target_environment","is_mandatory","timezone","logs_enabled","label_job_name","label_job_type","label_job_mode","cluster_name","project_id","connection_id","region","custom_metrics"]
process_alert_notification_column_list  = ["application_name","program_name","process_name","platform_name","env_name","sla_buffer","alert_failure","process_failed_alert_priority","alert_sla_misses","process_sla_miss_alert_priority","opsgenie_team","alert_metadata","alert_additional_info","alert_required_columns","notify_success","success_notification_email_list","notify_failure","failure_notification_email_list","notify_sla_miss","sla_miss_notification_email_list","notification_required_columns","notification_additional_info","notify_process_status_change","process_status_change_notification_email_list","dependency_notification","dependency_notification_email_list","alert_pending","process_pending_alert_priority","alert_tool","notification_level","alert_team","retries"]
config__column_list  = ["variable","dev","test","prod"]
frequency_list = ["real_time", "hourly_custom", "weekly", "monthly", "fixed", "hourly", "daily"]
flags = ["y","n"]
bool_flag = ["true","false"]
platform_list = ["gcp","unix","aws","teradata","edl","ca7"]
env_list = ["gcp_bq","unix","java","edw_td","gcp","edl_hdfs","gcp_gcs","onprem_hive","gcp_hive","aws","ca7","vbg_td"]
scheduler_list = ["airflow","composer","espx","cron","oozie","trigger","mainframe"]
businessunit_list = ["vbg","network","vcg","corporate"]
source_type_list = ["pubsub","table","esp","file","kafka","CA"]
target_type_list = ["file","kafka","pubsub","table","topic","esp","CA"]
time_zone_list = ["UTC","EST"]
priority_list = ["P1","P2","P3","P4","P5","critical","high","medium"]
true_flag = ["y","Y"]

def dof_meta_gcs_bq_trigger_api(event, context):
    email_poc = "aid-dof-vzi-notify@verizon.com"
    #email_poc = "nikhila.appidi@verizon.com,vishnu.sarma.konidena@verizon.com,navaneetha.krishnan.radhakrishnan@verizon.com,arun.kumar6@verizon.com"
    try:
        client = bigquery.Client()
        env = "dev"

        #env = os.environ.get("env")
        print(env)
        # name of the file which triggers the function
        file_name = event["name"]
        print("File Name is: " + file_name)
        blob_name = file_name
        storage_client = storage.Client()
        bucket_name = event["bucket"]
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        data_bytes = blob.download_as_bytes()
        process_count = 0
        d24run_hour_count = w24run_hour_count = m24run_hour_count = 0
        d24sla_count = w24sla_count = m24sla_count = 0
        frequency_validation_cnt = total_process_count = is_critical_validation_cnt = 0
        t_cnt,f_cnt,p_cnt,pa_cnt,rt_cnt,cnt = 0,0,0,0,0,0
        env_name_validtion_cnt = duration_threshold_low_value_validation_cnt = duration_threshold_high_value_validation_cnt = 0
        if "tag_file" in file_name.lower():
            print("File Name is: " + file_name)
            tag_df = pd.read_excel(data_bytes, sheet_name="Labels", dtype=str)
            if not tag_df.empty:
                tag_df.dropna(axis=0, how="all", inplace=True)
                pandas_gbq.to_gbq(
                    tag_df,
                    "dataobservability_tbls.dof_tag_file_stg",
                    if_exists="replace",
                )
                print("Data loaded to Tagging staging Table")
                tag_sql = """CALL `dataobservability_tbls.dof_insert_into_tags` ()"""
                load_data(tag_sql)
                print("Load to Tagging tables completed.")
        else:
            config_df = pd.read_excel(data_bytes, sheet_name="Config", dtype=str)
            config_df.dropna(axis=0, how="all", inplace=True)
            config_df = config_df.applymap(str)
            variable_list = list(config_df["variable"])
            dev_list = list(config_df["dev"])
            test_list = list(config_df["test"])
            prod_list = list(config_df["prod"])
            ple_list =  list(config_df["ple"])
            print(variable_list)
            print("-------------------------------------------------")
            print(dev_list)
            print("-------------------------------------------------")
            print(test_list)
            print("-------------------------------------------------")
            print(prod_list)
            print("-------------------------------------------------")
            print(ple_list)
            print("-------------------------------------------------")

            if len(variable_list) != len(dev_list):
                raise ValueError(
                    "Variable column and dev column values are not mapping correctly. Length of either of them has issues. Kindly check the config tab"
                )
            if len(variable_list) != len(test_list):
                raise ValueError(
                    "Variable column and test column values are not mapping correctly. Length of either of them has issues. Kindly check the config tab"
                )
            if len(variable_list) != len(prod_list):
                raise ValueError(
                    "Variable column and prod column values are not mapping correctly. Length of either of them has issues. Kindly check the config tab"
                )
            if len(variable_list) != len(ple_list):
                raise ValueError(
                    "Variable column and prod column values are not mapping correctly. Length of either of them has issues. Kindly check the config tab"
                )
            # Loading to Table staging

            tables_df = pd.read_excel(data_bytes, sheet_name="Tables", dtype=str)
            files_df = pd.read_excel(data_bytes, sheet_name="Files", dtype=str)           
            process_df = pd.read_excel(data_bytes, sheet_name="Process", dtype=str,na_filter = False)
            process_alert_notification_df = pd.read_excel(data_bytes, sheet_name="Process_alert_and_notification", dtype=str)
            realtime_process_df = pd.read_excel(data_bytes, sheet_name="Realtime_Process", dtype=str)

            #email_poc = process_df["obs_poc"].iloc[0]

            #----------------------------------------column_validation-----------------------------------------------
            metadata_tables_columns = list(tables_df.columns.values)
            metadata_files_columns = list(files_df.columns.values)
            metadata_process_columns = list(process_df.columns.values)
            metadata_rt_process_columns = list(realtime_process_df.columns.values)
            metadata_pa_notification_columns = list(process_alert_notification_df.columns.values)

            missed_tbl_col = (set(tables_column_list).difference(metadata_tables_columns))
            missed_fle_col = (set(files_column_list).difference(metadata_files_columns))
            
            missed_prcs_col = (set(process_column_list).difference(metadata_process_columns))
            missed_rt_prcs_col = (set(realtime_process_column_list).difference(metadata_rt_process_columns))
            missed_pa_notify_col = (set(process_alert_notification_column_list).difference(metadata_pa_notification_columns))

            add_tbl_col = (set(metadata_tables_columns).difference(metadata_tables_columns))
            add_fle_col = (set(metadata_files_columns).difference(metadata_files_columns))
            add_prcs_col = (set(metadata_process_columns).difference(metadata_process_columns))
            add_rt_prcs_col = (set(metadata_rt_process_columns).difference(metadata_rt_process_columns))
            add_pa_notify_col = (set(metadata_pa_notification_columns).difference(metadata_pa_notification_columns))

            missing_column_cnt = len(missed_tbl_col) + len(missed_fle_col) + len(missed_prcs_col) + len(missed_rt_prcs_col) + len(missed_pa_notify_col)
            additional_column_cnt = len(add_tbl_col) + len(add_fle_col) + len(add_prcs_col) + len(add_rt_prcs_col) + len(add_pa_notify_col)
            if len(missed_tbl_col)== 0:
                missed_tbl_col = 'NA'
            if len(missed_fle_col)== 0:
                missed_fle_col = 'NA'
            if len(missed_prcs_col)== 0:
                missed_prcs_col = 'NA'
            if len(missed_rt_prcs_col)== 0:
                missed_rt_prcs_col = 'NA'	
            if len(missed_pa_notify_col)== 0:
                missed_pa_notify_col = 'NA'
            if len(add_tbl_col)== 0:
                add_tbl_col = 'NA'
            if len(add_fle_col)== 0:
                add_fle_col = 'NA'
            if len(add_prcs_col)== 0:
                add_prcs_col = 'NA'
            if len(add_rt_prcs_col)== 0:
                add_rt_prcs_col = 'NA'	
            if len(add_pa_notify_col)== 0:
                add_pa_notify_col = 'NA'	
                
#---------------------------------------------------------------------------------------
            if not tables_df.empty:
                tables_df.dropna(axis=0, how="all", inplace=True)
                # Checking the null values in Mandatory Columns
                tbl_nullval_db_name = tables_df[tables_df["db_name"].isna()]
                tbl_nullval_table_name = tables_df[tables_df["table_name"].isna()]
                tbl_nullval_application_name = tables_df[tables_df["application_name"].isna()]
                tbl_nullval_platform_name = tables_df[tables_df["platform_name"].isna()]
                tbl_nullval_env_name = tables_df[tables_df["env_name"].isna()]
                tbl_nullval_servername = tables_df[tables_df["servername"].isna()]

                # Filling the null values in rows as empty

                tables_df = tables_df.fillna("")
                if "dev" in env:
                    tables_df.replace(variable_list, dev_list, inplace=True, regex=True)
                    print(tables_df)
                elif "test" in env:
                    tables_df.replace(variable_list, test_list, inplace=True, regex=True)
                    print(tables_df)
                elif "ple" in env:
                    tables_df.replace(variable_list, ple_list, inplace=True, regex=True)
                    print(tables_df)
                else:
                    tables_df.replace(variable_list, prod_list, inplace=True, regex=True)
                    print(tables_df)
                
                tables_df = tables_df.applymap(str)
                table_stg = 1
                pandas_gbq.to_gbq(tables_df,"dataobservability_tbls.dof_table_meta_stg",if_exists="replace",)
                """
                if len(tbl_nullval_db_name) > 0 or  len(tbl_nullval_table_name) > 0 or len(tbl_nullval_servername) > 0 or len(tbl_nullval_application_name) > 0 or len(tbl_nullval_platform_name) > 0 or len(tbl_nullval_env_name) > 0 :
                    raise ValueError("TABLE META VALIDATION:\n\n Column validation failed counts : \n\tEmpty DB name  : {0}\n\tEmpty table name  : {1}\n\tEmpty server name  : {2}\n\tEmpty application name  : {3}\n\tEmpty platform name  : {4}\n\tEmpty Env name  : {5}".format(	tbl_nullval_db_name,tbl_nullval_table_name,tbl_nullval_servername,tbl_nullval_application_name,tbl_nullval_platform_name,tbl_nullval_env_name))
                """
                #if len(tbl_nullval_db_name) > 0:
                #    print(tbl_nullval_db_name)
                #    raise ValueError("DB Name is a mandatory Column in Tables Tab. It can't be left empty.")
                #if len(tbl_nullval_table_name) > 0:
                #    print(tbl_nullval_table_name)
                #    raise ValueError("Table Name is a mandatory Column in Tables Tab. It can't be left empty.")
                #if len(tbl_nullval_servername) > 0:
                #    print(tbl_nullval_servername)
                #    raise ValueError("Server Name is a mandatory Column in Tables Tab. It can't be left empty.")
                #if len(tbl_nullval_application_name) > 0:
                #    print(tbl_nullval_application_name)
                #    raise ValueError("Application Name is a mandatory Column in Tables Tab. It can't be left empty.")
                #if len(tbl_nullval_platform_name) > 0:
                #    print(tbl_nullval_platform_name)
                #    raise ValueError("Platform Name is a mandatory Column in Tables Tab. It can't be left empty.")
                #if len(tbl_nullval_env_name) > 0:
                #    print(tbl_nullval_env_name)
                #    raise ValueError("Env Name is a mandatory Column in Tables Tab. It can't be left empty.")
                #else:
                #    pandas_gbq.to_gbq(tables_df,"dataobservability_tbls.dof_table_meta_stg",if_exists="replace",)
                #    print("Data loaded to Table staging Table")
            else:
                print("Tables Tab is Empty")
                table_stg = 0
                table_query_job = client.query(""" DELETE from dataobservability_tbls.dof_table_meta_stg where true""")
                print(table_query_job.result())
                # -------------------------------------------------------------------------------------------------------------------------------

            if not files_df.empty:
                files_df.dropna(axis=0, how="all", inplace=True)
                
                f_application_name_validation_cnt=file_name_validation_cnt=f_frequency_validation_cnt=f_platform_name_validtion_cnt=f_env_name_validtion_cnt=f_is_active_validation_cnt=file_path_validation_cnt=f_timezone_validation_cnt=source_poc_validation_cnt=file_prefix_cnt=file_middle_pattern_cnt=file_suffix_cnt=notify_sla_misses_cnt=notify_file_size_cnt=file_size_limit_cnt=f_threshold_low_cnt=f_threshold_up_cnt = 0
                files_df.dropna(axis=0, how="all", inplace=True)
                total_files_count = files_df.shape[0]

                #application name validation
                f_application_name_validation_cnt = files_df.loc[files_df['application_name'].str.contains('nan',case=False), :].shape[0]+files_df.loc[files_df['application_name'].str.contains(r'\s',case=False), :].shape[0]

                #file name validation
                file_name_validation_cnt = files_df['file_name'].isna().sum() +files_df.loc[files_df['file_name'].str.contains(r'\s',case=False), :].shape[0]

                #frequency validation - ["real_time", "hourly_custom", "weekly", "monthly", "fixed", "hourly", "daily"] case sensitive      
                f_frequency_validation_cnt = total_files_count - files_df['frequency'].str.lower().isin(frequency_list).sum()

                #platform_name validation -[GCP,Unix,AWS,Teradata,EDL]
                f_platform_name_validtion_cnt = total_files_count - files_df['platform_name'].str.lower().isin(platform_list).sum()

                #env_name validation -[GCP_BQ,Unix,JAVA,EDW_TD,GCP,EDL_HDFS,GCP_GCS,OnPrem_Hive,GCP_HIVE)]
                f_env_name_validtion_cnt = total_files_count - files_df['env_name'].str.lower().isin(env_list).sum()

                #timezone validation [utc/est]
                f_timezone_validation_cnt = total_files_count - files_df['timezone'].str.lower().isin(["utc","est"]).sum()

                #file path validation
                #file_path_validation_cnt = ((files_df['file_path'].isna()) | (~files_df['file_path'].str.startswith('gs')) | (~files_df['file_path'].str.startswith('/')) | (~files_df['file_path'].str.endswith('/'))).sum()

                #is_active validation
                f_is_active_validation_cnt =  total_files_count - files_df['is_active'].fillna('n').str.lower().isin(flags).sum()

                #source_poc_validation
                null_source_poc_cnt = files_df['source_poc'].isnull().sum()
                source_poc_validation_cnt =  total_files_count -  files_df.loc[files_df['source_poc'].fillna('na').str.contains(r'[^@]+@[^@]+\.[^@]+',case=False), :].shape[0]
                source_poc_validation_cnt = source_poc_validation_cnt - null_source_poc_cnt

                #process_owner validation
                null_process_owner_cnt = files_df['process_owner'].isnull().sum()
                process_owner_validation_cnt =  total_files_count -  files_df.loc[files_df['process_owner'].fillna('na').str.contains(r'[^@]+@[^@]+\.[^@]+',case=False), :].shape[0]
                process_owner_validation_cnt = process_owner_validation_cnt - null_process_owner_cnt
                
                #file_prefix/file_middle_pattern/file_suffix validation
                #file_middle_pattern can be null the condition is file_name = file_prefix + file_middle_pattern + file_suffix
                #file_prefix_cnt = ((files_df['file_middle_pattern'].notna()) & (files_df['file_name'] != files_df['file_prefix'] + files_df['file_middle_pattern'] + files_df['file_suffix'])).sum() + ((files_df['file_middle_pattern'].isna()) & (files_df['file_name'] != files_df['file_prefix'] + files_df['file_suffix'])).sum()
                file_suffix_cnt = file_middle_pattern_cnt = file_prefix_cnt
                
                #notify_sla_misses validation
                notify_sla_misses_cnt = len(files_df['notify_sla_misses'].fillna('n').replace('','n',inplace = False).str.lower().loc[~files_df['notify_sla_misses'].fillna('n').replace('','n',inplace = False).str.lower().isin(flags)])

                #notify_file_size validation
                notify_file_size_cnt =len(files_df['notify_file_size'].fillna('n').replace('','n',inplace = False).str.lower().loc[~files_df['notify_file_size'].fillna('n').replace('','n',inplace = False).str.lower().isin(flags)])

                #file_size_limit validation
                file_size_limit_cnt = len(files_df['file_size_limit'].fillna('n').replace('','n',inplace = False).str.lower().loc[~files_df['file_size_limit'].fillna('n').replace('','n',inplace = False).str.lower().isin(flags)])

                #threshold_low validation
                low_null_count = files_df['threshold_low'].replace('',pd.NA,inplace = False).isna().sum()
                low_numeric_count = files_df['threshold_low'].fillna('').astype(str).apply(lambda x: x.strip().isdigit()).sum()
                f_threshold_low_cnt = total_files_count - low_null_count - low_numeric_count

                #threshold_up validation
                up_null_count = files_df['threshold_up'].replace('',pd.NA,inplace = False).isna().sum()
                up_numeric_count = files_df['threshold_up'].fillna('').astype(str).apply(lambda x: x.strip().isdigit()).sum()
                f_threshold_up_cnt = total_files_count - up_null_count - up_numeric_count

                if  missing_column_cnt > 0 or additional_column_cnt > 0  or  cnt > 0 or f_application_name_validation_cnt > 0 or file_name_validation_cnt > 0 or f_frequency_validation_cnt > 0 or f_platform_name_validtion_cnt > 0 or f_env_name_validtion_cnt > 0 or f_is_active_validation_cnt > 0 or file_prefix_cnt > 0 or source_poc_validation_cnt > 0 or f_threshold_up_cnt > 0 or f_threshold_low_cnt > 0 or notify_file_size_cnt > 0 or notify_sla_misses_cnt > 0 or file_size_limit_cnt > 0:
                    raise ValueError("COMMON VALIDATION:\n\n Missing columns in metadata :\n\tFiles:{0} \n\tTables:{1} \n\tProcess:{2} \n\tRT_process:{3} \n\tProcess_alert_notification: {4} \nAdditional columns in metadata :\n\tFiles:{5} \n\tTables:{6} \n\tProcess:{7} \n\tRT_process:{8} \n\tProcess_alert_notification: {9} \nVariables still present in metadata: \n\tFiles:{10} \n\tTables:{11} \n\tProcess:{12} \n\tRT_process:{13} \n\tProcess_alert_notification: {14}\n\n FILE META VALIDATION:\n\n Column validation failed counts : \n\tapplication_name  : {15} \n\tfile_name  : {16} \n\tfrequency  : {17} \n\tplatform_name_validtion  : {18} \n\tenv_name_validtion  : {19} \n\tis_active  : {20} \n\tfile_path_validation : {21} \n\ttimezone_validation : {22}  \n\tsource_poc_validation : {23} \n\tfile_prefix_validation : {24} \n\tfile_middle_validation : {25} \n\tfile_suffix_validation : {26} \n\tnotify_sla_misses_validation : {27} \n\tnotify_file_size_validation : {28} \n\tfile_size_limit_validation : {29} \n\tthreshold_low_validation : {30} \n\tthreshold_up_validation : {31}".format(missed_fle_col,missed_tbl_col,missed_prcs_col,missed_rt_prcs_col,missed_pa_notify_col,add_fle_col,add_tbl_col,add_prcs_col,add_rt_prcs_col,add_pa_notify_col,f_cnt,t_cnt,p_cnt,rt_cnt,pa_cnt,f_application_name_validation_cnt,file_name_validation_cnt,f_frequency_validation_cnt,f_platform_name_validtion_cnt,f_env_name_validtion_cnt,f_is_active_validation_cnt,file_path_validation_cnt,f_timezone_validation_cnt,source_poc_validation_cnt,file_prefix_cnt,file_middle_pattern_cnt,file_suffix_cnt,notify_sla_misses_cnt,notify_file_size_cnt,file_size_limit_cnt,f_threshold_low_cnt,f_threshold_up_cnt))
                # Checking the null values in Mandatory Columns
                
                file_nullval_file_name = files_df[files_df["file_name"].isna()]
                file_nullval_file_path = files_df[files_df["file_path"].isna()]
                file_nullval_application_name = files_df[
                    files_df["application_name"].isna()
                ]
                file_nullval_platform_name = files_df[files_df["platform_name"].isna()]
                file_nullval_env_name = files_df[files_df["env_name"].isna()]
                file_nullval_server_name = files_df[files_df["server_name"].isna()]

                if "dev" in env:
                    files_df.replace(variable_list, dev_list, inplace=True, regex=True)
                    print(files_df)
                elif "test" in env:
                    files_df.replace(variable_list, test_list, inplace=True, regex=True)
                    print(files_df)
                elif "ple" in env:
                    process_df.replace(variable_list, ple_list, inplace=True, regex=True)
                else:
                    files_df.replace(variable_list, prod_list, inplace=True, regex=True)
                    print(files_df)
                files_df["interval"].fillna("0", inplace=True)
                files_df = files_df.fillna("")
                files_df = files_df.applymap(str)
                file_stg = 1
                if len(file_nullval_file_name) > 0 or len(file_nullval_file_path ) > 0 or len(file_nullval_application_name ) > 0 or len(file_nullval_platform_name ) > 0 or len(file_nullval_env_name ) > 0 or len(file_nullval_server_name ) > 0 :
                    raise ValueError("FILE META VALIDATION\n\n Column validation failed counts : \n\tEmpty File name  : {0}\n\tEmpty File path  : {1}\n\tEmpty application name  : {2}\n\tEmpty platform name  : {3}\n\tEmpty env name  : {4}\n\tEmpty Server name  : {5}".format(file_nullval_file_name,file_nullval_file_path,file_nullval_application_name ,file_nullval_platform_name,file_nullval_env_name ,file_nullval_server_name))	
                #if len(file_nullval_file_name) > 0:
                #    print(file_nullval_file_name)
                #    raise ValueError("File Name is a mandatory Column in Files Tab. It can't be left empty.")
                #if len(file_nullval_file_path) > 0:
                #    print(file_nullval_file_path)
                #    raise ValueError("File Path is a mandatory Column in Files Tab. It can't be left empty.")
                #if len(file_nullval_server_name) > 0:
                #    print(file_nullval_server_name)
                #    raise ValueError("Server Name is a mandatory Column in Files Tab. It can't be left empty.")
                #if len(file_nullval_application_name) > 0:
                #    print(file_nullval_application_name)
                #    raise ValueError("Application Name is a mandatory Column in Files Tab. It can't be left empty.")
                #if len(file_nullval_platform_name) > 0:
                #    print(file_nullval_platform_name)
                #    raise ValueError("Platform Name is a mandatory Column in Files Tab. It can't be left empty.")
                #if len(file_nullval_env_name) > 0:
                #    print(file_nullval_env_name)
                #    raise ValueError("Env Name is a mandatory Column in Files Tab. It can't be left empty.")
                else:
                    pandas_gbq.to_gbq(files_df,"dataobservability_tbls.dof_file_meta_stg",if_exists="replace",)
                    print("Data loaded to File staging Table")
            else:
                print("Files Tab is Empty")
                file_stg = 0
                file_query_job = client.query(
                    """ DELETE from dataobservability_tbls.dof_file_meta_stg where true"""
                )
                print(file_query_job.result())

            # -------------------------------------------------------------------------------------------------------------------------------


            if not process_df.empty:
                process_df.dropna(axis=0, how="all", inplace=True)
                total_process_count = process_df.shape[0]
                obs_poc_cnt = process_df['obs_poc'].isna().sum()
                program_name_validation_cnt = process_df['program_name'].isna().sum()
                application_name_validation_cnt = process_df['application_name'].isna().sum()+process_df.loc[process_df['application_name'].str.strip().str.contains(r'\s',case=False), :].shape[0]
                process_name_validation_cnt = process_df['process_name'].isna().sum() +process_df.loc[process_df['process_name'].str.strip().str.contains(r'\s',case=False), :].shape[0]
                subprocess_name_validation_cnt = process_df['subprocess_name'].isna().sum() +process_df.loc[process_df['subprocess_name'].str.strip().str.contains(r'\s',case=False), :].shape[0]
                process_df["frequency"] = process_df["frequency"].str.lower()
                frequency_validation_cnt = process_df['frequency'].isna().sum() + (total_process_count - process_df['frequency'].str.strip().str.lower().isin(frequency_list).sum())
                process_df['is_critical']= process_df['is_critical'].replace('',"N")
                is_critical_validation_cnt = total_process_count-process_df['is_critical'].fillna('n').str.lower().isin(flags).sum()
                platform_name_validtion_cnt = total_process_count - process_df['platform_name'].fillna('na').str.lower().isin(platform_list).sum()
                env_name_validtion_cnt = total_process_count - process_df['env_name'].fillna('na').str.lower().isin(env_list).sum()
                process_poc_validation_cnt =  total_process_count -  process_df.loc[process_df['process_poc'].fillna('na').str.contains(r'[^@]+@[^@]+\.[^@]+',case=False), :].shape[0]
                scheduler_name_validation_cnt =  total_process_count - process_df['scheduler_name'].fillna('na').str.lower().isin(scheduler_list).sum()
                step_id_validation_cnt =  total_process_count - process_df.loc[process_df['step_id'].fillna('1').str.contains(r'[0-9]',case=False), :].shape[0]
                #process_df['logs_enabled' ]= process_df['logs_enabled'].replace('',"N")
                logs_enabled_validation_cnt =  total_process_count - process_df['logs_enabled'].str.lower().isin(flags).sum()
                process_df['is_mandatory'] = process_df['is_mandatory'].replace('',"N")
                is_mandatory_validation_cnt =  total_process_count - process_df['is_mandatory'].fillna('n').str.lower().isin(flags).sum()
                businessunit_validation_cnt =  total_process_count - process_df['businessunit'].fillna('n').str.lower().isin(businessunit_list).sum()
                vsad_validation_cnt = process_df['vsad'].isna().sum()
                #process_df["timezone"] = process_df["timezone"].fillna("UTC")
                process_df["timezone"] = process_df["timezone"].replace('',"UTC")
                process_df["timezone"] = process_df["timezone"].str.upper()
                timezone_validation_cnt = total_process_count - process_df['timezone'].isin(time_zone_list).sum()
                #process_df_source_filtered = process_df[process_df['source'].notna()]
                process_df_source_filtered = process_df[process_df['source'].str.len() != 0]
                source_type_validation_cnt = process_df_source_filtered.shape[0] - process_df_source_filtered['source_type'].str.lower().isin(source_type_list).sum()
                #process_df_target_filtered = process_df[process_df['target'].notna()]
                process_df_target_filtered = process_df[process_df['target'].str.len() != 0]
                target_type_validation_cnt = process_df_target_filtered.shape[0] - process_df_target_filtered['target_type'].str.lower().isin(target_type_list).sum()
                process_df["logs_trace_id_enabled"] = process_df["logs_trace_id_enabled"].fillna('N')
                logs_trace_id_enabled_validation_cnt =  total_process_count - process_df['logs_trace_id_enabled'].fillna('n').str.lower().isin(flags).sum()
                is_active_validation_cnt =  total_process_count - process_df['is_active'].fillna('n').str.lower().isin(flags).sum()
                process_df_is_active_filtered = process_df[process_df['is_active'].isin(['n','N'])]
                reason_is_active_change_validation_cnt = process_df_is_active_filtered.shape[0] - process_df_is_active_filtered[process_df_is_active_filtered['reason_is_active_change']!='nan'].shape[0]
                process_df['run_date_buffer_interval'] = process_df['run_date_buffer_interval'].replace('',"0")
                run_date_buffer_interval_validation_cnt =  total_process_count -process_df.loc[process_df['run_date_buffer_interval'].fillna('0').str.contains(r'[0-9]',case=False), :].shape[0]              
                process_df['collect_volume']=process_df['collect_volume'].replace('',"false")
                collect_volume_validation_cnt =  total_process_count - process_df['collect_volume'].fillna('false').str.lower().isin(bool_flag).sum()
                process_df_volume_query_filtered = process_df[process_df['collect_volume'].isin(['true','TRUE','True'])] 
                volume_query_validation_cnt = process_df_volume_query_filtered.shape[0] - process_df_volume_query_filtered.loc[~process_df_volume_query_filtered['volume_query'].isna(), 'volume_query'].shape[0]
                #process_df['duration_threshold_low_value'] = process_df['duration_threshold_low_value'].fillna('20')
                #process_df['duration_threshold_high_value'] = process_df['duration_threshold_high_value'].fillna('20')
                process_df['duration_threshold_low_value'] = process_df['duration_threshold_low_value'].replace('',"20")
                process_df['duration_threshold_high_value'] = process_df['duration_threshold_high_value'].replace('',"20")
                duration_threshold_low_value_validation_cnt =  total_process_count - process_df.loc[process_df['duration_threshold_low_value'].fillna('20').str.contains(r'[0-9]',case=False), :].shape[0]
                duration_threshold_high_value_validation_cnt =  total_process_count - process_df.loc[process_df['duration_threshold_high_value'].fillna('20').str.contains(r'[0-9]',case=False), :].shape[0]
                
                process_df['additional_info'] = process_df['additional_info'].replace(['NaN', 'nan', ''], np.nan)
                process_df_additional_info_filtered = process_df.dropna(subset = ['additional_info'])
                
                interval_validation_cnt = 0


                def is_valid_json(json_str):
                    try:
                        json.loads(json_str)
                        return True
                    except json.JSONDecodeError:
                        return False
                    except (ValueError, TypeError):
                        return False

                additional_info_validation_cnt = process_df_additional_info_filtered.shape[0] - process_df_additional_info_filtered.apply(is_valid_json).sum()


                process_df_daily = process_df.loc[(process_df['frequency'].str.lower() =='daily')]
                process_df_weekly = process_df.loc[(process_df['frequency'].str.lower() =='weekly')]
                process_df_monthly = process_df.loc[(process_df['frequency'].str.lower() =='monthly')]
                process_df_hourly = process_df.loc[(process_df['frequency'].str.lower() =='hourly')]
                process_df_custom_hourly = process_df.loc[(process_df['frequency'].str.lower() =='hourly_custom')]
                process_df_fixed = process_df.loc[(process_df['frequency'].str.lower() =='hourly_custom')]

                #interval validation
                hourly_frequencies = ['hourly','fixed','hourly_custom']
                hourly_jobs = process_df[process_df['frequency'].isin(hourly_frequencies)]
                interval_validation_cnt = hourly_jobs['interval'].isna().sum()

                #run_day validation
                daily_run_day = process_df_daily[(process_df_daily['run_day'].str.len() != 7) | (process_df_daily['run_day'].str.contains('0'))]
                #daily_run_day = process_df_daily[(process_df_daily['run_day'].str.len() != 7)]
                fixed_run_day = process_df_fixed[(process_df_fixed['run_day'].str.len() != 7) | (process_df_fixed['run_day'].str.contains('0'))]
                hourly_run_day = process_df_hourly[(process_df_hourly['run_day'].str.len() != 7) | (process_df_hourly['run_day'].str.contains('0'))]
                hourly_custom_run_day = process_df_custom_hourly[(process_df_custom_hourly['run_day'].str.len() != 7) | (process_df_custom_hourly['run_day'].str.contains('0'))]
                weekly_run_day = process_df_weekly[(process_df_weekly['run_day'].str.len() != 7) | (process_df_weekly['run_day'].str.count('1') > 3)]
                monthly_run_day = process_df_monthly[process_df_monthly['run_day'].str.len() > 32] | process_df_monthly[process_df_monthly['run_day'].str.len() < 8]
                
                daily_run_day_cnt = daily_run_day.shape[0]
                fixed_run_day_cnt = fixed_run_day.shape[0]
                hourly_run_day_cnt = hourly_run_day.shape[0]
                hourly_custom_run_day_cnt = hourly_custom_run_day.shape[0]
                weekly_run_day_cnt = weekly_run_day.shape[0]
                monthly_run_day_cnt = monthly_run_day.shape[0]


                daily_count = process_df_daily.shape[0]
                weekly_count = process_df_weekly.shape[0]
                monthly_count = process_df_monthly.shape[0]
                hourly_count = process_df_hourly.shape[0]
                custom_hourly_count = process_df_custom_hourly.shape[0]
                
                sla_cnt = 0
                rh_cnt = 0
                # -----------------------------Process Daily validations-----------------------------------------------------------------------          
                
                d24sla_count=process_df_daily['sla'].str.strip().str.count('24:00:00').sum()
                d24sla_count = d24sla_count +(daily_count -  process_df_daily.loc[process_df_daily['sla'].str.strip().str.contains(r'^\d{2}:\d{2}:\d{2}$',case=False), :].shape[0])
                d24run_hour_count = pd.to_datetime(process_df_daily['run_hour'].str.strip(),format= '%H:%M:%S',  errors='coerce').dt.time.isnull().sum()
                # -----------------------------Process Weekly validations-----------------------------------------------------------------------
                
                w24sla_count=process_df_weekly['sla'].str.strip().str.count('24:00:00').sum()
                w24sla_count = w24sla_count +(weekly_count -  process_df_weekly.loc[process_df_weekly['sla'].str.strip().str.contains(r'^\d{2}:\d{2}:\d{2}$',case=False), :].shape[0])
                w24run_hour_count = pd.to_datetime(process_df_weekly['run_hour'].str.strip(),format= '%H:%M:%S',  errors='coerce').dt.time.isnull().sum()
                # -----------------------------Process Monthly validations-----------------------------------------------------------------------
                
                m24sla_count=process_df_monthly['sla'].str.strip().str.count('24:00:00').sum()
                m24sla_count = m24sla_count +(monthly_count -  process_df_monthly.loc[process_df_monthly['sla'].str.strip().str.contains(r'^[0-9]{2,4}:\d{2}:\d{2}$',case=False), :].shape[0])
                m24run_hour_count = pd.to_datetime(process_df_monthly['run_hour'].str.strip(),format= '%H:%M:%S',  errors='coerce').dt.time.isnull().sum()
  
                # -----------------------------Process Hourly validations-----------------------------------------------------------------------
                h24sla_count=process_df_hourly['sla'].str.strip().str.contains('24:00:00').sum()
                h24run_hour_count=process_df_hourly['run_hour'].str.strip().str.contains('24:00:00').sum()

                # -----------------------------Process custom hourly validations-----------------------------------------------------------------------
                ch24sla_count=process_df_custom_hourly['sla'].str.strip().str.contains('24:00:00').sum()
                ch24run_hour_count=process_df_custom_hourly['run_hour'].str.strip().str.contains('24:00:00').sum()

                # -------------------------------------------------------------------------------------------------------------------------------
                '''
                if program_name_validation_cnt > 0 or application_name_validation_cnt > 0 or process_name_validation_cnt > 0 or subprocess_name_validation_cnt > 0 or frequency_validation_cnt > 0 or is_critical_validation_cnt > 0 or platform_name_validtion_cnt > 0 :
                    raise ValueError("PROCESS META VALIDATION:\n\n Column validation failed counts : \n\tprogram_name  : {0}\n\tapplication_name  : {1} \n\tprocess_name  : {2} \n\tsubprocess_name  : {3} \n\tfrequency  : {4}\n\tis_critical  : {5} \n\tplatform_name_validtion  : {6}".format(program_name_validation_cnt,application_name_validation_cnt,process_name_validation_cnt,subprocess_name_validation_cnt,frequency_validation_cnt,is_critical_validation_cnt,platform_name_validtion_cnt))
                '''
                if  missing_column_cnt > 0 or additional_column_cnt > 0  or  cnt > 0  or d24run_hour_count > 0 or d24sla_count > 0 or w24run_hour_count > 0 or w24sla_count > 0 or m24run_hour_count > 0 or m24sla_count > 0 or program_name_validation_cnt > 0 or application_name_validation_cnt > 0 or process_name_validation_cnt > 0 or subprocess_name_validation_cnt > 0 or frequency_validation_cnt > 0 or is_critical_validation_cnt > 0 or platform_name_validtion_cnt > 0 or env_name_validtion_cnt > 0 or process_poc_validation_cnt > 0 or scheduler_name_validation_cnt > 0 or businessunit_validation_cnt > 0 or vsad_validation_cnt > 0 or source_type_validation_cnt > 0 or target_type_validation_cnt > 0 or is_active_validation_cnt > 0 or reason_is_active_change_validation_cnt > 0 or run_date_buffer_interval_validation_cnt > 0 or collect_volume_validation_cnt > 0 or volume_query_validation_cnt > 0 or duration_threshold_low_value_validation_cnt > 0 or duration_threshold_high_value_validation_cnt > 0 or h24run_hour_count > 0 or h24sla_count > 0 or ch24run_hour_count > 0 or ch24sla_count > 0 or daily_run_day_cnt > 0 or fixed_run_day_cnt > 0 or hourly_custom_run_day_cnt > 0 or hourly_run_day_cnt >0 or weekly_run_day_cnt > 0 or monthly_run_day_cnt > 0 or obs_poc_cnt > 0 or logs_enabled_validation_cnt > 0 or interval_validation_cnt > 0:
                    raise ValueError("COMMON VALIDATION:\n\n Missing columns in metadata :\n\tFiles:{0} \n\tTables:{1} \n\tProcess:{2} \n\tRT_process:{3} \n\tProcess_alert_notification: {4} \nAdditional columns in metadata :\n\tFiles:{5} \n\tTables:{6} \n\tProcess:{7} \n\tRT_process:{8} \n\tProcess_alert_notification: {9} \nVariables still present in metadata: \n\tFiles:{10} \n\tTables:{11} \n\tProcess:{12} \n\tRT_process:{13} \n\tProcess_alert_notification: {14}\n\n PROCESS META VALIDATION:\n\n Column validation failed counts : \n\tprogram_name  : {15}\n\tapplication_name  : {16} \n\tprocess_name  : {17} \n\tsubprocess_name  : {18} \n\tfrequency  : {19}\n\tis_critical  : {20} \n\tplatform_name_validtion  : {21}\n\tenv_name_validtion  : {22}\n\tprocess_poc  : {23}\n\tscheduler_name  : {24}\n\tbusinessunit  : {25}\n\tvsad  : {26}\n\tsource_type  : {27}\n\ttarget_type  : {28}\n\tis_active  : {29}\n\treason_is_active_change  : {30}\n\trun_date_buffer_interval  : {31}\n\tcollect_volume  : {32}\n\tvolume_query  : {33}\n\tduration_threshold_low_value  : {34}\n\tduration_threshold_high_value  : {35}\nDaily process Invalid time entries:\n\trun_hour: {36}\n\tSLA: {37}  \nWeekly process Invalid time entries:\n\trun_hour: {38}\n\tSLA: {39} \nMonthly process Invalid time entries:\n\trun_hour: {40}\n\tSLA: {41}\nHourly process Invalid time entries:\n\trun_hour: {42}\n\tSLA: {43}\nHourly Custom process Invalid time entries:\n\trun_hour: {44}\n\tSLA: {45}\n\tdaily_run_day: {46}\n\tfixed_run_day: {47}\n\thourly_custom_run_day: {48}\n\thourly_run_day: {49}\n\tweekly_run_day: {50}\n\tmonthly_run_day: {51}\n\tobs_poc: {52}\n\tlogs_enabled_validation_cnt: {53}\n\interval_validation_cnt: {54}".format(missed_fle_col,missed_tbl_col,missed_prcs_col,missed_rt_prcs_col,missed_pa_notify_col,add_fle_col,add_tbl_col,add_prcs_col,add_rt_prcs_col,add_pa_notify_col,f_cnt,t_cnt,p_cnt,rt_cnt,pa_cnt,program_name_validation_cnt,application_name_validation_cnt,process_name_validation_cnt,subprocess_name_validation_cnt,frequency_validation_cnt,is_critical_validation_cnt,platform_name_validtion_cnt,env_name_validtion_cnt,process_poc_validation_cnt,scheduler_name_validation_cnt,businessunit_validation_cnt,vsad_validation_cnt,source_type_validation_cnt,target_type_validation_cnt,is_active_validation_cnt,reason_is_active_change_validation_cnt,run_date_buffer_interval_validation_cnt,collect_volume_validation_cnt,volume_query_validation_cnt,duration_threshold_low_value_validation_cnt,duration_threshold_high_value_validation_cnt,d24run_hour_count, d24sla_count,w24run_hour_count, w24sla_count,m24run_hour_count, m24sla_count,h24run_hour_count,h24sla_count,ch24run_hour_count,ch24sla_count,daily_run_day_cnt,fixed_run_day_cnt,hourly_custom_run_day_cnt,hourly_run_day_cnt,weekly_run_day_cnt,monthly_run_day_cnt,obs_poc_cnt,logs_enabled_validation_cnt,interval_validation_cnt))
                
                
                process_nullval_process_name = process_df[
                    process_df["process_name"].isna()
                ]
                process_nullval_subprocess_name = process_df[
                    process_df["subprocess_name"].isna()
                ]
                process_nullval_application_name = process_df[
                    process_df["application_name"].isna()
                ]
                process_nullval_platform_name = process_df[
                    process_df["platform_name"].isna()
                ]
                process_nullval_env_name = process_df[process_df["env_name"].isna()]
                process_nullval_program_name = process_df[
                    process_df["program_name"].isna()
                ]
                dictionary = dict(zip(variable_list, dev_list))
                if "dev" in env:
                    #for col in process_column_list:
                    #    if process_df[col].str.contains('<').any():
                    #        process_df[col]=process_df[col].astype(str).str.replace(dictionary, regex=True)
                    process_df.replace(dictionary, inplace=True, regex=True)
                   #print(process_df)
                elif "test" in env:
                    process_df.replace(variable_list, test_list, inplace=True, regex=True)
                elif "ple" in env:
                    process_df.replace(variable_list, ple_list, inplace=True, regex=True)
                else:
                    process_df.replace(variable_list, prod_list, inplace=True, regex=True)
                
                
                #process_df["interval"].fillna("0", inplace=True)

                # Validate sla and run_hour column in HH:MM:SS format (24 hour format)
                # pd.to_datetime(process_df["sla"], format='%H:%M:%S', errors='raise').dt.time #disabled validation for hourly,fixed for multiple sla timings - rnk
                # pd.to_datetime(process_df["run_hour"], format='%H:%M:%S', errors='raise').dt.time

                # Vaidate is_active and is_critical column value
                #if not process_df['is_active'].isin(['Y','N','y','n']).any():
                #    raise ValueError("is_active value apart from Y, N is not allowed")
                #if not process_df['is_critical'].isin(['Y','N','y','n']).any():
                #    raise ValueError("is_critical value apart from Y, N is not allowed")
                #if not process_df['logs_enabled'].isin(['Y','N','y','n','']).any():
                #    raise ValueError("logs_enabled value apart from Y, N is not allowed")

                # ------------------------------------------------
                # Custom validation started
                # Frequency validation
                #if not process_df['frequency'].isin(frequency_list).all():
                #    raise ValueError(f"frequency value apart from {frequency_list} is not allowed")

                # Timezone validation
                # Replace Null with UTC
                #process_df["timezone"] = process_df["timezone"].fillna("UTC")

                #if not process_df['timezone'].isin(pytz.all_timezones).all():
                #    raise ValueError(f"Invalid timezone value.")

                # duration_threshold_low_value validation
                # Create new df with available values
                #duration_threshold_low_value_df = process_df[process_df["duration_threshold_low_value"].notna()]
                #if not pd.to_numeric(duration_threshold_low_value_df['duration_threshold_low_value'],
                #                     errors='coerce').notna().all():
                #    raise ValueError(f"duration_threshold_low_value have invalid values")

                # duration_threshold_high_value validation
                # Create new df with available values
                #duration_threshold_high_value_df = process_df[process_df["duration_threshold_high_value"].notna()]

                #if not pd.to_numeric(duration_threshold_high_value_df['duration_threshold_high_value'],
                #                     errors='coerce').notna().all():
                #    raise ValueError(f"duration_threshold_high_value have invalid values")
                # ---------------------------- --------------------

                process_df = process_df.fillna("")
                process_df = process_df.applymap(str)
                process_stg = 1

                if len(process_nullval_process_name) > 0:
                    print(process_nullval_process_name)
                    raise ValueError(
                        "Process Name is a mandatory Column in Process Tab. It can't be left empty."
                    )
                if len(process_nullval_subprocess_name) > 0:
                    print(process_nullval_subprocess_name)
                    raise ValueError(
                        "Subprocess Name is a mandatory Column in Process Tab. It can't be left empty."
                    )
                if len(process_nullval_program_name) > 0:
                    print(process_nullval_program_name)
                    raise ValueError(
                        "Server Name is a mandatory Column in Process Tab. It can't be left empty."
                    )
                if len(process_nullval_application_name) > 0:
                    print(process_nullval_application_name)
                    raise ValueError(
                        "Application Name is a mandatory Column in Process Tab. It can't be left empty."
                    )
                if len(process_nullval_platform_name) > 0:
                    print(process_nullval_platform_name)
                    raise ValueError(
                        "Platform Name is a mandatory Column in Process Tab. It can't be left empty."
                    )
                if len(process_nullval_env_name) > 0:
                    print(process_nullval_env_name)
                    raise ValueError(
                        "Env Name is a mandatory Column in Process Tab. It can't be left empty."
                    )
                else:
                    pandas_gbq.to_gbq(
                        process_df,
                        "dataobservability_tbls.dof_process_meta_stg",
                        if_exists="replace",
                    )
                    print("Data loaded to Process staging Table")
            else:
                print("Process Tab is Empty")
                process_stg = 0
                process_query_job = client.query(
                    """ DELETE from dataobservability_tbls.dof_process_meta_stg where true"""
                )
                print(process_query_job.result())
            # -------------------------------------------------------------------------------------------------
            if not process_alert_notification_df.empty:
                if "dev" in env:
                    process_alert_notification_df.replace(variable_list, dev_list, inplace=True, regex=True)
                elif "test" in env:
                    process_alert_notification_df.replace(variable_list, test_list, inplace=True, regex=True)
                elif "ple" in env:
                    process_alert_notification_df.replace(variable_list, ple_list, inplace=True, regex=True)
                else:
                    process_alert_notification_df.replace(variable_list, prod_list, inplace=True, regex=True)
                print('process_alert_notification_df not empty')
                process_alert_notification_df.dropna(axis=0, how="all", inplace=True)
                all_columns = list(process_alert_notification_df) # Creates list of all column headers
                process_alert_notification_df[all_columns] = process_alert_notification_df[all_columns].astype(str)
                total_alert_notification_df_count = process_alert_notification_df.shape[0]
                application_name_validation_count_a= process_alert_notification_df['application_name'].isna().sum()+process_alert_notification_df.loc[process_alert_notification_df['application_name'].str.strip().str.contains(r'\s',case=False), :].shape[0]
                program_name_validation_count_a	 = process_alert_notification_df['program_name'].isna().sum()
                process_name_validation_count_a = process_alert_notification_df['process_name'].isna().sum() +process_alert_notification_df.loc[process_alert_notification_df['process_name'].str.strip().str.contains(r'\s',case=False), :].shape[0]
                platform_name_validation_count_a = total_alert_notification_df_count - process_alert_notification_df['platform_name'].fillna('na').str.lower().isin(platform_list).sum()	
                env_name_validation_count_a = total_alert_notification_df_count - process_alert_notification_df['env_name'].fillna('na').str.lower().isin(env_list).sum()
                duplicate_rows = process_alert_notification_df.duplicated(subset=['application_name', 'program_name','process_name','env_name'])
                process_alert_notification_duplicate_count_a = duplicate_rows.sum()
                alert_failure_validation_count_a =  total_alert_notification_df_count - process_alert_notification_df['alert_failure'].fillna('N').str.lower().isin(flags).sum()
                process_alert_notification_df["process_failed_alert_priority"] = process_alert_notification_df["process_failed_alert_priority"].fillna('P3').replace('',"P3")
                process_failed_alert_priority_validation_count_a = total_alert_notification_df_count  - process_alert_notification_df["process_failed_alert_priority"].isin(priority_list).sum()
                alert_sla_misses_validation_count_a =  total_alert_notification_df_count - process_alert_notification_df['alert_sla_misses'].fillna('N').str.lower().isin(flags).sum()
                process_alert_notification_df["process_sla_miss_alert_priority"] = process_alert_notification_df["process_sla_miss_alert_priority"].fillna("P3").replace('',"P3")
                process_sla_miss_alert_priority_validation_count_a =  total_alert_notification_df_count  - process_alert_notification_df["process_sla_miss_alert_priority"].isin(priority_list).sum()
                alert_pending_validation_count_a =  total_alert_notification_df_count - process_alert_notification_df['alert_pending'].fillna('N').str.lower().isin(flags).sum() 
                process_alert_notification_df["process_pending_alert_priority"] = process_alert_notification_df["process_pending_alert_priority"].fillna("P3").replace('',"P3")
                process_pending_alert_priority_validation_count_a = total_alert_notification_df_count  - process_alert_notification_df["process_pending_alert_priority"].isin(priority_list).sum()
                if (application_name_validation_count_a > 0 or program_name_validation_count_a > 0 or process_name_validation_count_a > 0 or platform_name_validation_count_a > 0 or process_alert_notification_duplicate_count_a > 0 or alert_failure_validation_count_a > 0 or process_failed_alert_priority_validation_count_a > 0 or alert_sla_misses_validation_count_a > 0 or process_sla_miss_alert_priority_validation_count_a > 0 or alert_pending_validation_count_a > 0 or process_pending_alert_priority_validation_count_a > 0):
                    raise ValueError("ALERT NOTIFICATION META VALIDATION:\n\n\tapplication_name_validation_count : {0}\n\tprogram_name_validation_count : {1}\n\trocess_name_validation_count : {2}\n\tplatform_name_validation_count : {3}\n\tenv_name_validation_count : {4},\n\tDuplicate_row_validation_count : {5},\n\talert_failure_validation_count : {6}\n\tprocess_failed_alert_priority_validation_count : {7}\n\talert_sla_misses_validation_count : {8}\n\tprocess_sla_miss_alert_priority_validation_count : {9}\n\talert_pending_validation_count : {10}\n\tprocess_pending_alert_priority_validation_count : {11}\n\t".format(application_name_validation_count_a,program_name_validation_count_a,process_name_validation_count_a,platform_name_validation_count_a,env_name_validation_count_a,process_alert_notification_duplicate_count_a,alert_failure_validation_count_a,process_failed_alert_priority_validation_count_a,alert_sla_misses_validation_count_a,process_sla_miss_alert_priority_validation_count_a,alert_pending_validation_count_a,process_pending_alert_priority_validation_count_a))
                process_alert_stg = 1
                pandas_gbq.to_gbq(process_alert_notification_df,"dataobservability_tbls.dof_process_alert_notification_meta_stg",if_exists="replace",)
                
                
                # process_df["interval"].fillna( '0', inplace = True)
                #process_alert_notification_df = process_alert_notification_df.fillna("")
                #process_alert_notification_df = process_alert_notification_df.applymap(str)
                #if len(process_alert_nullval_process_name) > 0:
                    #raise ValueError("Process Name is a mandatory Column in Process Alert and Notification Tab. It can't be left empty.")
                #if len(process_alert_nullval_program_name) > 0:
                    #raise ValueError("program_name is a mandatory Column in Process Alert and Notification Tab. It can't be left empty.")
                #if len(process_alert_nullval_application_name) > 0:
                    #raise ValueError("Application Name is a mandatory Column in Process Alert and Notification Tab. It can't be left empty.")
                #if len(process_alert_nullval_platform_name) > 0:
                    #raise ValueError("Platform Name is a mandatory Column in Process Alert and Notification Tab. It can't be left empty.")
                #if len(process_alert_nullval_env_name) > 0:
                    #raise ValueError( "Env Name is a mandatory Column in Process Alert and Notification Tab. It can't be left empty.")
                
                print("Data loaded to Process Alert and Notification staging Table")
            else:
                print("Process Alert and Notification Tab is Empty")
                process_alert_stg = 0
                process_alert_query_job = client.query(
                    """ DELETE from dataobservability_tbls.dof_process_alert_notification_meta_stg where true"""
                )
                print(process_alert_query_job.result())
            # -------------------------------------------------------------------------------------------------------------------------------#

            process_alert_sql = """CALL `dataobservability_tbls.dof_upsert_into_process_alert_notification_meta` ()"""
            print(
                "Starting to load staging data to process alert and Notification metadata."
            )
            if process_alert_stg == 1:
                load_data(process_alert_sql)
                print("Load to process alert and notification base table completed.")
                # process_count += 1
            else:
                print(
                    "Procedure to update Process_alert_notification_meta will not run!"
                )
                # process_count += 1
            print("-----------------------------------------------------")
            print("-----------------------------------------------------")

            #is_active_change_query = """select stg.process_name as process_name, stg.application_name as application_name, stg.program_name as program_name, stg.is_active as is_active,stg.platform_name as platform_name, stg.subprocess_name as subprocess_name, stg.reason_is_active_change as reason from dataobservability_tbls.dof_process_meta meta join dataobservability_tbls.dof_process_meta_stg stg on stg.program_name  = meta.program_name and stg.application_name= meta.application_name and stg.process_name = meta.process_name and stg.subprocess_name = meta.subprocess_name and stg.platform_name = meta.platform_name and stg.env_name = meta.env_name and stg.is_active != meta.is_active"""
            is_active_change_query = """SELECT
                                            application_name,
                                            program_name,
                                            process_name,
                                            is_active,
                                            ARRAY_AGG(subprocess_name) AS subprocess_names,
                                            ARRAY_AGG(IFNULL(reason, '')) AS reasons
                                            FROM (
                                            SELECT
                                                stg.process_name AS process_name,
                                                stg.application_name AS application_name,
                                                stg.program_name AS program_name,
                                                stg.is_active AS is_active,
                                                stg.subprocess_name AS subprocess_name,
                                                stg.reason_is_active_change AS reason
                                            FROM
                                                dataobservability_tbls.dof_process_meta meta
                                            JOIN
                                                dataobservability_tbls.dof_process_meta_stg stg
                                            ON
                                                stg.program_name = meta.program_name
                                                AND stg.application_name= meta.application_name
                                                AND stg.process_name = meta.process_name
                                                AND stg.subprocess_name = meta.subprocess_name
                                                AND stg.env_name = meta.env_name
                                                AND stg.is_active != meta.is_active)
                                            GROUP BY
                                            process_name,
                                            application_name,
                                            program_name,
                                            is_active"""
            is_active_change_rows = select_query(is_active_change_query)
            for row in is_active_change_rows:
                print(
                    row.get('process_name'),
                    row.get('application_name'),
                    row.get('program_name'),
                    row.get('is_active'),
                    row.get('subprocess_names'),
                    row.get('reasons'),
                )
                send_run_state_notification(
                    row.get('process_name'),
                    row.get('application_name'),
                    row.get('program_name'),
                    row.get('is_active'),
                    row.get('subprocess_names'),
                    row.get('reasons'),
                )

            # -------------------------------------------------------------------------------------------------------------------------------

            
            if not realtime_process_df.empty:
                realtime_process_df.dropna(axis=0, how="all", inplace=True)

                realtime_process_nullval_process_name = realtime_process_df[
                    realtime_process_df["process_name"].isna()
                ]
                realtime_process_nullval_subprocess_name = realtime_process_df[
                    realtime_process_df["subprocess_name"].isna()
                ]
                realtime_process_nullval_application_name = realtime_process_df[
                    realtime_process_df["application_name"].isna()
                ]
                realtime_process_nullval_platform_name = realtime_process_df[
                    realtime_process_df["platform_name"].isna()
                ]
                realtime_process_nullval_env_name = realtime_process_df[
                    realtime_process_df["env_name"].isna()
                ]
                realtime_process_nullval_program_name = realtime_process_df[
                    realtime_process_df["program_name"].isna()
                ]

                if "dev" in env:
                    realtime_process_df.replace(
                        variable_list, dev_list, inplace=True, regex=True
                    )
                    print(realtime_process_df)
                elif "test" in env:
                    realtime_process_df.replace(variable_list, test_list, inplace=True, regex=True)
                    print(realtime_process_df)
                elif "ple" in env:
                    realtime_process_df.replace(variable_list, ple_list, inplace=True, regex=True)
                
                else:
                    realtime_process_df.replace(
                        variable_list, prod_list, inplace=True, regex=True
                    )
                    print(realtime_process_df)
                realtime_process_df = realtime_process_df.fillna("")
                realtime_process_df = realtime_process_df.applymap(str)
                realtime_process_stg = 1

                if len(realtime_process_nullval_process_name) > 0:
                    print(realtime_process_nullval_process_name)
                    raise ValueError(
                        "Process Name is a mandatory Column in Realtime Process Tab. It can't be left empty."
                    )
                if len(realtime_process_nullval_subprocess_name) > 0:
                    print(realtime_process_nullval_subprocess_name)
                    raise ValueError(
                        "Subprocess Name is a mandatory Column in Realtime Process Tab. It can't be left empty."
                    )
                if len(realtime_process_nullval_program_name) > 0:
                    print(realtime_process_nullval_program_name)
                    raise ValueError(
                        "program_name is a mandatory Column in Realtime Process Tab. It can't be left empty."
                    )
                if len(realtime_process_nullval_application_name) > 0:
                    print(realtime_process_nullval_application_name)
                    raise ValueError(
                        "Application Name is a mandatory Column in Realtime Process Tab. It can't be left empty."
                    )
                if len(realtime_process_nullval_platform_name) > 0:
                    print(realtime_process_nullval_platform_name)
                    raise ValueError(
                        "Platform Name is a mandatory Column in Realtime Process Tab. It can't be left empty."
                    )
                if len(realtime_process_nullval_env_name) > 0:
                    print(realtime_process_nullval_env_name)
                    raise ValueError(
                        "Env Name is a mandatory Column in Realtime Process Tab. It can't be left empty."
                    )
                else:
                    pandas_gbq.to_gbq(
                        realtime_process_df,
                        "dataobservability_tbls.dof_realtime_process_meta_stg",
                        if_exists="replace",
                    )
                    print("Data loaded to Realtime Process staging Table")
            else:
                print("Realtime Process Tab is Empty")
                realtime_process_stg = 0
                realtime_process_query_job = client.query(
                    """ DELETE from dataobservability_tbls.dof_realtime_process_meta_stg where true"""
                )
                print(realtime_process_query_job.result())
            
            for col in tables_column_list:
                if tables_df[col].str.contains('<').any():
                    t_cnt = t_cnt+1
            for col in files_column_list:
                if files_df[col].str.contains('<').any():
                    f_cnt = f_cnt+1
            for col in process_column_list:
                if process_df[col].str.contains('<').any():
                    p_cnt = p_cnt+1
            for col in process_alert_notification_column_list:
                if process_alert_notification_df[col].str.contains('<').any():
                    pa_cnt = pa_cnt+1
            for col in realtime_process_column_list:
                if realtime_process_df[col].str.contains('<').any():
                    rt_cnt = rt_cnt+1
            cnt = t_cnt+f_cnt+p_cnt+pa_cnt+rt_cnt
            if missing_column_cnt > 0 or additional_column_cnt > 0  or  cnt > 0  or d24run_hour_count > 0 or d24sla_count > 0 or w24run_hour_count > 0 or w24sla_count > 0 or m24run_hour_count > 0 or m24sla_count > 0 or h24run_hour_count > 0 or h24sla_count > 0 or ch24run_hour_count > 0 or ch24sla_count > 0 :
                    raise ValueError("COMMON VALIDATION:\n\n Missing columns in metadata :\n\tFiles:{0} \n\tTables:{1} \n\tProcess:{2} \n\tRT_process:{3} \n\tProcess_alert_notification: {4} \nAdditional columns in metadata :\n\tFiles:{5} \n\tTables:{6} \n\tProcess:{7} \n\tRT_process:{8} \n\tProcess_alert_notification: {9} \nVariables still present in metadata: \n\tFiles:{10} \n\tTables:{11} \n\tProcess:{12} \n\tRT_process:{13} \n\tProcess_alert_notification: {14}\n\n \nDaily process Invalid time entries:\n\trun_hour: {15}\n\tSLA:{16}\nWeekly process Invalid time entries:\n\trun_hour: {17}\n\tSLA: {18} \nMonthly process Invalid time entries:\n\trun_hour: {19}\n\tSLA: {20}\nHourly process Invalid time entries:\n\trun_hour: {21}\n\tSLA: {22}\nHourly Custom process Invalid time entries:\n\trun_hour: {23}\n\tSLA: {24}".format(missed_fle_col,missed_tbl_col,missed_prcs_col,missed_rt_prcs_col,missed_pa_notify_col,add_fle_col,add_tbl_col,add_prcs_col,add_rt_prcs_col,add_pa_notify_col,f_cnt,t_cnt,p_cnt,rt_cnt,pa_cnt,d24run_hour_count, d24sla_count,w24run_hour_count, w24sla_count,m24run_hour_count, m24sla_count,h24run_hour_count,h24sla_count,ch24run_hour_count,ch24sla_count))
            #if missing_column_cnt > 0 or additional_column_cnt > 0  or  cnt > 0  or d24run_hour_count > 0 or d24sla_count > 0 or w24run_hour_count > 0 or w24sla_count > 0 or m24run_hour_count > 0 or m24sla_count > 0:
             #   raise ValueError("\nMissing columns in metadata :\n\tFiles:{0} \n\tTables:{1} \n\tProcess:{2} \n\tRT_process:{3} \n\tProcess_alert_notification: {4} \nAdditional columns in metadata :\n\tFiles:{5} \n\tTables:{6} \n\tProcess:{7} \n\tRT_process:{8} \n\tProcess_alert_notification: {9} \nVariables still present in metadata: \n\tFiles:{10} \n\tTables:{11} \n\tProcess:{12} \n\tRT_process:{13} \n\tProcess_alert_notification: {14} \nDaily process Invalid time entries:\n\trun_hour: {15}\n\tSLA: {16}  \nWeekly process Invalid time entries:\n\trun_hour: {17}\n\tSLA: {18} \nMonthly process Invalid time entries:\n\trun_hour: {19}\n\tSLA: {20}".format(missed_fle_col,missed_tbl_col,missed_prcs_col,missed_rt_prcs_col,missed_pa_notify_col,add_fle_col,add_tbl_col,add_prcs_col,add_rt_prcs_col,add_pa_notify_col,f_cnt,t_cnt,p_cnt,rt_cnt,pa_cnt,d24run_hour_count, d24sla_count,w24run_hour_count, w24sla_count,m24run_hour_count, m24sla_count))
	
            file_sql = """CALL `dataobservability_tbls.dof_upsert_into_file_meta` ()"""
            table_sql = (
                """CALL `dataobservability_tbls.dof_upsert_into_table_meta` ()"""
            )
            process_sql = (
                """CALL `dataobservability_tbls.dof_upsert_into_process_meta` ()"""
            )
            lineage_sql = """CALL `dataobservability_tbls.dof_upsert_into_entity_relation_meta` ()"""

            realtime_process_sql = """CALL `dataobservability_tbls.dof_upsert_into_realtime_process_meta` ()"""

            print("-----------------------------------------------------")
            print("Starting to load staging data to file metadata.")
            if file_stg == 1:
                load_data(file_sql)
                print("Load to files base table completed.")
                process_count += 1
            else:
                print("Procedure to update file_meta will not run!")
                process_count += 1
            print("-----------------------------------------------------")
            print("-----------------------------------------------------")

            print("Starting to load staging data to table metadata.")
            if table_stg == 1:
                load_data(table_sql)
                print("Load to Tables base table completed.")
                process_count += 1
            else:
                print("Procedure to update Table_meta will not run!")
                process_count += 1
            print("-----------------------------------------------------")
            print("-----------------------------------------------------")

            print("Starting to load staging data to process metadata.")
            if process_stg == 1:
                load_data(process_sql)
                print("Load to process base table completed.")
                process_count += 1
            else:
                print("Procedure to update Process_meta will not run!")
                process_count += 1
            print("-----------------------------------------------------")
            print("-----------------------------------------------------")

            print("Starting to load staging data to Lineage metadata.")
            load_data(lineage_sql)
            print("Load to Lineage base table completed.")
            process_count += 1
            print("-----------------------------------------------------")
            print("-----------------------------------------------------")

            print("Starting to load staging data to realtime process metadata.")
            if realtime_process_stg == 1:
                load_data(realtime_process_sql)
                print("Load to process base table completed.")
                process_count += 1
            else:
                print("Procedure to update realtime Process_meta will not run!")
                process_count += 1
            print("-----------------------------------------------------")
            print("-----------------------------------------------------")


            # calling process monitor and file monitor's hourly instance process and weekly,monthly split process
            print("The process_monitor process: ")
          
            #below select the process_id from process_meta which is not present in process_hour
            process_hourly_fixed_query = """
            select process_id,process_name,subprocess_name as sub_process,`interval` as interval_time,run_hour,coalesce(run_hour_end,'23:59:59') as run_hour_end,sla,frequency from dataobservability_tbls.dof_process_meta where frequency in ('fixed','hourly') and `interval` is not null and `interval` != 0 and concat(process_id,run_hour,sla) not in (select distinct concat(process_id,run_hour,sla_time) from dataobservability_tbls.dof_process_hour )
             """
            print("The process_hourly_fixed_query data:", process_hourly_fixed_query)
            process_hourly_fixed_rows = select_query(process_hourly_fixed_query)
            process_monitor_process_rows(process_hourly_fixed_rows)
            print("The process_hourly_fixed_query completed...")

            process_hourly_custom_query = """select process_id,process_name,subprocess_name as sub_process,run_hour,sla,`interval` as interval_time, frequency from dataobservability_tbls.dof_process_meta where frequency='hourly_custom' and concat(process_id,run_hour,sla) not in (select distinct concat(process_id,run_hour,sla_time) from dataobservability_tbls.dof_process_hour)"""
            print("The process_hourly_custom_query data:", process_hourly_custom_query)
            process_hourly_custom_rows = select_query(process_hourly_custom_query) 
            process_monitor_process_rows(process_hourly_custom_rows)
            print("The process_hourly_custom_query completed...")

            print("delete old instances of hourly and fixed start")
            #below delete query will delete the old instances from the dof_process_hour
            #to avoid the buffer stream error added the 90 min condition in query
            process_hourly_delete_query = """
            delete from `dataobservability_tbls.dof_process_hour` where concat(process_id,insert_timestamp) in 
            (select process from
            (select concat(process_id,insert_timestamp) as process,dense_rank() over (partition by process_id order by TIMESTAMP_TRUNC(insert_timestamp, minute) desc) rank1 from `dataobservability_tbls.dof_process_hour` where insert_timestamp < TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 90 MINUTE) qualify rank1 != 1) )
            """
            select_query(process_hourly_delete_query)
            print("delete process old instances of hourly and fixed end")
            #below process_weekly_monthly_query will return the records present in dof_process_meta and not present in dof_process_meta_split

            process_weekly_monthly_query = """select process_id,program_name,process_name,subprocess_name,run_day,frequency from dataobservability_tbls.dof_process_meta where frequency in ('weekly','monthly') and concat(process_id,run_day) not in (select distinct concat(process_id,coalesce(actual_run_day,'0000000')) from dataobservability_tbls.dof_process_meta_split)"""
            print("The process_weekly_monthly_query query data::",process_weekly_monthly_query)
            process_weekly_monthly_rows = select_query(process_weekly_monthly_query)
            #from google.oauth2 import service_account
            #try :
            #    client = bigquery.Client()#(credentials= credentials,project=project_id)
            #    query_job = client.query(process_weekly_monthly_query)
            #    process_weekly_monthly_rows = query_job.result()
            #except Exception as e:
            #    print(e)
            process_dof_process_meta_split_rows(process_weekly_monthly_rows)
            print("The process_weekly_monthly_query query completed...")
            
            
            #below delete query will delete the old instances from the dof_process_meta_split
            #to avoid the buffer stream error added the 90 min condition in query
            process_hourly_delete_query = """
            delete from `dataobservability_tbls.dof_process_meta_split` where concat(process_id,coalesce(actual_run_day,'0000000')) in 
            (select process from
            (select concat(process_id,coalesce(actual_run_day,'0000000')) as process,dense_rank() over (partition by process_id order by TIMESTAMP_TRUNC(insert_timestamp, minute) desc) rank1 from `dataobservability_tbls.dof_process_meta_split`  where insert_timestamp < TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 90 MINUTE) qualify rank1 != 1) )
            """
            print("The process_hourly_delete_query query completed...")
            print("The process_monitor completed...")

            print(
                "==================================================================================================="
            )
            print("The file_monitor: ")
            file_hourly_fixed_query = """select file_id,file_name,`interval` as interval_time,run_hour,sla,frequency from dataobservability_tbls.dof_file_meta where frequency in ('fixed','hourly') and `interval` is not null and `interval` != 0 and concat(file_id,run_hour,sla) not in (select distinct concat(file_id,run_hour,sla_time) from dataobservability_tbls.dof_file_hour)"""
            print("The file_hourly_fixed_query data:", file_hourly_fixed_query)
            file_hourly_fixed_rows = select_query(file_hourly_fixed_query)
            process_dof_file_hour_rows(file_hourly_fixed_rows)
            print("The file_hourly_fixed_query query completed...")

            file_hourly_custom_query = """select file_id,file_name,`interval` as interval_time,run_hour,sla,frequency from dataobservability_tbls.dof_file_meta where frequency='hourly_custom' and concat(file_id,run_hour,sla) not in (select distinct concat(file_id,run_hour,sla_time) from dataobservability_tbls.dof_file_hour)"""
            print("The file_hourly_custom_query query data:", file_hourly_custom_query)
            file_hourly_custom_rows = select_query(file_hourly_custom_query)
            process_dof_file_hour_rows(file_hourly_custom_rows)
            print("The file_hourly_custom_query query completed...")         

            file_weekly_monthly_query = """select file_id,file_name,process_day,frequency from dataobservability_tbls.dof_file_meta where frequency in ('weekly','monthly') and concat(file_id,process_day) not in (select distinct concat(file_id,coalesce(actual_process_day,'0000000')) from dataobservability_tbls.dof_file_meta_split)"""
            print("The file_weekly_monthly_query query data:", file_weekly_monthly_query)
            file_weekly_monthly_rows = select_query(file_weekly_monthly_query)
            process_dof_file_meta_split_rows(file_weekly_monthly_rows)
            print("The file_weekly_monthly_query query completed...")
            
            #below delete query will delete the old instances from the dof_file_hour
            #to avoid the buffer stream error added the 90 min condition in query
            file_hourly_delete_query = """
            delete from `dataobservability_tbls.dof_file_hour` where concat(file_id,insert_timestamp) in 
            (select process from
            (select concat(file_id,insert_timestamp) as process,dense_rank() over (partition by file_id order by TIMESTAMP_TRUNC(insert_timestamp, minute) desc) rank1 from `dataobservability_tbls.dof_file_hour` where insert_timestamp < TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 90 MINUTE) qualify rank1 != 1) )
            """
            select_query(file_hourly_delete_query)
            print("delete file old instances of hourly and fixed end")
            
            
            print("The file_monitor completed...")

            # Send Email for success

            if process_count < 4:
                raise ValueError(
                    "Someting in Pipeline Went Wrong. Some base table hasn't been loaded."
                )
            else:
                msg = EmailMessage()
                success = "FILE NAME: " + file_name + " Load Successful!"
                msg.set_content(success)
                msg["Subject"] = "(" + env + ") Metadata Pipeline "+file_name+" load Success!"
                msg["From"] = "do-not-reply@verizon.com"
                msg["To"] = "aid-dof-vzi-notify@verizon.com"
                #msg["To"] = email_poc
                smtpObj = smtplib.SMTP("vzsmtp.verizon.com", 25)
                smtpObj.starttls()
                smtpObj.send_message(msg)
                print("Successfully sent email for Success!")
                smtpObj.quit()
    except Exception as e:
        exc_type, exc_obj, exc_tb = sys.exc_info()
        fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]
        error = f"Error Occurred in FILE: {file_name}\n ERROR: {str(e)}, \n Python File Name: {fname} \n Line Number: {exc_tb.tb_lineno}"
        print(error)
        msg = EmailMessage()
        msg.set_content(error)
        msg["Subject"] = "(" + env + ") Metadata validation failed for file : "+file_name+", Error occured in Metadata Pipeline!"
        msg["From"] = "do-not-reply@verizon.com"
        msg["To"] = "aid-dof-vzi-notify@verizon.com"
        try:
            smtpObj = smtplib.SMTP("vzsmtp.verizon.com", 25)
            smtpObj.starttls()
            smtpObj.send_message(msg)
            print("Successfully sent email for Error!")
            smtpObj.quit()
        except SMTPException:
            print("Error: unable to send email")
--------------------------------------------------------------------------------------------------------------------------------------------------------------

import pandas as pd
import sys
import os
import re
import json
import numpy as np
import pytz
import pandas_gbq
import smtplib
from google.cloud import bigquery
from email.message import EmailMessage
from google.cloud import storage
# from croniter import croniter
from datetime import datetime, timedelta
import config
from metadata_functions import *

# def load_table(tables_meta,table_id):
#     job_config=bigquery.LoadJobConfig (autodetect=False,write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE)
#     job=client.load_table_from_dataframe(tables_meta,table_id,job_config=job_config)
#     return job.result()

client=bigquery.Client()
storage_client = storage.Client()
project_id= config.project_id
dataset=config.dataset
bucket_name = config.bucket_name
process_table_id=f"{project_id}.{dataset}.datax_process_meta_stg"
task_table_id=f"{project_id}.{dataset}.datax_task_meta_stg"
files_table_id=f"{project_id}.{dataset}.datax_file_meta_stg"
tables_table_id=f"{project_id}.{dataset}.datax_table_meta_stg"
finops_table_id=f"{project_id}.{dataset}.datax_label_meta_stg"


def dof_master_meta_gcs_bq_trigger_api(event,context):
    try:
        env= "dev"
        # env = os.environ.get("env")
        print(env)
        # name of the file which triggers the function
        file_name = event["name"]
        print("File Name is: " + file_name)
        blob_name = file_name
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        data_bytes = blob.download_as_bytes()
        process_count = 0

        tables_meta = pd.read_excel(data_bytes, sheet_name="DataX_Table_Meta",keep_default_na=False,dtype=str,index_col=None)
        files_meta = pd.read_excel(data_bytes, sheet_name="DataX_File_Meta",keep_default_na=False,dtype=str,index_col=None)
        process_meta = pd.read_excel(data_bytes, sheet_name="DataX_Process_Meta",keep_default_na=False,dtype=str,index_col=None)
        task_meta = pd.read_excel(data_bytes, sheet_name="DataX_Task_Meta",keep_default_na=False,dtype=str,index_col=None)
        finops_meta = pd.read_excel(data_bytes, sheet_name="DataX_Label_Meta",keep_default_na=False,dtype=str,index_col=None)

        # Drop first column
        tables_meta=tables_meta.iloc[:,1:]
        files_meta=files_meta.iloc[:,1:]
        process_meta=process_meta.iloc[:,1:]
        task_meta=task_meta.iloc[:,1:]
        finops_meta=finops_meta.iloc[:,1:]

        # Drop row which contains description
        process_meta=process_meta.reset_index(drop=True)
        process_meta=process_meta.drop(index=[0,1])
        task_meta=task_meta.reset_index(drop=True)
        task_meta=task_meta.drop(index=[0,1])
        files_meta=files_meta.reset_index(drop=True)
        files_meta=files_meta.drop(index=[0,1])
        tables_meta=tables_meta.reset_index(drop=True)
        tables_meta=tables_meta.drop(index=[0,1])
        finops_meta=finops_meta.reset_index(drop=True)
        finops_meta=finops_meta.drop(index=[0,1])

        tables_column_list=['dag_id','task_id','composer_instance_name','step_id','table_name','db_name','server_name','platform_name','environment_name','is_active','notify_table_size','threshold_unit','threshold_low','threshold_up','volume_incident_team_name','table_size_notification_email_list','alert_table_size','volume_alert_channel','volume_query','montior_volume']
        files_column_list=['dag_id','composer_instance_name','task_id','step_id','file_name','file_path','landing_server_name','direction','environment_name','file_middle_pattern','file_prefix','file_suffix','schedule_interval','frequency','timezone','is_mandatory','is_active','file_arrival_custom_interval','sla','source_system','source_poc','sla_notification_buffer','notify_source','notify_sla_misses','alert_channel','alert_target_team_name','notify_file_size','size_threshold_low_limit','size_threshold_high_limit','size_threshold_limit_unit','interval','archive_locaton','failure_location','sourceFormat','sourceUris','fixedWidth','fieldDelimiter','skipLeadingRows','maxBadRecords','nullMarker','allowQuotedNewlines','allowJaggedRows','ignoreUnknownValues','filesensorretries','filesensorretrydelay','timeout','fileSensor','gcsBucket','object']
        process_column_list= ['process_name','process_description','dag_id','project_id','composer_instance_name','vsad','project_space','job_type','data_stream','load_type','platform_name','job_url','scheduler_name','timezone','schedule_interval','frequency','max_active_runs','sla','interval','tracex_tags','run_date_buffer_interval','sla_alert_buffer','duration_threshold_high_value','alert_channel','alert_channal_target_name','alert_additional_info','alert_failure_by','alert_priority','alert_failure','alert_slamiss','alert_longrunning','alert_startoverdue','is_critical','logs_trace_id_enabled','sla_type','code_gcs_location','code_git_link','prod_request_number','production_date','notify_email_success','notify_email_failure','notify_email_slamiss','notify_email_longrunning','notify_email_startoverdue','notify_slack_success','notify_slack_failure','notify_slack_slamiss','notify_slack_longrunning','notify_slack_startoverdue','dev_poc','dev_poc_manager','devgroup_email','mod_poc','legacy_esp_application_name','legacy_esp_process_name','runbook_link','logs_enabled','upstream','downstream','additional_details_json']
        task_column_list=['dag_id','composer_instance_name','task_id','description','step_id','task_type','platform_name','env_name','source_type','source_environment','source_server_name','source','rt_subscription','target_type','target_environment','target_server_name','target','retries','retry_delay','is_active','is_mandatory']
        finops_column_list=['job_id','instance_name','job_engine','vsad','project_id','lob','sub_lob','portfolio','product_area','product_name','program_name','data_product_name','data_domain','data_sub_domain','use_case_name','is_active','updated_timestamp','inserted_timestamp','modified_by']
        config__column_list  = ["variable","dev","test","prod"]

        process_integer_column=['duration_threshold_high_value','max_active_runs','run_date_buffer_interval','interval']
        task_integer_column=['retries','step_id']
        files_integer_column=['step_id','file_arrival_custom_interval','size_threshold_low_limit','size_threshold_high_limit','interval']
        table_integer_column=['step_id','threshold_up','threshold_low']
        finops_integer_column=[]
        process_boolean_column=['alert_failure','alert_slamiss','is_critical','logs_enabled','logs_trace_id_enabled']
        table_boolean_column=['notify_table_size','is_active','alert_table_size']
        task_boolean_column=['is_mandatory','is_active']
        finops_boolean_column=['is_active']
        files_boolean_column=['is_mandatory','is_active','notify_source','notify_sla_misses','notify_file_size']

        tables_mandatory_column_list=["dag_id","task_id","step_id","table_name","db_name","server_name","platform_name","is_active"]
        task_mandatory_column_list=["dag_id","composer_instance_name","task_id","description","task_type","platform_name","env_name","source_type","source_environment","source_server_name"]
        process_mandatory_column_list=['process_name','process_description','dag_id','project_id','composer_instance_name','vsad','project_space','job_type','data_stream','load_type','platform_name','job_url','scheduler_name','timezone','schedule_interval','frequency','sla','is_critical','sla_type','code_gcs_location','code_git_link','prod_request_number','production_date','dev_poc','dev_poc_manager','devgroup_email','mod_poc','runbook_link']
        files_mandatory_column_list=['dag_id','composer_instance_name','task_id','file_name','file_path','landing_server_name','environment_name','file_prefix','file_suffix','schedule_interval','frequency','direction','sla','source_system','source_poc','alert_channel','alert_target_team_name']
        finops_mandatory_column_list=['job_id','instance_name','job_engine', 'vsad','project_id','lob','sub_lob','portfolio','product_area','product_name','program_name']



        frequency_list = ["real_time", "hourly_custom", "weekly", "monthly", "fixed", "hourly", "daily"]
        flags = [True,False]
        bool_flag = ["true","false"]
        platform_list = ["gcp","unix","aws","teradata","edl"]
        env_list = ["gcp_bq","unix","java","edw_td","gcp","edl_hdfs","gcp_gcs","onprem_hive","gcp_hive"]
        scheduler_list = ["airflow","composer","espx","cron","oozie","trigger"]
        businessunit_list = ["vbg","network","vcg","corporate"]
        source_type_list = ["pubsub","table","esp","file","kafka"]
        target_type_list = ["file","kafka","pubsub","table","topic"]
        config_column_list  = ["variable","dev","test","prod"]
        time_zone_list = ["UTC","EST"]
        priority_list = ["P1","P2","P3","P4","P5"]
        true_flag = ["y","Y"]
        env="dev"

        config_df = pd.read_excel(data_bytes, sheet_name="Variables", dtype=str)
        config_df.dropna(axis=0, how="all", inplace=True)
        config_df = config_df.applymap(str)
        variable_list = list(config_df["variable"])
        dev_list = list(config_df["dev"])
        test_list = list(config_df["test"])
        prod_list = list(config_df["prod"])
        ple_list =  list(config_df["ple"])
        print(variable_list)
        print("-------------------------------------------------")
        print(dev_list)
        print("-------------------------------------------------")
        print(test_list)
        print("-------------------------------------------------")
        print(prod_list)
        print("-------------------------------------------------")
        print(ple_list)
        print("-------------------------------------------------")

        #Check for mismatch between variable list and values provided in columns
        if len(variable_list) != len(dev_list):
            raise ValueError(
                "Variable column and dev column values are not mapping correctly. Length of either of them has issues. Kindly check the config tab"
            )
        if len(variable_list) != len(test_list):
            raise ValueError(
                "Variable column and test column values are not mapping correctly. Length of either of them has issues. Kindly check the config tab"
            )
        if len(variable_list) != len(prod_list):
            raise ValueError(
                "Variable column and prod column values are not mapping correctly. Length of either of them has issues. Kindly check the config tab"
            )
        if len(variable_list) != len(ple_list):
            raise ValueError(
                "Variable column and prod column values are not mapping correctly. Length of either of them has issues. Kindly check the config tab"
            )


        #----------------------------------------column_validation-----------------------------------------------
        metadata_tables_columns = list(tables_meta.columns.values)
        metadata_files_columns = list(files_meta.columns.values)
        metadata_process_columns = list(process_meta.columns.values)
        metadata_task_columns = list(task_meta.columns.values)
        metadata_finops_columns = list(finops_meta.columns.values)

        missed_tbl_col = (set(tables_column_list).difference(metadata_tables_columns))
        missed_fle_col = (set(files_column_list).difference(metadata_files_columns))
        missed_prcs_col = (set(process_column_list).difference(metadata_process_columns))
        missed_tsk_col = (set(task_column_list).difference(metadata_task_columns))
        missed_fnps_col = (set(finops_column_list).difference(metadata_finops_columns))

        add_tbl_col = (set(metadata_tables_columns).difference(metadata_tables_columns))
        add_fle_col = (set(metadata_files_columns).difference(metadata_files_columns))
        add_prcs_col = (set(metadata_process_columns).difference(metadata_process_columns))
        add_tsk_col = (set(metadata_task_columns).difference(metadata_task_columns))
        add_fnps_col = (set(metadata_finops_columns).difference(metadata_finops_columns))

        missing_column_cnt = len(missed_tbl_col) + len(missed_fle_col) + len(missed_prcs_col) + len(missed_tsk_col) + len(missed_fnps_col)
        additional_column_cnt = len(add_tbl_col) + len(add_fle_col) + len(add_prcs_col) + len(add_tsk_col) + len(add_fnps_col)
        if len(missed_tbl_col)== 0:
            missed_tbl_col = 'NA'
        if len(missed_fle_col)== 0:
            missed_fle_col = 'NA'
        if len(missed_prcs_col)== 0:
            missed_prcs_col = 'NA'
        if len(missed_tsk_col)== 0:
            missed_tsk_col = 'NA'
        if len(missed_fnps_col)== 0:
            missed_fnps_col = 'NA'
        if len(add_tbl_col)== 0:
            add_tbl_col = 'NA'
        if len(add_fle_col)== 0:
            add_fle_col = 'NA'
        if len(add_prcs_col)== 0:
            add_prcs_col = 'NA'
        if len(add_tsk_col)== 0:
            add_tsk_col = 'NA'
        if len(add_fnps_col)== 0:
            add_fnps_col = 'NA'

        #-----------------------TABLE_VALIDATION_COLUMNS----------------------------------------------------------------
        if not tables_meta.empty:
            # Converting empty string to null values
            tables_meta= tables_meta.infer_objects(copy=False).replace(r'^\s*$',np.nan,regex=True)
            
            #Remove whitespaces
            tables_meta=tables_meta.map(lambda x:x.strip() if isinstance(x,str) else x)
            #Updating the Variables
            if "dev" in env:
                tables_meta.infer_objects(copy=False).replace(variable_list, dev_list,inplace=True,regex=True)
                print(tables_meta)
            elif "test" in env:
                tables_meta.infer_objects(copy=False).replace(variable_list, test_list, inplace=True,regex=True)
                print(tables_meta)
            elif "ple" in env:
                tables_meta.infer_objects(copy=False).replace(variable_list, ple_list, inplace=True,regex=True)
                print(tables_meta)
            else:
                tables_meta.infer_objects(copy=False).replace(variable_list, prod_list, inplace=True,regex=True)
                print(tables_meta)

            tables_meta = tables_meta.astype("string") 
            #Updating the datatype for Integer Column
            for col in table_integer_column:
                tables_meta[col]=tables_meta[col].replace('',np.nan).fillna('0').astype(int)

             #Updating the datatype for Boolean Column    
            for col in table_boolean_column:
                tables_meta[col]=tables_meta[col].replace('',np.nan).map({'Y': True,'N':False}).fillna(False)

            # Checking the null values in Mandatory Columns
            table_col_check=[]
            for mandatory_col in tables_mandatory_column_list:
                if(tables_meta[mandatory_col].isna().any() or (tables_meta[mandatory_col].astype(str).str.strip()=="").any()):
                    print("checking for mandatory columns")
                    table_col_check.append(mandatory_col)

            table_stg = 1
            
            if len(table_col_check)>0:
                raise ValueError("TABLE META VALIDATION:\n\n Following are the mandatory column in Table tab and can't be left empty :" + str(table_col_check))
            else:
                load_table(tables_meta,tables_table_id)
                print("Data Loaded Table staging Table")        
            """
            if len(tbl_nullval_db_name) > 0 or  len(tbl_nullval_table_name) > 0 or len(tbl_nullval_servername) > 0 or len(tbl_nullval_application_name) > 0 or len(tbl_nullval_platform_name) > 0 or len(tbl_nullval_env_name) > 0 :
                raise ValueError("TABLE META VALIDATION:\n\n Column validation failed counts : \n\tEmpty DB name  : {0}\n\tEmpty table name  : {1}\n\tEmpty server name  : {2}\n\tEmpty application name  : {3}\n\tEmpty platform name  : {4}\n\tEmpty Env name  : {5}".format(	tbl_nullval_db_name,tbl_nullval_table_name,tbl_nullval_servername,tbl_nullval_application_name,tbl_nullval_platform_name,tbl_nullval_env_name))
            """

        else:
            print("Tables Tab is Empty")
            table_stg = 0

        #-----------------------FILE_VALIDATION_COLUMNS----------------------------------------------------------------

        if not files_meta.empty:
            # Converting empty string to null values
            files_meta= files_meta.infer_objects(copy=False).replace(r'^\s*$',np.nan,regex=True)
            
            #Remove whitespaces
            files_meta=files_meta.map(lambda x:x.strip() if isinstance(x,str) else x)

            #Updating the Variables
            if "dev" in env:
                files_meta.infer_objects(copy=False).replace(variable_list, dev_list, inplace=True,regex=True)
                print(files_meta)
            elif "test" in env:
                files_meta.infer_objects(copy=False).replace(variable_list, test_list,inplace=True,regex=True)
                print(files_meta)
            elif "ple" in env:
                files_meta.infer_objects(copy=False).replace(variable_list, ple_list,inplace=True,regex=True)
                print(files_meta)
            else:
                files_meta.infer_objects(copy=False).replace(variable_list, prod_list,inplace=True,regex=True)
                print(files_meta)

            files_meta = files_meta.astype("string")
            #Updating the datatype for Integer Column 
            for col in files_integer_column:
                files_meta[col]=files_meta[col].replace('',np.nan).fillna('0').astype(int)

            #Updating the datatype for Boolean Column     
            for col in files_boolean_column:
                files_meta[col]=files_meta[col].replace('',np.nan).map({'Y': True,'N':False}).fillna(False)

            file_col_check=[]

            file_stg=1

            #Checking for mandatory columns
            for mandatory_col in files_mandatory_column_list:
                if(files_meta[mandatory_col].isna().any() or (files_meta[mandatory_col].astype(str).str.strip()=="").any()):
                    print("checking for mandatory columns")
                    file_col_check.append(mandatory_col)

            if len(file_col_check)>0:
                raise ValueError("FILE META VALIDATION:\n\n Following are the mandatory column in FILES tab and can't be left empty :" + str(file_col_check))
            else:
                load_table(files_meta,files_table_id)
                print("Data loaded to File staging Table")
        else:
            print("Files Tab is Empty")
            file_stg = 0

        #------------------TASK_VALIDATION_COLUMNS---------------------
        if not task_meta.empty:
            # Converting empty string to null values
            task_meta= task_meta.infer_objects(copy=False).replace(r'^\s*$',np.nan,regex=True)
            
            #Remove whitespaces
            task_meta=task_meta.map(lambda x:x.strip() if isinstance(x,str) else x)
            dictionary = dict(zip(variable_list, dev_list))
            if "dev" in env:
                task_meta.infer_objects(copy=False).replace(dictionary, inplace=True,regex=True)
            elif "test" in env:
                task_meta.infer_objects(copy=False).replace(variable_list, test_list, inplace=True,regex=True)
            elif "ple" in env:
                task_meta.infer_objects(copy=False).replace(variable_list, ple_list, inplace=True,regex=True)
            else:
                task_meta.infer_objects(copy=False).replace(variable_list, prod_list, inplace=True,regex=True)

            task_meta = task_meta.astype("string")
            #Updating the datatype for Integer Column 
            for col in task_integer_column:
                task_meta[col]=task_meta[col].replace('',np.nan).fillna('0').astype(int)

            #Updating the datatype for Boolean Column 
            for col in task_boolean_column:
                task_meta[col]=task_meta[col].replace('',np.nan).map({'Y': True,'N':False}).fillna(False)
            
            task_col_check=[]
             #Checking for mandatory columns
            for mandatory_col in task_mandatory_column_list:
                if(task_meta[mandatory_col].isna().any() or (task_meta[mandatory_col].astype(str).str.strip()=="").any()):
                    print("checking for mandatory columns")
                    task_col_check.append(mandatory_col)

            task_stg = 1
            if len(task_col_check)>0:
                raise ValueError("TASK META VALIDATION:\n\n Following are the mandatory column in TASK tab and can't be left empty :" + str(task_col_check))
            else:
                load_table(task_meta,task_table_id)
                print("Data loaded to Process staging Table")
        else:
            print("Process Tab is Empty")
            task_stg = 0

        #------------------PROCESS_VALIDATION_COLUMNS---------------------
        if not process_meta.empty:
            # Converting empty string to null values
            process_meta= process_meta.infer_objects(copy=False).replace(r'^\s*$',np.nan,regex=True)
            
            #Remove whitespaces
            process_meta=process_meta.map(lambda x:x.strip() if isinstance(x,str) else x)

            # Updating the Variable
            dictionary = dict(zip(variable_list, dev_list))
            if "dev" in env:
                process_meta.infer_objects(copy=False).replace(dictionary,inplace=True,regex=True)
            elif "test" in env:
                process_meta.infer_objects(copy=False).replace(variable_list, test_list,  inplace=True,regex=True)
            elif "ple" in env:
                process_meta.infer_objects(copy=False).replace(variable_list, ple_list, inplace=True,regex=True)
            else:
                process_meta.infer_objects(copy=False).replace(variable_list, prod_list, inplace=True,regex=True)

            process_meta = process_meta.astype("string")
             #Updating the datatype for Integer Column
            for col in process_integer_column:
                process_meta[col]=process_meta[col].replace('',np.nan).fillna('0').astype(int)

             #Updating the datatype for Boolean Column
            for col in process_boolean_column:
                process_meta[col]=process_meta[col].replace('',np.nan).map({'Y': True,'N':False}).fillna(False)

             #Checking for mandatory columns
            process_col_check=[]
            for mandatory_col in process_mandatory_column_list:
                if(process_meta[mandatory_col].isna().any() or (process_meta[mandatory_col].astype(str).str.strip()=="").any()):
                    print("checking for mandatory columns")
                    process_col_check.append(mandatory_col)

            process_meta['schedule_interval']=process_meta['schedule_interval'].fillna('').astype(str)
            process_meta['frequency']=process_meta['frequency'].fillna('').astype(str)

            # -----------------------------------Cron Validation-------------------------------------------------------------

            for index,row in process_meta.iterrows():
                dly_cron_vld,wkly_cron_vld,mntly_cron_vld,hrly_cron_vld,chrly_cron_vld,uknwn_cron_vld=0,0,0,0,0,0
                schedule=row['schedule_interval']
                if row['frequency']== 'daily':
                # Valid daily schedule should match: '0 HH  ' or similar (e.g., "30 14  ")
                    daily_pattern = r'^\d{1,2} \d{1,2} \* \* \*$'

                    if not re.match(daily_pattern, schedule):
                        dly_cron_vld+=1
                        raise ValueError(schedule + " : Invalid cron for daily frequency. Expected format: 'MINUTE HOUR  *'.",{schedule})   
                
                elif row['frequency'] == 'weekly':

                # Valid weekly schedule should match: '0 HH  DAY' (e.g., "0 0  MON")
                    weekly_pattern = weekly_pattern = r'^\d{1,2} \d{1,2} \* \* (([0-6](,[0-6])*)|((SUN|MON|TUE|WED|THU|FRI|SAT)(,(SUN|MON|TUE|WED|THU|FRI|SAT))*))$'
                    if not re.match(weekly_pattern, schedule):
                        wkly_cron_vld+=1
                        raise ValueError(schedule + " :Invalid cron for weekly frequency. Expected format: 'MINUTE HOUR  DAY'.")

            
                elif row['frequency'] == 'hourly':

                # Valid hourly schedule should match: '0  ' or 'MINUTE  '
                    hourly_pattern = r'^\d{1,2} \* \* \* \*$'
                    if not re.match(hourly_pattern, schedule):
                        hrly_cron_vld+=1
                        raise ValueError(schedule + " :Invalid cron for hourly frequency. Expected format: 'MINUTE  '.")
                
                elif row['frequency'] == 'monthly':

                # Valid monthly schedule should match: '0 HH DD ' (e.g., "0 0 1 " for the 1st of the month)
                    monthly_pattern = r'^\d{1,2} \d{1,2} ((\d{1,2})(,\d{1,2})*) \* \*$'
                    if not re.match(monthly_pattern, schedule):
                        mntly_cron_vld+=1
                        raise ValueError(schedule + " :Invalid cron for monthly frequency. Expected format: 'MINUTE HOUR DAY '.")

                elif row['frequency'] == 'hourly_custom':

                # Valid custom hourly schedule should match: '*/N  ' or 'MINUTE HOUR1,HOUR2,...  *'
                    custom_hourly_pattern = r'^\d{1,2} (?:\*/\d{1,2}|\d{1,2}(?:,\d{1,2})*) \* \* \*$'
                    if not re.match(custom_hourly_pattern, schedule):
                        chrly_cron_vld+=1
                        raise ValueError(schedule + " :Invalid cron for custom hourly frequency. Expected format: 'MINUTE HOUR1,HOUR2,...  ' or '/N  '.")
                else:
                    uknwn_cron_vld+=1
                    raise ValueError( f"Unknown frequency: {row['frequency' ]}")
                
            
            process_stg = 1
            if len(process_col_check)>0:
                raise ValueError("PROCESS META VALIDATION:\n\n Following are the mandatory column in PROCESS tab and can't be left empty :" + str(process_col_check))
            else:
                load_table(process_meta,process_table_id)
                print("Data loaded to Process staging Table")
        else:
            print("Process Tab is Empty")
            process_stg = 0

        #-----------------------Finops_VALIDATION_COLUMNS----------------------------------------------------------------
        if not finops_meta.empty:
            # Converting empty string to null values
            finops_meta=finops_meta.infer_objects(copy=False).replace(r'^\s*$',np.nan,regex=True)

            #Remove whitespaces
            finops_meta=finops_meta.map(lambda x:x.strip() if isinstance(x,str) else x)

            #Updating the Variables
            if "dev" in env:
                finops_meta.infer_objects(copy=False).replace(variable_list, dev_list, inplace=True,regex=True)
                print(finops_meta)
            elif "test" in env:
                finops_meta.infer_objects(copy=False).replace(variable_list, test_list, inplace=True,regex=True)
                print(finops_meta)
            elif "ple" in env:
                finops_meta.infer_objects(copy=False).replace(variable_list, ple_list, inplace=True,regex=True)
                print(finops_meta)
            else:
                finops_meta.infer_objects(copy=False).replace(variable_list, prod_list, inplace=True,regex=True)
                print(finops_meta)
            
            finops_meta = finops_meta.astype("string")
             #Updating the datatype for Integer Column
            for col in finops_integer_column:
                finops_integer_column[col]=finops_meta[col].replace('',np.nan).fillna('0').astype(int)

             #Updating the datatype for Boolean Column           
            for col in finops_boolean_column:
                finops_meta[col]=finops_meta[col].replace('',np.nan).map({'Y': True,'N':False}).fillna(False)
            
            #Checking for mandatory column
            finops_col_check=[]
            for mandatory_col in finops_mandatory_column_list:
                if(finops_meta[mandatory_col].isna().any() or (finops_meta[mandatory_col].astype(str).str.strip()=="").any()):
                    print("checking for mandatory columns")
                    finops_col_check.append(mandatory_col)      
            
            finops_stg = 1
            
            if len(finops_col_check)>0:
                raise ValueError("FINOPS META VALIDATION:\n\n Following are the mandatory column in Table tab and can't be left empty :" + str(finops_col_check))
            else:
               load_table(finops_meta,finops_table_id)
               print("Data Loaded Label taging Table")        
            
        else:
            print("Label Tab is Empty")
            finops_stg = 0

        # --------------------Vaiables Still Present in Metadata------------------------------

        t_var=[]
        f_var=[]
        p_var=[]
        tsk_var=[]
        fnps_var=[]
        for col in tables_column_list:
            if pd.notna(tables_meta[col]).all() and tables_meta[col].astype(str).str.contains('<').any():
                t_var.append(col)
        for col in files_column_list:
            if pd.notna(files_meta[col]).all() and files_meta[col].astype(str).str.contains('<').any():
                f_var.append(col)
        for col in process_column_list:
            if pd.notna(process_meta[col]).all() and process_meta[col].astype(str).str.contains('<').any():
                p_var.append(col)
        for col in task_column_list:
            if pd.notna(task_meta[col]).all() and task_meta[col].astype(str).str.contains('<').any():
                tsk_var.append(col)
        for col in finops_column_list:
            if pd.notna(finops_meta[col]).all() and finops_meta[col].astype(str).str.contains('<').any():
                fnps_var.append(col)

        
        file_sql = """CALL `aid_epdo_prd_tbls.upsert_datax_file` ()"""
        table_sql = """CALL `aid_epdo_prd_tbls.upsert_datax_table` ()"""
        process_sql = """CALL `aid_epdo_prd_tbls.upsert_datax_process` ()"""
        task_sql = """CALL `aid_epdo_prd_tbls.upsert_datax_task` ()"""
        finops_sql = """CALL `aid_epdo_prd_tbls.upsert_datax_label` ()"""

        if missing_column_cnt > 0 or additional_column_cnt > 0  or  len(t_var) > 0  or  len(f_var) > 0  or  len(p_var) > 0  or  len(t_var) > 0 or len(fnps_var) > 0:
                    raise ValueError("COMMON VALIDATION:\n\n Missing columns in metadata :\n\tFiles:{0} \n\tTables:{1} \n\tProcess:{2} \n\tTask:{3} \n\tTask:{4} \nAdditional columns in metadata :\n\tFiles:{5} \n\tTables:{6} \n\tProcess:{7} \n\tTask:{8} \n\tTask:{9} \nVariables still present in metadata: \n\tFiles:{10} \n\tTables:{11} \n\tProcess:{12} \n\tTask:{13} \n\tFinops:{14}".format(missed_fle_col,missed_tbl_col,missed_prcs_col,missed_tsk_col,missed_fnps_col,add_fle_col,add_tbl_col,add_prcs_col,add_tsk_col,add_fnps_col,f_var,t_var,p_var,tsk_var,fnps_var))

# --------------------Loading Data from Stage to Meta tables------------------------------

        print("Starting to load staging data to process metadata.")
        if process_stg == 1:
            query_job=client.query(process_sql)
            query_job.result()
            print("Load to process base table completed.")
            process_count += 1
        else:
            print("Procedure to update Process_meta will not run!")
            process_count += 1
        print("-----------------------------------------------------")
        print("-----------------------------------------------------")

        print("-----------------------------------------------------")
        print("Starting to load staging data to file metadata.")
        if file_stg == 1:
            client.query(file_sql)
            print("Load to files base table completed.")
            process_count += 1
        else:
            print("Procedure to update file_meta will not run!")
            process_count += 1
        print("-----------------------------------------------------")
        print("-----------------------------------------------------")

        print("Starting to load staging data to table metadata.")
        if table_stg == 1:
            client.query(table_sql)
            print("Load to Tables base table completed.")
            process_count += 1
        else:
            print("Procedure to update Table_meta will not run!")
            process_count += 1
        print("-----------------------------------------------------")
        print("-----------------------------------------------------")

        if finops_stg==1:
            print("Starting to load staging data to finops metadata.")
            client.query(finops_sql)
            print("Load to finops base table completed.")
            process_count += 1
        else:
            print("Procedure to update Finops_meta will not run!")
        print("-----------------------------------------------------")
        print("-----------------------------------------------------")

        if task_stg==1:
            print("Starting to load staging data to task metadata.")
            client.query(task_sql)
            print("Load to task base table completed.")
            process_count += 1
        else:
            print("Procedure to update Task_meta will not run!")
        print("-----------------------------------------------------")
        print("-----------------------------------------------------")

         # Send Email for success
        process_count=4
        if process_count < 4:
            raise ValueError(
                "Someting in Pipeline Went Wrong. Some base table hasn't been loaded."
            )
        else:
            msg = EmailMessage()
            success = "FILE NAME: " + file_name + " Load Successful!"
            msg.set_content(success)
            msg["Subject"] = "(" + env + ") Metadata Pipeline - Success!"
            msg["From"] = "do-not-reply@verizon.com"
            msg["To"] = "ayush.singh1@verizon.com"
            smtpObj = smtplib.SMTP("vzsmtp.verizon.com", 25)
            smtpObj.send_message(msg)
            print("Successfully sent email for Success!")
            smtpObj.quit()
    except Exception as e:
        # exc_type,exc_obj,exc_tb=sys.exc_info
        # line_number=exc_tb.tb.lineno
        error = "Error Occurred in FILE: " + file_name + "\n" + "ERROR: " + str(e)
        print(error)
        msg = EmailMessage()
        msg.set_content(error)
        msg["Subject"] = "(" + env + ") Error occured in Metadata Pipeline!"
        msg["From"] = "do-not-reply@verizon.com"
        msg["To"] = "ayush.singh1@verizon.com"

        try:
            smtpObj = smtplib.SMTP("vzsmtp.verizon.com", 25)
            smtpObj.send_message(msg)
            print("Successfully sent email for Error!")
            smtpObj.quit()
        except SMTPException:
            print("Error: unable to send email")

