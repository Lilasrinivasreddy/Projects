# Data Cleaning & Preprocessing

# Converting date columns to datetime
df['RPT_DT'] = pd.to_datetime(df['RPT_DT'])
df['ACTIVITY_DT'] = pd.to_datetime(df['ACTIVITY_DT'])

# Checking for duplicates
duplicates = df.duplicated().sum()

# Checking for missing values
missing_values = df.isnull().sum()

# Summary Statistics
summary_stats = df.describe(include='all')

# Distribution of numerical columns
sales_distribution = df['SALES'].describe()
gross_adds_distribution = df['GROSS_ADDS'].describe()

# Checking for outliers using IQR
Q1 = df[['SALES', 'GROSS_ADDS']].quantile(0.25)
Q3 = df[['SALES', 'GROSS_ADDS']].quantile(0.75)
IQR = Q3 - Q1
outliers = ((df[['SALES', 'GROSS_ADDS']] < (Q1 - 1.5 * IQR)) | (df[['SALES', 'GROSS_ADDS']] > (Q3 + 1.5 * IQR))).sum()

# Unique values in categorical columns
unique_channels = df['Channel'].unique()
unique_data_tiers = df['DATA_TIER_F'].unique()

# Output the findings
duplicates, missing_values, summary_stats, sales_distribution, gross_adds_distribution, outliers, unique_channels, unique_data_tiers
=================


# Regenerating visualizations with observations and explanations

# Histogram
plt.figure(figsize=(10, 5))
plt.hist(df['SALES'], bins=10, alpha=0.7, label='SALES')
plt.hist(df['GROSS_ADDS'], bins=10, alpha=0.7, label='GROSS_ADDS')
plt.xlabel('Values')
plt.ylabel('Frequency')
plt.title('Histogram of SALES and GROSS_ADDS')
plt.legend()
plt.show()

# Observation:
# The histogram shows that most values are concentrated in the lower ranges for both SALES and GROSS_ADDS.
# However, there are some high values indicating that on certain days, sales and gross additions spiked significantly.
# This indicates possible sales events or special promotions.

# Boxplot
plt.figure(figsize=(10, 5))
sns.boxplot(data=df[['SALES', 'GROSS_ADDS']])
plt.title('Boxplots of SALES and GROSS_ADDS')
plt.show()

# Observation:
# The boxplot reveals clear outliers for both SALES and GROSS_ADDS, as shown by the points beyond the whiskers.
# The interquartile range (IQR) is small compared to the spread, indicating that most of the data is clustered, except for a few extreme values.

# Correlation Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(df[['SALES', 'GROSS_ADDS']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap between SALES and GROSS_ADDS')
plt.show()

# Observation:
# The heatmap shows a strong positive correlation between SALES and GROSS_ADDS (close to 1).
# This implies that when sales increase, gross additions also increase proportionally, which is expected in a sales-driven dataset.

# Trend Analysis
plt.figure(figsize=(10, 5))
sns.lineplot(data=df, x='RPT_DT', y='SALES', marker='o', label='SALES')
sns.lineplot(data=df, x='RPT_DT', y='GROSS_ADDS', marker='o', label='GROSS_ADDS')
plt.xticks(rotation=45)
plt.title('Trend Analysis of SALES and GROSS_ADDS over Report Dates')
plt.xlabel('Report Date')
plt.ylabel('Values')
plt.legend()
plt.show()

# Observation:
# The trend analysis shows significant peaks in both SALES and GROSS_ADDS on certain dates, indicating possible promotional periods.
# The pattern of both lines is similar, indicating a direct relationship between sales and gross additions over time.
# Sales and additions are generally stable but experience sharp increases occasionally, likely driven by external factors such as marketing campaigns.



Updated Observations from the Visualizations:
Histogram:

The majority of SALES and GROSS_ADDS values are concentrated in lower ranges, with occasional high values indicating potential sales events or promotional spikes.
Boxplot:

Outliers are clearly visible, showing days with extraordinarily high sales and gross additions compared to the usual range.
The main data cluster is tight, suggesting typical sales activity, but extreme values indicate peak sales periods.
Correlation Heatmap:

High correlation (~0.99) confirms that SALES and GROSS_ADDS move together, reflecting that an increase in sales usually results in increased gross additions.
Trend Analysis:

Peaks in SALES and GROSS_ADDS occur simultaneously, indicating specific periods of high activity.
The trend is mostly stable with sharp increases, suggesting external factors such as promotions, end-of-month sales, or special campaigns influencing these spikes.


# Channel-wise analysis, outlier handling, and granular insights

# Channel-wise Summary
channel_summary = df.groupby('Channel')[['SALES', 'GROSS_ADDS']].agg(['mean', 'sum', 'max', 'min'])

# Outlier Handling - Removing outliers using IQR
Q1 = df[['SALES', 'GROSS_ADDS']].quantile(0.25)
Q3 = df[['SALES', 'GROSS_ADDS']].quantile(0.75)
IQR = Q3 - Q1
df_no_outliers = df[~((df[['SALES', 'GROSS_ADDS']] < (Q1 - 1.5 * IQR)) | (df[['SALES', 'GROSS_ADDS']] > (Q3 + 1.5 * IQR))).any(axis=1)]

# Summary after outlier removal
summary_after_outlier = df_no_outliers.describe()

# Visualizing channel-wise data
plt.figure(figsize=(10, 5))
sns.boxplot(data=df, x='Channel', y='SALES', palette='viridis')
plt.title('Channel-wise Boxplot for SALES')
plt.show()

plt.figure(figsize=(10, 5))
sns.boxplot(data=df, x='Channel', y='GROSS_ADDS', palette='magma')
plt.title('Channel-wise Boxplot for GROSS_ADDS')
plt.show()

# Channel-wise trend
plt.figure(figsize=(12, 6))
sns.lineplot(data=df, x='RPT_DT', y='SALES', hue='Channel', marker='o')
plt.title('Channel-wise Trend Analysis of SALES')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(12, 6))
sns.lineplot(data=df, x='RPT_DT', y='GROSS_ADDS', hue='Channel', marker='o')
plt.title('Channel-wise Trend Analysis of GROSS_ADDS')
plt.xticks(rotation=45)
plt.show()

channel_summary, summary_after_outlier

Channel-wise Analysis, Outlier Handling, and Granular Insights:
Channel-wise Summary:

Digital channel has lower average SALES (631.6) and GROSS_ADDS (592.5) compared to Stores with SALES at 842.2 and GROSS_ADDS at 791.5.
Stores channel had the highest single-day sales (6228) and gross adds (6072), while the Digital channelâ€™s peak was 1885 and 1861 respectively.
Outlier Handling:

After removing outliers, the mean SALES dropped from 736.9 to 262.7, and GROSS_ADDS from 692 to 224.2, showing how high-value entries were skewing the dataset.
The range of values became narrower, with max sales at 1401 and gross adds at 1375, providing a clearer view of typical performance.
Boxplots:

Sales by Channel: Digital shows a smaller range of values compared to Stores, which has more variability and several outliers.
Gross Adds by Channel: Similar trend with Stores having more variability and Digital being more consistent.
Trend Analysis by Channel:

Digital sales and gross adds show steady, smaller peaks.
Stores have more volatile trends with sharp increases on specific days, likely due to in-store promotions or footfall-driven sales.





from statsmodels.tsa.seasonal import seasonal_decompose

# Perform time-series decomposition for SALES and GROSS_ADDS
decompose_sales = seasonal_decompose(df.groupby('RPT_DT')['SALES'].sum(), model='additive', period=7)
decompose_gross_adds = seasonal_decompose(df.groupby('RPT_DT')['GROSS_ADDS'].sum(), model='additive', period=7)

# Plot SALES decomposition
plt.figure(figsize=(12, 8))
plt.subplot(4, 1, 1)
plt.plot(decompose_sales.observed, label='Observed SALES', color='steelblue')
plt.legend()
plt.subplot(4, 1, 2)
plt.plot(decompose_sales.trend, label='Trend', color='darkorange')
plt.legend()
plt.subplot(4, 1, 3)
plt.plot(decompose_sales.seasonal, label='Seasonality', color='seagreen')
plt.legend()
plt.subplot(4, 1, 4)
plt.plot(decompose_sales.resid, label='Residuals', color='gray')
plt.legend()
plt.suptitle('Time Series Decomposition of SALES')
plt.show()

# Plot GROSS_ADDS decomposition
plt.figure(figsize=(12, 8))
plt.subplot(4, 1, 1)
plt.plot(decompose_gross_adds.observed, label='Observed GROSS_ADDS', color='steelblue')
plt.legend()
plt.subplot(4, 1, 2)
plt.plot(decompose_gross_adds.trend, label='Trend', color='darkorange')
plt.legend()
plt.subplot(4, 1, 3)
plt.plot(decompose_gross_adds.seasonal, label='Seasonality', color='seagreen')
plt.legend()
plt.subplot(4, 1, 4)
plt.plot(decompose_gross_adds.resid, label='Residuals', color='gray')
plt.legend()
plt.suptitle('Time Series Decomposition of GROSS_ADDS')
plt.show()

Time-Series Analysis and Seasonality Detection:
Trend:

Both SALES and GROSS_ADDS exhibit clear upward trends, with notable peaks indicating periods of high sales activity.
Seasonality:

Regular oscillations in the seasonal component suggest cyclical patterns, potentially linked to weekly or promotional cycles.
Residuals:

The residual component shows random fluctuations, indicating some irregularities that aren't captured by trend or seasonality, possibly external factors or noise.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
from scipy import stats

# Load the dataset
data = """
RPT_DT,ACTIVITY_DT,Channel,DATA_TIER_F,SALES,GROSS_ADDS
2025-02-03,2025-02-06,Stores,Phones,148,126
2025-01-30,2025-02-06,Stores,Phones,51,30
2025-02-06,2025-02-06,Digital,Phones,1885,1861
2025-02-04,2025-02-06,Digital,Phones,1401,1375
2025-02-01,2025-02-06,Digital,Phones,58,16
2025-02-04,2025-02-06,Stores,Phones,148,93
2025-01-31,2025-02-06,Stores,Phones,26,-2
2025-02-02,2025-02-06,Digital,Phones,122,107
2025-02-02,2025-02-06,Stores,Phones,24,10
2025-01-30,2025-02-06,Digital,Phones,190,119
2025-02-05,2025-02-06,Digital,Phones,670,637
2025-02-05,2025-02-06,Stores,Phones,77,13
2025-02-03,2025-02-06,Digital,Phones,599,558
2025-01-31,2025-02-06,Digital,Phones,128,67
2025-02-06,2025-02-06,Stores,Phones,6228,6072
2025-02-01,2025-02-06,Stores,Phones,36,-10
"""

# Convert data to DataFrame
from io import StringIO
df = pd.read_csv(StringIO(data))

# Convert date columns to datetime format
df['RPT_DT'] = pd.to_datetime(df['RPT_DT'])
df['ACTIVITY_DT'] = pd.to_datetime(df['ACTIVITY_DT'])

# Detect anomalies using different techniques

# 1. IQR Method
Q1 = df[['SALES', 'GROSS_ADDS']].quantile(0.25)
Q3 = df[['SALES', 'GROSS_ADDS']].quantile(0.75)
IQR = Q3 - Q1
df['IQR_Anomaly'] = ((df[['SALES', 'GROSS_ADDS']] < (Q1 - 1.5 * IQR)) |
                      (df[['SALES', 'GROSS_ADDS']] > (Q3 + 1.5 * IQR))).any(axis=1)

# 2. Z-Score Method
z_scores = np.abs(stats.zscore(df[['SALES', 'GROSS_ADDS']]))
df['Z_Anomaly'] = (z_scores > 3).any(axis=1)

# 3. Isolation Forest
iso_forest = IsolationForest(contamination=0.1, random_state=42)
df['IsoForest_Anomaly'] = iso_forest.fit_predict(df[['SALES', 'GROSS_ADDS']]) == -1

# 4. Local Outlier Factor (LOF)
lof = LocalOutlierFactor(n_neighbors=5, contamination=0.1)
df['LOF_Anomaly'] = lof.fit_predict(df[['SALES', 'GROSS_ADDS']]) == -1

# 5. One-Class SVM
oc_svm = OneClassSVM(nu=0.1, kernel="rbf", gamma='auto')
df['SVM_Anomaly'] = oc_svm.fit_predict(df[['SALES', 'GROSS_ADDS']]) == -1

# Filter anomalies
anomalies = df[df[['IQR_Anomaly', 'Z_Anomaly', 'IsoForest_Anomaly', 'LOF_Anomaly', 'SVM_Anomaly']].any(axis=1)]

# Display anomalies
import ace_tools as tools
tools.display_dataframe_to_user(name="Anomaly Detection Results", dataframe=anomalies)

# Using a small dataset (first 5 rows) instead of just one row
data = {
    "SALES": [148, 51, 1885, 1401, 58],
    "GROSS_ADDS": [126, 30, 1861, 1375, 16]
}

df = pd.DataFrame(data)

# Recalculate IQR method
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
df["IQR_Anomaly"] = ((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)

# Z-Score Method
df["Z_Score_SALES"] = zscore(df["SALES"])
df["Z_Score_GROSS_ADDS"] = zscore(df["GROSS_ADDS"])
df["Z_Anomaly"] = ((df["Z_Score_SALES"].abs() > 3) | (df["Z_Score_GROSS_ADDS"].abs() > 3))

# Isolation Forest
iso_forest = IsolationForest(contamination=0.2, random_state=42)
df["IsoForest_Anomaly"] = iso_forest.fit_predict(df[["SALES", "GROSS_ADDS"]]) == -1

# Local Outlier Factor (LOF)
lof = LocalOutlierFactor(n_neighbors=2, contamination=0.2)
df["LOF_Anomaly"] = lof.fit_predict(df[["SALES", "GROSS_ADDS"]]) == -1

# One-Class SVM
svm_model = OneClassSVM(nu=0.2, kernel="rbf", gamma=0.1)
df["SVM_Anomaly"] = svm_model.fit_predict(df[["SALES", "GROSS_ADDS"]]) == -1

# Display the calculations
tools.display_dataframe_to_user(name="Anomaly Detection (First 5 Rows)", dataframe=df)


pip install numpy pandas tensorflow scikit-learn
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Load the dataset
data = {
    "SALES": [148, 51, 1885, 1401, 58],
    "GROSS_ADDS": [126, 30, 1861, 1375, 16]
}

df = pd.DataFrame(data)

# Normalize the data
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)

# Split data for training (assuming first 80% is normal)
X_train, X_test = train_test_split(df_scaled, test_size=0.2, random_state=42)

# Define the autoencoder model
input_dim = X_train.shape[1]
encoding_dim = 2  # Latent space dimension

autoencoder = keras.Sequential([
    keras.layers.Dense(4, activation="relu", input_shape=(input_dim,)),
    keras.layers.Dense(encoding_dim, activation="relu"),
    keras.layers.Dense(4, activation="relu"),
    keras.layers.Dense(input_dim, activation="sigmoid")
])

# Compile the model
autoencoder.compile(optimizer="adam", loss="mse")

# Train the autoencoder
autoencoder.fit(X_train, X_train, epochs=100, batch_size=2, shuffle=True, verbose=0)

# Compute reconstruction error
reconstructed = autoencoder.predict(df_scaled)
mse = np.mean(np.power(df_scaled - reconstructed, 2), axis=1)

# Set anomaly threshold (e.g., 95th percentile)
threshold = np.percentile(mse, 95)
df["Anomaly_Score"] = mse
df["Anomaly"] = df["Anomaly_Score"] > threshold

# Show results
print(df)












# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import ttest_ind, mannwhitneyu
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.seasonal import seasonal_decompose

# Load dataset
df = pd.read_csv("your_data.csv")

# Convert dates to datetime format
df['RPT_DT'] = pd.to_datetime(df['RPT_DT'])
df['ACTIVITY_DT'] = pd.to_datetime(df['ACTIVITY_DT'])

# Drop any duplicates
df = df.drop_duplicates()

# Handling missing values (if any)
df.fillna(0, inplace=True)

# Display dataset overview
print("\nDataset Info:")
print(df.info())

# Display first few rows
print("\nFirst 5 Rows of Data:")
print(df.head())

# Descriptive statistics
print("\nDescriptive Statistics:")
summary_stats = df[['SALES', 'GROSS_ADDS']].describe()
print(summary_stats)

# Correlation Analysis
correlation_matrix = df[['SALES', 'GROSS_ADDS']].corr()
plt.figure(figsize=(6, 4))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation between SALES & GROSS_ADDS")
plt.show()

# Moving Averages for Trend Analysis
df['MA_SALES'] = df['SALES'].rolling(window=7).mean()
df['MA_GROSS_ADDS'] = df['GROSS_ADDS'].rolling(window=7).mean()

# SALES & GROSS_ADDS Trend Over Time
plt.figure(figsize=(14, 6))
plt.plot(df['ACTIVITY_DT'], df['SALES'], label="Actual SALES", alpha=0.5, color='blue')
plt.plot(df['ACTIVITY_DT'], df['MA_SALES'], label="7-Day Moving Avg (SALES)", color="red")
plt.plot(df['ACTIVITY_DT'], df['GROSS_ADDS'], label="Actual GROSS_ADDS", alpha=0.5, color='green')
plt.plot(df['ACTIVITY_DT'], df['MA_GROSS_ADDS'], label="7-Day Moving Avg (GROSS_ADDS)", color="orange")
plt.title("SALES & GROSS_ADDS Trend Over Time")
plt.xlabel("Date")
plt.ylabel("Value")
plt.legend()
plt.grid()
plt.show()

# Sales Distribution by Channel
plt.figure(figsize=(10, 5))
sns.boxplot(data=df, x='Channel', y='SALES', palette="coolwarm")
plt.title("Sales Distribution by Channel")
plt.grid()
plt.show()

# Gross Adds Distribution by Channel
plt.figure(figsize=(10, 5))
sns.boxplot(data=df, x='Channel', y='GROSS_ADDS', palette="coolwarm")
plt.title("Gross Adds Distribution by Channel")
plt.grid()
plt.show()

# Anomaly Detection using Isolation Forest for SALES & GROSS_ADDS
iso_forest = IsolationForest(contamination=0.05, random_state=42)
df['IF_OUTLIER'] = iso_forest.fit_predict(df[['SALES', 'GROSS_ADDS']])
df['IF_OUTLIER_LABEL'] = df['IF_OUTLIER'].apply(lambda x: "Anomaly" if x == -1 else "Normal")

plt.figure(figsize=(14, 6))
sns.scatterplot(data=df, x="ACTIVITY_DT", y="SALES", hue="IF_OUTLIER_LABEL", palette={"Anomaly": "red", "Normal": "blue"})
plt.title("SALES Anomalies Detected by Isolation Forest")
plt.xlabel("Date")
plt.ylabel("Sales")
plt.grid()
plt.show()

# DBSCAN Clustering for Outlier Detection
scaler = StandardScaler()
df_scaled = scaler.fit_transform(df[['SALES', 'GROSS_ADDS']])
dbscan = DBSCAN(eps=1.5, min_samples=5)
df['DBSCAN_OUTLIER'] = dbscan.fit_predict(df_scaled)

plt.figure(figsize=(14, 6))
sns.scatterplot(data=df, x="ACTIVITY_DT", y="SALES", hue=df['DBSCAN_OUTLIER'], palette="coolwarm")
plt.title("DBSCAN Outlier Detection for SALES")
plt.xlabel("Date")
plt.ylabel("Sales")
plt.grid()
plt.show()

# Time-Series Decomposition for SALES
df_sorted = df.sort_values(by="ACTIVITY_DT")
df_sorted.set_index("ACTIVITY_DT", inplace=True)
result = seasonal_decompose(df_sorted['SALES'], model='additive', period=7)
result.plot()
plt.show()

# Autocorrelation Analysis
plt.figure(figsize=(12, 6))
plot_acf(df_sorted['SALES'], lags=30)
plt.title("Autocorrelation Function (ACF) for SALES")
plt.show()

plt.figure(figsize=(12, 6))
plot_pacf(df_sorted['SALES'], lags=30)
plt.title("Partial Autocorrelation Function (PACF) for SALES")
plt.show()

# Hypothesis Testing for SALES between Digital & Stores
digital_sales = df[df['Channel'] == 'Digital']['SALES']
stores_sales = df[df['Channel'] == 'Stores']['SALES']
t_stat, p_value = ttest_ind(digital_sales, stores_sales)

print("\nT-Test for SALES between Digital & Stores:")
print(f"T-Statistic: {t_stat:.4f}, P-Value: {p_value:.4f}")

mwu_stat, mwu_p = mannwhitneyu(digital_sales, stores_sales)

print("\nMann-Whitney U Test Results:")
print(f"U-Statistic: {mwu_stat:.4f}, P-Value: {mwu_p:.4f}")

# Save processed data
df.to_csv("processed_sales_gross_adds_analysis.csv", index=False)
print("Processed data saved successfully!")
