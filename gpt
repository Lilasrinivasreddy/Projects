import os
import re
import json
import time
import base64
import logging
from google.cloud import bigquery
from django.http import HttpResponse
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator
import google.auth

# ==========================
# ✅ Custom Logger Setup
# ==========================
class Logger:
    def __init__(self, name='', path=''):
        self.log = self.logger_config(name, path)

    @staticmethod
    def logger_config(name, path):
        logger = logging.getLogger(name)
        formatter = logging.Formatter('%(asctime)s : %(name)s - %(funcName)s - %(levelname)s : %(message)s', '%Y-%m-%d %H:%M:%S')
        file_handler = logging.FileHandler(path, mode='a')
        file_handler.setFormatter(formatter)
        stream_handler = logging.StreamHandler()
        stream_handler.setLevel(logging.INFO)
        stream_handler.setFormatter(formatter)
        logger.setLevel(logging.INFO)
        logger.addHandler(file_handler)
        logger.addHandler(stream_handler)
        return logger

# ==========================
# ✅ BigQuery Query Execution Class
# ==========================
class ExecuteUploadedQueries:
    def __init__(self, auth_config, logger_path):
        self.config = auth_config  # BigQuery Authentication Config
        self.logger = Logger(name="BQ_EXECUTOR", path=logger_path).log  # Initialize Logger
        self.client = None  # Placeholder for BigQuery Client

    # ==========================
    # ✅ Check if Token is Expired
    # ==========================
    def is_token_expired(self, path):
        try:
            self.logger.info(f"Checking token file at {path}")
            if os.path.exists(path):
                with open(path, 'r') as f:
                    old_access_token = json.load(f)['access_token'].split('.')[1]
                    old_access_token += '=' * (-len(old_access_token) % 4)
                    old_token_json_decoded = json.loads(base64.b64decode(old_access_token))
                    auth_time = old_token_json_decoded['auth_time']
                    expires_in = old_token_json_decoded['expires_in']
                    curr_epoch_time = int(time.time())

                    if curr_epoch_time - auth_time < expires_in - 120:
                        self.logger.info("Token is valid.")
                        return False
                    else:
                        self.logger.info("Token is expired.")
            return True
        except Exception as e:
            self.logger.error(f"Error checking token expiration: {e}")
            return True  # Assume expired if any error occurs

    # ==========================
    # ✅ Initialize BigQuery Client
    # ==========================
    def initialize_bigquery_client(self):
        if self.is_token_expired(self.config["oidc_token"]):
            self.logger.info("Refreshing token...")
            self.refresh_token()

        self.logger.info("Setting Google Cloud environment variables...")
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = self.config["sa_json_file"]
        os.environ["GOOGLE_CLOUD_PROJECT"] = self.config["project_id"]

        credentials, _ = google.auth.default()
        self.client = bigquery.Client(credentials=credentials, project=self.config["project_id"])
        self.logger.info(f"Connected to BigQuery Project: {self.config['project_id']}")

    # ==========================
    # ✅ Read Queries from Uploaded File
    # ==========================
    def read_queries_from_uploaded_file(self, file):
        try:
            content = file.read().decode('utf-8')
            queries = [q.strip() for q in re.split(r';\s*\n', content) if q.strip()]
            self.logger.info(f"Read {len(queries)} queries from file.")
            return queries
        except Exception as e:
            self.logger.error(f"Error reading file: {e}")
            return []

    # ==========================
    # ✅ Execute Queries in BigQuery
    # ==========================
    def execute_queries(self, queries):
        self.initialize_bigquery_client()  # Ensure BigQuery client is set up

        for query in queries:
            try:
                self.logger.info(f"Executing Query:\n{query}")
                query_job = self.client.query(query)
                query_job.result()  # Wait for execution to complete
                self.logger.info("Query executed successfully.")
            except Exception as e:
                self.logger.error(f"Query execution error: {e}")

    # ==========================
    # ✅ Django API Endpoint: Handle File Upload & Execute Queries
    # ==========================
    @method_decorator(csrf_exempt, name='dispatch')
    def post(self, request, *args, **kwargs):
        try:
            if 'file' not in request.FILES:
                self.logger.error("No file uploaded.")
                return HttpResponse("No file uploaded.", status=400)

            file = request.FILES['file']
            self.logger.info(f"Received file: {file.name}")

            queries = self.read_queries_from_uploaded_file(file)
            if not queries:
                self.logger.error("No valid queries found.")
                return HttpResponse("No valid queries found.", status=400)

            self.execute_queries(queries)  # Execute SQL Queries in BigQuery
            return HttpResponse("File uploaded and queries executed successfully.", status=200)

        except Exception as e:
            self.logger.error(f"Error processing request: {e}")
            return HttpResponse("Error processing request.", status=500)
===========================
import os
import re
from google.cloud import bigquery
from django.http import HttpResponse
from django.views.decorators.csrf import csrf_exempt
from django.utils.decorators import method_decorator

class ExecuteUploadedQueries(CredentialsandConnectivity):

    # Read SQL queries from the uploaded text file
    def read_queries_from_uploaded_file(self, file):
        try:
            content = file.read().decode('utf-8')
            queries = [q.strip() for q in re.split(r';\s*\n', content) if q.strip()]
            self.logger.info(f"Read {len(queries)} queries from file.")
            return queries
        except Exception as e:
            self.logger.error(f"Error reading file: {e}")
            return []

    # Handle file upload and process SQL queries
    @method_decorator(csrf_exempt, name='dispatch')
    def post(self, request, *args, **kwargs):
        try:
            if 'file' not in request.FILES:
                self.logger.error("No file uploaded.")
                return HttpResponse("No file uploaded.", status=400)

            file = request.FILES['file']
            self.logger.info(f"Received file: {file.name}")

            queries = self.read_queries_from_uploaded_file(file)
            if not queries:
                self.logger.error("No valid queries found.")
                return HttpResponse("No valid queries found.", status=400)

            self.execute_queries(queries)
            return HttpResponse("File uploaded and queries executed successfully.", status=200)

        except Exception as e:
            self.logger.error(f"Error processing request: {e}")
            return HttpResponse("Error processing request.", status=500)

    # Execute queries in BigQuery
    def execute_queries(self, queries):
        self.dq_bigquery_client()
        for query in queries:
            try:
                query_job = self.client.query(query)
                query_job.result()
                self.logger.info("Query executed successfully.")
            except Exception as e:
                self.logger.error(f"Query execution error: {e}")
