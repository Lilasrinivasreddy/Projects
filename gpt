┌────────────────────────────────────────────────────────────────────────────────────────────┐
│                                   QVerse End-to-End Flow                                    │
│                                                                                            │
│ 1. User inputs a query via Genie UI (FastAPI endpoint triggered).                          │
│                                                                                            │
│ 2. Request is passed through JWT authentication middleware.                                │
│                                                                                            │
│ 3. `Flow.validate_user_request()` validates session, logs history, loads config.           │
│                                                                                            │
│ 4. `Flow.fetch_db_details()` connects to Teradata or configured DB using secure credentials.│
│                                                                                            │
│ 5. `Flow.thinkforge_search()` or `Flow.ask_thinkforge()` checks if similar NLQ exists.     │
│     - If cache hit → use cached SQL.                                                       │
│     - Else → use reasoning trace + LLM to create SQL.                                      │
│                                                                                            │
│ 6. If cache fails, `Catalog_lookup.generate_catalog_query()` retrieves vector-based context│
│     - Uses PGVector + TF-IDF over table metadata and business rules.                       │
│                                                                                            │
│ 7. `Agents.generate_query()` sends metadata context to LLM to generate SQL + follow-ups.   │
│                                                                                            │
│ 8. `Flow.validate_response()` ensures query is syntactically valid and retry up to 3x      │
│     - Auto-corrects SQL via LLM if execution fails.                                        │
│                                                                                            │
│ 9. Final query is executed on DB. Results are captured and formatted using Pandas.         │
│                                                                                            │
│10. `Flow.execute_query()` calls LLM again to generate:                                     │
│     - Recommended visualizations (chart type + fields)                                     │
│     - Summary or insights from result set                                                  │
│                                                                                            │
│11. Final output returned to UI:                                                            │
│     - SQL, Data, Visuals, Summary, Follow-ups, Metadata                                    │
│                                                                                            │
└────────────────────────────────────────────────────────────────────────────────────────────┘


QVerse End-to-End Flow
=======================

                      ┌─────────────────────┐
                      │   User Interface    │
                      │ (AutoBI / Genie UI) │
                      └─────────┬───────────┘
                                │
                                ▼
                  ┌───────────────────────────┐
                  │   FastAPI (main.py)       │
                  │ - JWT Auth & CORS         │
                  │ - API Route Dispatching   │
                  └─────────┬─────────────────┘
                            │
                            ▼
            ┌────────────────────────────────────┐
            │            Flow Agent              │
            │ (flow_agent.py - Flow class)       │
            └────────────────┬───────────────────┘
                             │
        ┌────────────────────┼────────────────────────────┐
        ▼                    ▼                            ▼
┌────────────────┐  ┌─────────────────────┐       ┌────────────────────┐
│Validate Session│  │Fetch DB Connection  │       │ ThinkForge Search  │
│ & Chat History │  │ (Teradata, etc.)    │       │ & Reasoning Layer  │
└────────────────┘  └─────────────────────┘       └────────────────────┘
         │                   │                            │
         └────────────┬──────┴──────────────┬────────────┘
                      ▼                     ▼
           ┌────────────────┐    ┌─────────────────────────────┐
           │ Catalog Lookup │    │ VectorDB (Metadata Context) │
           └────────────────┘    └─────────────────────────────┘
                      │                     │
                      ▼                     ▼
            ┌────────────────────────────────────────┐
            │        Agents.generate_query           │
            │ - Uses LLM to generate SQL             │
            │ - Extracts follow-up, metadata, etc.   │
            └────────────────┬───────────────────────┘
                             ▼
               ┌────────────────────────────┐
               │ validate_response (SQL     │
               │ validation + error fix via │
               │ LLM correction loop)       │
               └────────────────────────────┘
                             ▼
               ┌────────────────────────────┐
               │  SQL Execution via DB      │
               │  with Time Tracking        │
               └────────────────────────────┘
                             ▼
         ┌─────────────────────────────────────────────┐
         │ execute_query:                              │
         │ - Format result (JSON, datetime handling)   │
         │ - LLM calls for visualization & summary     │
         └─────────────────────────────────────────────┘
                             ▼
               ┌────────────────────────────┐
               │   Final Response to UI     │
               │ - SQL + Results + Charts   │
               │ - Summary + Follow-ups     │
               └────────────────────────────┘






QVerse Codebase Design Document

Overview

QVerse is an AI-powered system that enables users to ask natural language questions and receive answers derived from structured data through automatic SQL generation. The system leverages a combination of caching mechanisms, metadata-driven catalog search, PGVector-based semantic similarity, and large language models (LLMs) to facilitate this translation process. The codebase is modular, extensible, and deployed as a microservice-based architecture on Kubernetes within the GCP environment.

Key Objectives

Convert user questions to SQL using LLMs.

Use context-aware table metadata and semantic similarity for accurate query construction.

Enhance system performance with caching for known questions.

Allow domain and catalog-based fine-tuning using feedback and rules.

System Architecture

QVerse follows a modular architecture comprising routers, agents, services, and utility modules.

Core Modules

Genie Router: Handles user question input and directs it to the main pipeline for query generation and execution.

Flow Agent: Orchestrates the entire NLQ to SQL process, including validation, metadata retrieval, and LLM interactions.

Catalogs Agent: Interfaces with the metadata (context) layer to retrieve relevant tables and columns using PGVector.

Caching Agent: Looks up previous questions and responses in the ThinkForge cache to reuse SQL and reasoning.

NLQ Agent: Interacts with the LLM to construct SQL based on filtered metadata.

Detailed Flow Description

User Request Initiation

The user initiates a query via API (e.g., /genie/converse).

Authentication and authorization are validated using the session ID.

Config and Metadata Retrieval

The system fetches the configuration flags (use_cache, use_catalogs, use_history) from the database for the relevant domain and catalog.

Depending on the flags, the system determines whether to use caching or metadata for query generation.

Cache Check (ThinkForge)

If enabled, the system checks if a similar question has already been asked.

If a cache hit is found, the SQL and reasoning are reused.

If not, the system proceeds with metadata-based processing.

Metadata Search with PGVector

Table descriptions are embedded and stored in PGVector.

A semantic search identifies the top 100 relevant tables.

LLM then narrows this list to the top 5 tables based on descriptions and usage patterns.

Relevant columns are filtered based on token count and relevance.

SQL Generation

Filtered tables and columns are sent to the LLM along with user question.

The LLM returns an SQL query.

The system performs validation checks:

Must be a valid SELECT query.

No invalid table/column references.

Must be executable.

Execution and Response

SQL is executed against the appropriate database (PostgreSQL, GCP, Spanner).

The result is summarized and visualizations are generated if requested.

The final response includes the SQL, result set, and visual insights.

Metadata Context and Rules

Context Management

Metadata includes table names, column descriptions, data types, keys, usage patterns, and join rules.

Managed through the /contexts endpoints with upload and update functionality.

Supports CSV upload for bulk operations.

Rule Application

Table Rules: Defined manually for known issues (e.g., join on multiple keys).

Catalog Rules: Generated from user feedback to enforce global logic corrections.

These rules are injected into the LLM prompt during SQL generation to improve accuracy.

Endpoints Summary

Endpoint

Description

/genie/converse

Chat-based NLQ to SQL with history

/genie/ask_and_act

Stateless API for integration with other tools

/contexts/upload_contexts

Uploads metadata in bulk

/stats

Returns usage and performance analytics

Deployment Details

Platform: Google Kubernetes Engine (GKE)

Secrets: Managed via GCP Secret Manager and .env files

Vector DB: PGVector embedded in PostgreSQL

Cache: Redis

Auth: LDAP + JWT

Monitoring: Endpoints exposed via load balancer with future microservice split planned

Codebase Organization

src/
├── agents/                 # Core business logic
│   └── flow_agent.py, catalogs_agent.py, ...
├── app/routers/            # FastAPI routers for various modules
│   └── genie/, auth/, contexts/, cache/
├── database_connections/   # Connection and service logic
│   └── connection.py, service.py
├── utils/                  # Logging, exceptions, vector search helpers
├── llm/                    # LLM prompts and Vegas integration

Developer Guidelines

Follow CONTRIBUTING.md for code standards.

No credentials in code; use secrets/config.

Unit tests and CI required for all PRs.

Table aliases, join conditions, and rule logic must follow catalog/domain standards.

End-to-End Technical Flow (NLQ to SQL)

User/API sends a query to the FastAPI endpoint (/genie/converse).

Genie controller passes it to Genie service which invokes flow_agent.validate_user_request().

It fetches configuration flags (use_cache, use_catalogs, use_history) from the DB.

If use_cache is enabled, it checks ThinkForge cache via caching_agent.

If no cache hit, it calls catalogs_agent.semantic_table_search() which uses PGVector to shortlist 100 tables.

The top 5 tables are selected by LLM using table descriptions and usage patterns.

Relevant columns are filtered based on token count and question relevance.

NLQ is passed to the LLM through nlq_agent, which returns SQL.

flow_agent.validate_sql() ensures syntax and table validity.

Valid SQL is executed using flow_agent.execute_sql().

Summary and visualizations are generated.

Final response is returned in JSON format including result set, charts, and SQL.

Conclusion

QVerse provides an intelligent and extensible platform for translating business questions into executable queries using a blend of caching, metadata, and LLMs. Its modular architecture ensures flexibility, scalability, and maintainability across different domains and use cases.



End-to-End Design Explanation (Code Perspective)
The entire codebase is designed around a modular, agent-based architecture using FastAPI, where each major task — such as request handling, SQL generation, caching, summarization, and visualization — is handled by specialized classes and utilities. The entry point is main.py, which sets up the FastAPI server, adds security and observability (JWT auth + Prometheus), and routes incoming API calls to different controllers like authentication, genie (AutoBI), feedbacks, models, and contexts.

🧠 Flow Orchestration with flow_agent.py
The central orchestrator for a user’s query is the Flow class. It provides multiple async methods that drive the request from validation to execution:

validate_user_request() validates the session ID against the backend database and retrieves domain-specific configuration. It also sets up initial tracking using chat history logging.

fetch_db_details() connects to the appropriate data source by decoding credentials and determining the database type from catalog configuration. The DB connection object is saved into the artifacts dictionary for reuse.

thinkforge_search() and ask_thinkforge() interact with the cache layer, ThinkForge, to find existing SQL queries related to the user’s NLQ. If an exact match is found, it is reused. If only similar queries are found, an LLM is used to generate SQL using reasoning_trace.

catalogs_search() initiates catalog-based semantic search using vector similarity (PGVector), TF-IDF on rules, and table metadata. The context is built and then sent to the LLM for SQL generation.

validate_response() plays a key role in checking whether the generated SQL is a valid SELECT query. If execution fails, it retries with a corrected query using LLM error understanding. This retry happens up to 3 times.

execute_query() is responsible for running the final query, formatting the result DataFrame (especially dates and nested objects), and calling the LLM to generate chart visualizations and textual summaries. It combines these results into a structured response and updates chat history with SQL, visualization, and summary.

📚 Context Building with catalog_agents.py
The Catalog_lookup class builds the context required for SQL generation. It uses PGVector to find the top-matching tables and fields, then ranks and filters the results using TF-IDF on business_rules and catalog_rules. If the full metadata exceeds token limits, it uses LLM again to shortlist relevant columns dynamically. This context is eventually passed to the NLQ Agent for SQL generation.

🧠 Cache Layer with thinkforge_agent.py
The ThinkForge class handles all operations related to caching and reuse of past queries:

retrieve_cache() finds semantically similar past queries from a dedicated cache API.

insert_cache() stores successful NLQ → SQL mappings for future reusability.

ask_cache() sends the user’s query to ThinkForge’s LLM backend to reason and generate SQL if similar examples exist.

🤖 SQL Generation with nlq_agent.py
The Agents class handles context-driven SQL generation:

find_context_vdb() retrieves semantic context from a vector store based on the document type (rules, descriptions, etc.).

generate_query() takes the enriched context and passes it to the LLM for query generation, extracting follow-up questions, token usage, tables referred, and confidence score.

🛠 Utility Modules
AppDatabase manages all persistent operations like chat history, session validation, and metadata retrieval.

LLM is a centralized service that wraps LLM functionality: SQL generation, follow-up creation, query correction, summarization, and visualization.

logger_capture ensures structured logging at every step (debug/info/error), useful for debugging and monitoring.

📦 The Artifacts Dictionary
At the heart of this orchestration is the artifacts dictionary, which acts like a session-wide container. It carries essential inputs and outputs between stages (e.g., user query, DB connection, context, LLM response, final SQL, chart info, and summary). Each function updates and passes artifacts forward — keeping the design stateless and scalable.

Would you like this explanation inserted into your Word doc / PDF or exported as a Markdown or slide format?








Ask ChatGPT



Tools



ChatGPT can make mistakes. Check important info. See Cookie Preferences.
============

main.py
=======
import os
import warnings

import jwt
from fastapi import FastAPI, HTTPException, Request, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, PlainTextResponse
from fastapi.security import OAuth2PasswordBearer

# Import controllers for various routes
from src.app.routers.auth import controller as auth_controller
from src.app.routers.app_data import controller as app_data_controller
from src.app.routers.contexts import controller as contexts_controller
from src.app.routers.feedbacks import controller as feedbacks_controller
from src.app.routers.genie import controller as genie_controller
from src.app.routers.models import controller as models_controller
from src.app.routers.sessions import controller as sessions_controller
from src.config_setup import Config as config
from prometheus_fastapi_instrumentator import Instrumentator

import io
import json

# --- ORM Table Auto-Creation ---
from src.database_connections.app_database.orm_init import create_tables

# Create temporary directory if it doesn't exist
temp_dir = config.paths.TEMP
if temp_dir:
    os.makedirs(temp_dir, exist_ok=True)
# Ignore warnings
warnings.filterwarnings("ignore")

# Initialize FastAPI app
app = FastAPI()
Instrumentator().instrument(app).expose(app)
origins = ["*"]

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,  # This should be a list of strings
    allow_credentials=True,  # This should be a boolean
    allow_methods=["*"],  # This should be a list of strings
    allow_headers=["*"],  # This should be a list of strings
)

# Define OAuth2 scheme
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/api/v1.0/authentication/login")


@app.middleware("http")
async def auth_check(request: Request, call_next):
    """
    Middleware to check the validity of JWT tokens in incoming requests.

    Args:
        request (Request): The incoming request.
        call_next: The next middleware or route handler.

    Returns:
        Response: The response after processing the request.
    """
    try:
        # Allow OPTIONS requests to pass through
        if request.method == "OPTIONS":
            return await call_next(request)

        # Allow specific paths to pass through without authentication
        if request.url.path in [
            "/api/v1.0/authentication/login",
            "/docs",
            "/health",
            "/metrics",
            "/stats",
            "/openapi.json",
        ]:
            response = await call_next(request)
            return response

        # Check for Authorization header
        token = request.headers.get("Authorization")
        if token is None:
            raise HTTPException(status_code=401, detail="Invalid Token")
        else:
            encoded = token.split(" ")[-1]
            try:
                # Decode JWT token
                jwt.decode(encoded, "secret", algorithms=["HS256"])
            except jwt.ExpiredSignatureError:
                raise HTTPException(status_code=401, detail="Token has expired")
            except jwt.InvalidTokenError:
                raise HTTPException(status_code=401, detail="Invalid Token")

        return await call_next(request)
    except HTTPException as http_exc:
        return JSONResponse(
            status_code=http_exc.status_code, content={"detail": http_exc.detail}
        )
    except Exception as exc:
        return JSONResponse(
            status_code=500, content={"detail": f"Internal Server Error: {exc}"}
        )


# Include routers for various endpoints
app.include_router(
    auth_controller.router, prefix="/api/v1.0/authentication", tags=["Authentication"]
)
app.include_router(
    app_data_controller.router, prefix="/api/v1.0/stats", tags=["Analytics"]
)
app.include_router(
    models_controller.router, prefix="/api/v1.0/models", tags=["LLM Models"]
)
app.include_router(genie_controller.router, prefix="/api/v1.0/genie", tags=["AutoBI"])
app.include_router(
    feedbacks_controller.router, prefix="/api/v1.0/feedbacks", tags=["Feedbacks"]
)
app.include_router(
    contexts_controller.router, prefix="/api/v1.0/contexts", tags=["contexts"]
)
app.include_router(
    sessions_controller.router, prefix="/api/v1.0/sessions", tags=["Session Management"]
)
@app.get("/health")
async def health_check():
    return {"status": "ok"}

# Add security definitions to the OpenAPI schema
app.openapi_schema = app.openapi()
app.openapi_schema["components"]["securitySchemes"] = {
    "BearerAuth": {
        "type": "http",
        "scheme": "bearer",
        "bearerFormat": "JWT",
    }
}
for path in app.openapi_schema["paths"].values():
    for method in path.values():
        method["security"] = [{"BearerAuth": []}]

# Create tables in the database
create_tables()
===========
catalog_agents.py
===========
import numpy as np
import json
from datetime import datetime
from typing import Any
from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore
from sklearn.metrics.pairwise import cosine_similarity  # type: ignore
from src.llm.services import LLM
from src.utils.log_wrapper import logger_capture
from src.config_setup import tokenizer
from src.agents.nlq_agent import Agents
from src.database_connections.app_database.service import AppDatabase


class Catalog_lookup:

    @staticmethod
    def generate_catalog_query(
        session_logger: Any, artifacts: dict
    ) -> tuple[dict[str, Any], float, float] | None | tuple[None, None, None]:
        """
        Generates a catalog-based SQL query based on the user query and collection name.

        Args:
            session_logger (SessionLogger): The session logger instance.
            historyid (int): The history ID associated with the request.
            user_query (str): The user's input query.
            system (str): The name of the collection to search in.
            doc_filter (dict): The filter to apply to the documents.
            llm_model (str): The selected LLM model. Defaults to "gemini_pro".

        Returns:
            tuple: The generated query, catalog vector search time, and catalog LLM time.
        """

        historyid = artifacts["chat_history_id"]
        user_query = artifacts["translated_question"] if artifacts["translated_question"] else artifacts["user_query"]
        domain = artifacts["domain"]
        catalog = artifacts["catalog"]
        doc_filter = artifacts["doc_filter"]
        llm_model = artifacts["llm_model"]
        db_type = artifacts["db_type"]

        logger_capture.log_info(
            session_logger, f"Parameters:, {user_query},{domain},{doc_filter}"
        )

        # Process the catalog to find the best match
        catalog_context, catalog_vector_search_time_taken = (
            Catalog_lookup.process_catalog(
                session_logger,
                historyid,
                user_query,
                "contexts",
                domain,
                catalog,
            )
        )

        logger_capture.log_debug(
            session_logger, f"Best Match Catalog: {catalog_context}"
        )

        if catalog_context:
            query_feedbacks = AppDatabase.get_query_feedbacks(domain, catalog).to_dict(
                "records"
            )
            if query_feedbacks:
                questions = [feedback["question"] for feedback in query_feedbacks]

                # Initialize the TF-IDF vectorizer
                vectorizer = TfidfVectorizer()

                # Fit and transform the questions to TF-IDF vectors
                tfidf_matrix = vectorizer.fit_transform(questions)

                # Convert user question to TF-IDF vector
                query_vector = vectorizer.transform([user_query])

                # Compute cosine similarity between user question and feedback questions
                cosine_similarities = cosine_similarity(
                    query_vector, tfidf_matrix
                ).flatten()

                # Get the top N questions based on similarity score
                top_indices = np.argsort(cosine_similarities)[-5:][::-1]
                top_feedbacks = [query_feedbacks[i] for i in top_indices]
                for feedback in top_feedbacks:
                    feedback["query_feedback"] = (
                        "Liked" if feedback["query_feedback"] else "Disliked"
                    )
            else:
                top_feedbacks = []
            catalog_llm_start_time = datetime.now()

            # Generate the SQL query using the catalog context
            output = Agents.generate_query(
                session_logger,
                historyid,
                user_query,
                catalog_context,
                db_type,
                "contexts",
                None,
                llm_model,
                top_feedbacks,
            )
            catalog_llm_time_taken = (
                datetime.now() - catalog_llm_start_time
            ).total_seconds()
            # Always return exactly 3 values
            return output, catalog_vector_search_time_taken, catalog_llm_time_taken
        # Always return exactly 3 values
        return None, None, None

    def vector_search_columns(user_query, columns_data, business_rules, catalog_rules):
        # descriptions = [info['column_description'] for info in columns_data.values()]
        print(catalog_rules)
        combined_query = f"""{user_query}.
        {' '.join(business_rules)}.
        {'.'.join(descp['rule'] for descp in catalog_rules)}"""
        print(combined_query)

        descriptions = [
            f"""{info['column_description']} 
            {', '.join(info.get('possible_values', []))}
            {info.get('categorical_definitions', '')}"""
            for info in columns_data.values()
        ]
        column_names = list(columns_data.keys())

        # Convert descriptions to TF-IDF vectors
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform(descriptions)

        # Convert user query to TF-IDF vector
        query_vector = vectorizer.transform([combined_query])

        # Compute cosine similarity between user query and column descriptions
        cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()

        # Get the top 100 columns based on similarity score
        top_indices = np.argsort(cosine_similarities)[-100:][::-1]
        top_columns = {
            column_names[i]: columns_data[column_names[i]] for i in top_indices
        }
        print("Vector Shortlisted Columns", top_columns)

        return top_columns

    def search_catalog_rules(user_query, tables, domain):
        catalog_rules = AppDatabase.get_active_catalogrules(domain).to_dict("records")
        if not catalog_rules:
            return []
        rules = [
            rule["catalog_rules"]
            for rule in catalog_rules
            if any(table in rule["catalog_rules"].get("tables", []) for table in tables)
        ]
        print(rules)
        if rules:
            rule_descriptions = [descp["rule"] for descp in rules]
            # Convert descriptions to TF-IDF vectors
            vectorizer = TfidfVectorizer()
            tfidf_matrix = vectorizer.fit_transform(rule_descriptions)

            # Convert user query to TF-IDF vector
            query_vector = vectorizer.transform([user_query])

            # Compute cosine similarity between user query and column descriptions
            cosine_similarities = cosine_similarity(
                query_vector, tfidf_matrix
            ).flatten()

            top_indices = np.argsort(cosine_similarities)[-10:][::-1]
            top_matching_rules = [rules[i] for i in top_indices]

            return top_matching_rules
        else:
            return []

    def flatten_metadata_to_text(metadata):
        lines = []
        for table_name, table_info in metadata.items():
            table_description = table_info.get("description", "")
            columns = table_info.get("columns", {})
            rules = table_info.get("business_rules", [])

            lines.append(f"Table Name: {table_name}")
            lines.append(f"Description: {table_description}")
            lines.append(f"Business Rules: {rules}")
            lines.append("Columns:")
            lines.append(
                "column_name,column_alias,column_description,data_type,possible_values,categorical_definitions"
            )

            for column_name, column_info in columns.items():
                column_alias = column_info.get("alias", "")
                column_description = column_info.get("column_description", "")
                data_type = column_info.get("data_type", "")
                possible_values = column_info.get("possible_values", [])
                categorical_definitions = column_info.get("categorical_definitions", "")

                lines.append(
                    f"""{column_name},{column_alias},
                    {column_description},{data_type},
                    {possible_values},{categorical_definitions}"""
                )
            lines.append("")  # Add a blank line for separation between tables

        return "\n".join(lines)

    @staticmethod
    def process_catalog(
        session_logger: Any,
        historyid: int,
        user_query: str,
        doc_type: str,
        domain: str,
        catalog: str,
    ) -> tuple[dict[str, Any], float]:
        """
        Processes the catalog to find the best match based on the user query.

        Args:
            session_logger (SessionLogger): The session logger instance.
            historyid (int): The history ID associated with the request.
            user_query (str): The user's input query.
            embeddings (object): The embeddings object.
            doc_type (str): The type of document to search.
            domain (str): The name of the collection to search in.
            domainid (int): The domain ID.
            catalog (str): The catalog name.
            catalogid (int): The catalog ID.

        Returns:
            dict: The catalog context data.
        """
        logger_capture.log_debug(
            session_logger,
            f"[DEBUG] process_catalog called with: user_query={user_query}, doc_type={doc_type}, domain={domain}, catalog={catalog}"
        )
        k = 500  # Vector Search catalogs search limit
        descp_retri, catalog_vector_search_time_taken = Agents.find_context_vdb(
            doc_type, user_query, k, domain=domain, catalog=catalog,
        )
        logger_capture.log_debug(
            session_logger,
            f"[DEBUG] find_context_vdb returned {len(descp_retri)} tables.")
        if len(descp_retri) == 0:
            logger_capture.log_debug(
                session_logger,
                f"[DEBUG] No vector tables found. user_query={user_query}, domain={domain}, catalog={catalog}, doc_type={doc_type}"
            )
            
        # --- END ADDED LOGGING ---

        # Log the keys of each descp_retri entry to debug what fields are present
        for idx, entry in enumerate(descp_retri):
            logger_capture.log_debug(session_logger, f"descp_retri[{idx}] keys: {list(entry.keys())}")

        payload: dict[str, Any] = {"question": user_query, "context": []}

        required_tables = []
        table_meta_data = {}

        table_info = []
        vector_shortlisted_tables = []
        for j in range(0, len(descp_retri)):
            table_name = descp_retri[j]["tablename"]
            table_description = descp_retri[j]["table_description"]
            usage_patterns = descp_retri[j]["usage_patterns"]
            business_rules = descp_retri[j]["rules"]
            table_info.append(
                {
                    "table_name": table_name,
                    "description": table_description,
                    "usage_patterns": usage_patterns,
                    "business rules": business_rules,
                }
            )
            vector_shortlisted_tables.append(table_name)

        # Ask LLM to shortlist tables
        logger_capture.log_info(
            session_logger,
            "Going to LLM for table shortlisting...",
        )
        if len(vector_shortlisted_tables) > 5:
            shortlisted_tables = LLM.suggest_tables(
                session_logger, historyid, user_query, table_info
            )
            logger_capture.log_info(
                session_logger,
                f"LLM shortlisted tables: {shortlisted_tables}",
            )
            valid_tables = [
                table
                for table in shortlisted_tables
                if table in vector_shortlisted_tables
            ]
            if not valid_tables:
                shortlisted_tables = vector_shortlisted_tables[:5]
            else:
                shortlisted_tables = valid_tables
        else:
            shortlisted_tables = vector_shortlisted_tables

        try:
            catalog_rules = Catalog_lookup.search_catalog_rules(
                user_query, shortlisted_tables, domain
            )
            logger_capture.log_info(
                session_logger,
                f"Looked Through Catalog Rules: {catalog_rules}",
            )
            # Iterate through the retrieved descriptions
            for j in range(0, len(descp_retri)):
                table_name = descp_retri[j]["tablename"]
                if table_name not in shortlisted_tables:
                    continue
                score = round((1 - descp_retri[j]["distance"]) * 100, 2)
                logger_capture.log_info(
                    session_logger,
                    f"{j+1} Table: {table_name}, Similarity Score: {score}%",
                )

                if score > 50:
                    
                    table_columns_data = descp_retri[j]["columns"]
                    business_rules = descp_retri[j]["rules"]
                    if (
                        len(table_columns_data) >= 500
                        and len(shortlisted_tables) > 3
                    ):
                        shortlisted_columns_data = Catalog_lookup.vector_search_columns(
                            user_query,
                            table_columns_data,
                            business_rules,
                            catalog_rules,
                        )
                    else:
                        shortlisted_columns_data = table_columns_data

                    for column_name, column_info in shortlisted_columns_data.items():

                        if (
                            "possible_values" in column_info
                            and len(column_info["possible_values"]) == 0
                        ):
                            shortlisted_columns_data[column_name].pop("possible_values")
                        if (
                            "categorical_definitions" in column_info
                            and column_info["categorical_definitions"] == "{}"
                        ):
                            shortlisted_columns_data[column_name].pop(
                                "categorical_definitions"
                            )

                    table_meta_data[table_name] = {}
                    table_meta_data[table_name]["description"] = descp_retri[j]["table_description"]
                    table_meta_data[table_name]["business_rules"] = descp_retri[j]["rules"]
                    table_meta_data[table_name]["columns"] = shortlisted_columns_data
            logger_capture.log_info(
                session_logger,
                f"Shortlisted Tables Metadata: {table_meta_data}",
            )
            if table_meta_data:

                flattened_metadata = Catalog_lookup.flatten_metadata_to_text(
                    table_meta_data
                )
                logger_capture.log_info(
                    session_logger,
                    "Flattened Metadata",
                )
                # catalog_metadata_token_count = len(tokenizer.tokenize(json.dumps(table_meta_data)))
                catalog_metadata_token_count = len(
                    tokenizer.tokenize(flattened_metadata)
                )
                # catalog_metadata_token_count =0
                logger_capture.log_info(
                    session_logger,
                    f"Table Meta Data Token Count: {catalog_metadata_token_count}",
                )

                model_suggested_columns = {}

                if catalog_metadata_token_count > 31000:

                    # Suggest columns using the LLM
                    logger_capture.log_info(
                        session_logger,
                        "Going to LLM for column suggestions...",
                    )
                    model_suggested_columns = LLM.suggest_columns(
                        session_logger,
                        historyid,
                        user_query,
                        flattened_metadata,  # flattened_metadata
                    )
                    logger_capture.log_info(
                        session_logger,
                        f"LLM selected columns: {model_suggested_columns} with type {type(model_suggested_columns)}",
                    )
                else:
                    for table_name, meta_data in table_meta_data.items():
                        columns = list(meta_data["columns"].keys())
                        model_suggested_columns[table_name] = columns
                    logger_capture.log_info(
                        session_logger,
                        f"Metadata columns: {model_suggested_columns} with type {type(model_suggested_columns)}",
                    )

                if model_suggested_columns:
                    suggested_columns = model_suggested_columns
                    logger_capture.log_info(
                        session_logger,
                        f"json loaded: {suggested_columns} with type {type(suggested_columns)}",
                    )

                    # Iterate through the suggested columns and build the context entries
                    for table_name, recommended_columns in suggested_columns.items():
                        required_tables.append(table_name)
                        for j in range(0, len(descp_retri)):
                            meta_table_name = descp_retri[j][
                                "tablename"
                            ]
                            if meta_table_name == table_name:
                                
                                table_columns_data = descp_retri[j]["columns"]
                                for (
                                    column_name,
                                    column_info,
                                ) in table_columns_data.items():

                                    if (
                                        "possible_values" in column_info
                                        and len(column_info["possible_values"]) == 0
                                    ):
                                        table_columns_data[column_name].pop(
                                            "possible_values"
                                        )
                                    if "categorical_definitions" in column_info and (
                                        column_info["categorical_definitions"] == "{}"
                                        or column_info["categorical_definitions"]
                                        is None
                                    ):
                                        table_columns_data[column_name].pop(
                                            "categorical_definitions"
                                        )
                                    if "column_description" in column_info and (
                                        column_info["column_description"] == ""
                                        or column_info["column_description"] is None
                                    ):
                                        table_columns_data[column_name].pop(
                                            "column_description"
                                        )
                                column_names = [
                                    col.split()[0].lower()
                                    for col in recommended_columns
                                ]
                                logger_capture.log_info(
                                    session_logger,
                                    f"Suggested Columns for table: {table_name} are {column_names}",
                                )

                                context_entry = {
                                    "table": table_name,
                                    "columns": {
                                        key: value
                                        for key, value in table_columns_data.items()
                                        if key.lower() in column_names
                                    },
                                    # "columns": table_columns_data,
                                    "description": descp_retri[j]["table_description"],
                                    "rules": descp_retri[j]["rules"],
                                }

                                payload["context"].append(context_entry)

            if payload["context"]:
                global_rules = {
                    "query_guidelines": [
                        "Aggregate functions should come after column names in sql query"
                    ]
                }
                catalog_rules = {"catalog_rules": catalog_rules}
                payload["context"].append(global_rules)
                if catalog_rules:
                    payload["context"].append(catalog_rules)

                context_data = {"payload": payload}
                catalog_payload_token_count = len(
                    tokenizer.tokenize(json.dumps(context_data))
                )
                # catalog_payload_token_count =0 
                logger_capture.log_info(
                    session_logger,
                    f"Final Catalog token count: {catalog_payload_token_count}",
                )
            else:
                context_data = {}

        except Exception as e:
            logger_capture.log_error(session_logger, f"Error: {str(e)}")
            context_data = {}

        return context_data, catalog_vector_search_time_taken
=================
import base64
import concurrent.futures
import json
from datetime import datetime
import pandas as pd
from src.database_connections.app_database.service import AppDatabase
from src.database_connections.query_execution.query_execution_registry import (
    query_execution_dbs_registry,
)
from src.agents.catalogs_agent import Catalog_lookup
from src.llm.prompts import Prompt
from src.llm.services import LLM
from src.utils.log_wrapper import logger_capture
from src.utils.utils import Utils
from src.agents.thinkforge_agent import ThinkForge


class Flow:
    async def validate_user_request(session_logger, artifacts, stream=False):
        """
        Validates User Request

        Args:
            session_logger (SessionLogger): The session logger instance.
            Parameters: dict
        Returns:
            dict:
        """

        session_id = artifacts["session_id"]
        user_query = artifacts["user_query"]
        domain = artifacts["domain"]
        catalog = artifacts["catalog"]

        session_platform = AppDatabase.valid_sessionid(session_id)
        if not session_platform:
            raise ValueError(f"Session {session_id} is invalid or has expired")
        
        artifacts["session_platform"] = session_platform
        logger_capture.log_info(
            session_logger,
            f"""Received user request:: user_question:{user_query}, domain :{domain} from {session_platform}""",
        )
       
        # Validate system and category
        domainid, catalogs_df, config = Utils.get_workspace_config(
            session_logger, domain, catalog
        )
        
        catalog_ids = catalogs_df["catalogid"].tolist()

        if not artifacts["llm_model"]:
            llm_model = catalogs_df["default_llm_model"].tolist()[0]
            logger_capture.log_info(
                session_logger, f""" Using model {llm_model} as default"""
            )
        else:
            llm_model = artifacts["llm_model"]
        # Insert chat history
        
        chat_history_id = AppDatabase.insert_chat_history(
            domainid,
            catalog,
            (
                session_id,
                artifacts["start_time"].strftime("%Y-%m-%d %H:%M:%S"),
                user_query,
                artifacts["transaction_id"],
            ),
        )
        if not chat_history_id:
            raise Exception("Unable to start chat history in DB")
        logger_capture.log_debug(
            session_logger, f"User Chat Updated with id : {chat_history_id}"
        )
        ## Update Artifacts
        artifacts.update(
            {
                "catalog_ids": catalog_ids,
                "llm_model": llm_model,
                "chat_history_id": chat_history_id,
            }
        )
        
        return artifacts

    async def fetch_db_details(session_logger, artifacts):

        domain = artifacts["domain"]
        catalog = artifacts["catalog"]

        db_config = Utils.get_db_config(domain, catalog)
        db_type = db_config.get("db_type")
        encoded_credentials = db_config.get("credentials")

        # Decode the credentials
        decoded_credentials_json = base64.b64decode(encoded_credentials).decode("utf-8")
        db_credentials = json.loads(decoded_credentials_json)
        artifacts["db_type"] = db_type

        if db_type.lower() not in query_execution_dbs_registry:
            raise ValueError(f"Unsupported database type: {db_type}")

        # Initialize the database connection
        db_connection_start_time = datetime.now()
        db_connection = query_execution_dbs_registry[db_type](**db_credentials)
        artifacts["db_connection_time_taken"] = (
            datetime.now() - db_connection_start_time
        ).total_seconds()
        query_execution_count = 0
        ## Update Artifacts
        artifacts.update(
            {
                "db_connection": db_connection,
                "db_query_execution_time_taken": 0,
                "query_execution_count": query_execution_count,
            }
        )
        logger_capture.log_debug(
            session_logger, f"Received DB Credentials and Established Connection"
        )
        return artifacts

    async def thinkforge_search(session_logger, artifacts):
        logger_capture.log_debug(session_logger, f"Loading Caching Framework Layer....")

        domain = artifacts["domain"]
        catalog = artifacts["catalog"]
        # Initialize response times
        artifacts["entity_time_taken"] = 0.0

        entity_outcome = None
        recommed_visualization = None

        # Caching layer search
        entity_search_start_time = datetime.now()
        # Retrieve sample SQL pairs
        cached_similar_questions = ThinkForge.retrieve_cache(
            session_logger, artifacts["user_query"], domain, catalog
        )
        logger_capture.log_debug(session_logger, f"Retrieved Similar questions...")

        references = []
        if not cached_similar_questions:
            artifacts.update(
                {
                    "entity_time_taken": (
                        datetime.now() - entity_search_start_time
                    ).total_seconds(),
                    "entity_outcome": entity_outcome,
                    "recommed_visualization": recommed_visualization,
                }
            )
            return artifacts

        for example in cached_similar_questions:
            references.append(
                {
                    "question": example.get("nl_query"),
                    "sql": example.get("template"),
                    "reasoning_trace": example.get("reasoning_trace"),
                }
            )
        existing_reference = next(
            (ref for ref in references if ref["question"] == artifacts["user_query"]),
            None,
        )

        if existing_reference:
            cache_generated_sql = existing_reference["sql"]
            source = "Cache2.0 Exact Match"
        else:
            cache_generated_sql = LLM.generate_cache_sql(
                session_logger,
                artifacts["chat_history_id"],
                artifacts["user_query"],
                references,
            )
            source = "Cache2.0 Similar Traces"

        if cache_generated_sql:
            entity_outcome = {
                "generated_query": cache_generated_sql,
                "source": source,
                "status": "success",
                "tables_referred": None,
                "samples_referred": None,
                "suggested_followups": [],
            }

        ## Update Artifacts
        artifacts.update(
            {
                "entity_time_taken": (
                    datetime.now() - entity_search_start_time
                ).total_seconds(),
                "entity_outcome": entity_outcome,
                "recommed_visualization": recommed_visualization,
            }
        )
        return artifacts

    def generate_followups(session_logger, artifacts):
        logger_capture.log_debug(session_logger, f"Generating follow-up questions for Query generated by ThinkForge...")
        user_query = artifacts["translated_question"] if artifacts["translated_question"] else artifacts["user_query"]
        get_query_meta = LLM.extract_from_query(session_logger, artifacts['entity_outcome']['generated_query'], artifacts['chat_history_id'])
        metadata = []
        for table_name in get_query_meta.keys():
            table_metadata = AppDatabase.get_table_details_by_name(table_name, artifacts["domain"], artifacts["catalog"])
            metadata.append(table_metadata) 
        follow_ups = LLM.generate_followups(session_logger,artifacts['chat_history_id'],user_query,artifacts['entity_outcome']['generated_query'],metadata)
        return follow_ups

    async def ask_thinkforge(session_logger, artifacts):
        logger_capture.log_debug(session_logger, f"Loading Caching Framework Layer....")

        domain = artifacts["domain"]
        catalog = artifacts["catalog"]
        # Initialize response times
        artifacts["entity_time_taken"] = 0.0

        entity_outcome = None
        recommed_visualization = None

        # Caching layer search
        entity_search_start_time = datetime.now()
        # Retrieve sample SQL pairs
        user_query = artifacts["translated_question"] if artifacts["translated_question"] else artifacts["user_query"]
        response = ThinkForge.ask_cache(
            session_logger, user_query, domain, catalog
        )
        if not response:
            artifacts.update(
                {
                    "entity_time_taken": (
                        datetime.now() - entity_search_start_time
                    ).total_seconds(),
                    "entity_outcome": entity_outcome,
                    "recommed_visualization": recommed_visualization,
                }
            )
            return artifacts

        logger_capture.log_debug(
            session_logger, f"Received response from ThinkForge..."
        )

        source = "ThinkForge"

        cache_generated_sql = response.get("updated_template", None)

        if cache_generated_sql:
            entity_outcome = {
                "generated_query": cache_generated_sql,
                "is_confident": response.get("is_confident", None),
                "source": source,
                "status": "success",
                "tables_referred": None,
                "samples_referred": response.get("cache_template", None),
                "suggested_followups": Flow.generate_followups(session_logger,artifacts),
            }

        ## Update Artifacts
        artifacts.update(
            {
                "entity_time_taken": (
                    datetime.now() - entity_search_start_time
                ).total_seconds(),
                "entity_outcome": entity_outcome,
                "recommed_visualization": recommed_visualization,
            }
        )
        return artifacts

    async def catalogs_search(session_logger, artifacts):
        catalog_outcome, catalog_vector_search_time_taken, catalog_llm_time_taken = (
            Catalog_lookup.generate_catalog_query(session_logger, artifacts)
        )
        artifacts.update(
            {
                "catalog_vector_search_time_taken": catalog_vector_search_time_taken,
                "catalog_llm_time_taken": catalog_llm_time_taken,
                "catalog_outcome": catalog_outcome,
            }
        )
        return artifacts

    async def validate_response(session_logger, artifacts, layer):
        artifacts["source"] = layer
        if layer == "caching":
            response = artifacts["entity_outcome"]
        elif layer == "contexts":
            response = artifacts["catalog_outcome"]
        elif layer == "history":
            response = artifacts["history_outcome"]
        else:
            raise Exception("Invalid Layer")

        metadata = response.get("metadata", {})
        user_query = artifacts["translated_question"] if artifacts["translated_question"] else artifacts["user_query"]
        validity_error_message = None
        attempt = 0
        while attempt < 3:
            llm_sql_query = response["generated_query"]
            is_select_query = Utils.is_select_query(llm_sql_query)
            logger_capture.log_info(
                session_logger,
                f"LLM Generated Query: {llm_sql_query} is valid Select Query: {is_select_query}",
            )
            if is_select_query:

                if layer == "contexts":
                    query_valid_status, validity_error_message = Utils.validate_query(
                        session_logger,
                        artifacts["chat_history_id"],
                        llm_sql_query,
                        artifacts["domain"],
                        artifacts["catalog_ids"],
                        tables=response["tables_referred"],
                    )
                elif layer == "caching":
                    query_valid_status = "valid"

                elif layer == "history":
                    query_valid_status = "valid"

                if query_valid_status == "valid":
                    logger_capture.log_info(
                        session_logger,
                        f"LLM Generated Query: {llm_sql_query} based on {layer}",
                    )
                    logger_capture.log_debug(
                        session_logger, f"Attempting to Execute SQL Query...."
                    )
                    query_execution_start_time = datetime.now()
                    query_execution, error_message = artifacts[
                        "db_connection"
                    ].execute_sql(session_logger, llm_sql_query)
                    artifacts["db_query_execution_time_taken"] += (
                        datetime.now() - query_execution_start_time
                    ).total_seconds()
                    artifacts["query_execution_count"] += 1
                    artifacts["error_message"] = error_message
                    artifacts["query_execution"] = query_execution
                    logger_capture.log_debug(
                        session_logger, f"SQL Query Execution Completed..."
                    )
                    if not error_message:
                        artifacts["output"] = response
                        break
                    else:
                        error_response = {
                            "status": "error",
                            "source": layer,
                            "generated_query": llm_sql_query,
                            "message": str(error_message),
                            "history_id": artifacts["chat_history_id"],
                            "metadata": metadata,
                            "tables_referred": response["tables_referred"],
                            "samples_referred": response["samples_referred"],
                            "prompt_tokens": response.get("prompt_tokens", 0),
                            "completion_tokens": response.get("completion_tokens", 0),
                            "total_tokens": response.get("total_tokens", 0),
                        }

                        attempt += 1

                        logger_capture.log_info(
                            session_logger,
                            f"""Query Pulled from {layer} Layer failed to execute:
                            '{llm_sql_query}' with error: {error_message}""",
                        )
                        (
                            response["generated_query"],
                            tokens,
                        ) = LLM.rectify_query_error(
                            session_logger,
                            artifacts["chat_history_id"],
                            user_query,
                            llm_sql_query,
                            str(error_message),
                            metadata,
                        )
                        logger_capture.log_info(
                            session_logger,
                            f"""LLM Corrected the SQL Query based on Error Message....""",
                        )
                else:
                    logger_capture.log_info(
                        session_logger,
                        f"""LLM Generated Query: {llm_sql_query} based on {layer}
                        is invalid due to {validity_error_message}""",
                    )
                    error_response = {
                        "status": "error",
                        "source": layer,
                        "generated_query": llm_sql_query,
                        "message": validity_error_message,
                        "history_id": artifacts["chat_history_id"],
                        "metadata": metadata,
                        "tables_referred": response["tables_referred"],
                        "samples_referred": response["samples_referred"],
                        "prompt_tokens": response.get("prompt_tokens", 0),
                        "completion_tokens": response.get("completion_tokens", 0),
                        "total_tokens": response.get("total_tokens", 0),
                    }
                    # artifacts['output'] = error_response
                    # break
                    attempt += 1
                    (
                        response["generated_query"],
                        tokens,
                    ) = LLM.rectify_query_error(
                        session_logger,
                        artifacts["chat_history_id"],
                        user_query,
                        llm_sql_query,
                        validity_error_message,
                        metadata,
                    )
                    logger_capture.log_info(
                        session_logger,
                        f"""LLM Corrected the SQL Query based on Error Message....""",
                    )
            else:
                logger_capture.log_info(
                    session_logger,
                    f"LLM Generated Query: {llm_sql_query} is not a valid SELECT query",
                )
                error_response = {
                    "status": "error",
                    "source": layer,
                    "generated_query": llm_sql_query,
                    "message": "Generated Query is not a valid SELECT query",
                    "history_id": artifacts["chat_history_id"],
                    "metadata": metadata,
                    "tables_referred": response["tables_referred"],
                    "samples_referred": response["samples_referred"],
                    "prompt_tokens": response.get("prompt_tokens", 0),
                    "completion_tokens": response.get("completion_tokens", 0),
                    "total_tokens": response.get("total_tokens", 0),
                }
                artifacts["output"] = error_response

                break
        if layer == "catalogs" and attempt == 3 and not artifacts["output"]:
            artifacts["output"] = error_response

        return artifacts

    async def execute_query(session_logger, artifacts):
        llm_query = artifacts.get(
            "corrected_sql_query", artifacts["output"]["generated_query"]
        )
        user_query = artifacts["translated_question"] if artifacts["translated_question"] else artifacts["user_query"]
        source = artifacts["output"].get("source", None)
        logger_capture.log_info(
            session_logger,
            f"LLM Generated Query: {llm_query} based on {source}",
        )
        if artifacts["query_execution"] is not None:
            records_df = artifacts["query_execution"].copy()
            for column in records_df.columns:
                try:
                    if records_df[column].dtype.name == "dbdate":
                        records_df[column] = pd.to_datetime(records_df[column])
                except Exception as e:
                    logger_capture.log_error(
                        session_logger,
                        f"Failed to validate column {column} for datetime check with error:{e}",
                    )

            for column in records_df.columns:

                if pd.api.types.is_datetime64_any_dtype(records_df[column]):
                    records_df[column] = records_df[column].astype(str)
                records_df[column] = records_df[column].apply(
                    lambda x: json.dumps(x) if isinstance(x, (list, dict)) else x
                )

            records_json = records_df.to_json(orient="records")
        else:
            records_df = pd.DataFrame()
            records_json = records_df.to_json()

        # Use ThreadPoolExecutor to run tasks concurrently
        logger_capture.log_info(
            session_logger,
            f"""Going to LLM for Visualization and Summarization""",
        )

        recommed_visualization = None
        results_summary = None

        if (
            artifacts["show_summary"]
            and artifacts["show_visualization"]
            and len(records_df) > 0
        ):
            with concurrent.futures.ThreadPoolExecutor() as executor:
                vis_thread = executor.submit(
                    LLM.suggest_visualization_unified,
                    session_logger,
                    artifacts["chat_history_id"],
                    user_query,
                    llm_query,
                    records_df,
                    artifacts["llm_model"],
                )
                sum_thread = executor.submit(
                    LLM.summarize_call,
                    session_logger,
                    artifacts["db_connection"],
                    artifacts["chat_history_id"],
                    user_query,
                    llm_query,
                    records_df,
                    artifacts["llm_model"],
                    selected_prompt=Prompt.summarization_stats_prompt,
                )
                results_summary = sum_thread.result()

                # Get the recommended visualization
                recommed_visualization = artifacts["recommed_visualization"]
                if not recommed_visualization:
                    recommed_visualization = vis_thread.result()

            logger_capture.log_info(
                session_logger,
                f"""Received Both Responses for visualization and summary...""",
            )

        elif artifacts["show_summary"]:
            results_summary = LLM.summarize_call(
                session_logger,
                artifacts["db_connection"],
                artifacts["chat_history_id"],
                user_query,
                llm_query,
                records_df,
                artifacts["llm_model"],
                selected_prompt=Prompt.summarization_stats_prompt,
            )

        elif artifacts["show_visualization"] and len(records_df) > 0:
            recommed_visualization = LLM.suggest_visualization_unified(
                session_logger,
                artifacts["chat_history_id"],
                user_query,
                llm_query,
                records_df,
                artifacts["llm_model"],
            )

        artifacts["summary"] = results_summary
        artifacts["recommed_visualization"] = recommed_visualization
        if recommed_visualization:

            for viz in recommed_visualization:
                if viz["chart_type"] == "time_series":

                    time_based_column = next(
                        (
                            dim["field"]
                            for dim in viz["dimensions"]
                            if dim["type"] == "date"
                        ),
                        None,
                    )

                    if time_based_column:

                        records_df = records_df.sort_values(by=time_based_column)

                        break

            for column in records_df.columns:

                if pd.api.types.is_datetime64_any_dtype(records_df[column]):
                    records_df[column] = records_df[column].astype(str)
                records_df[column] = records_df[column].apply(
                    lambda x: json.dumps(x) if isinstance(x, (list, dict)) else x
                )
            records_json = records_df.to_json(orient="records")

        # Prepare the final output based on the session platform

        final_output = {
            "status": "success",
            "records": json.loads(records_json),
            "source": source,
            "generated_query": llm_query,
            "tables_referred": artifacts["output"].get("tables_referred", []),
            "samples_referred": artifacts["output"].get("samples_referred", []),
            "suggested_visualization": recommed_visualization,
            "summary": results_summary,
            "insights": "Data Insights",
            "suggested_followups": artifacts["output"].get("suggested_followups", []),
            "history_id": artifacts["chat_history_id"],
            "metadata": artifacts["output"].get("metadata", {}),
        }

        # Calculate the total session time and update the chat history
        end_time = datetime.now()

        history_update_status = AppDatabase.update_chat_history(
            artifacts["chat_history_id"],
            {
                "updated_at": end_time.strftime("%Y-%m-%d %H:%M:%S"),
                "sql_generated": artifacts["output"]["generated_query"],
                "suggested_visualization": json.dumps(recommed_visualization),
                "generated_summary": results_summary,
                "error_message": None,
            }
        )
        print(history_update_status)
        # Removed all AppDatabase.insert_response_times calls as timing is now persisted via insert_user_activity_history

        return final_output, artifacts
=====================================
nlq_agent.py
============
import json
from datetime import datetime
from typing import Any
from src.llm.services import LLM
from src.utils.exceptions import ServiceUnavailableException
from src.utils.log_wrapper import logger_capture
from src.utils.vectorstore import VectorStore


class Agents:
    @staticmethod
    def find_context_vdb(
        doc_type: str,
        user_input: str,
        k: int,
        domain: str = None,
        catalog: str = None,
    ) -> list[dict[str, Any]]:
        """
        Finds context from the VectorDB based on the user input.

        Args:
            doc_type (str): The type of document to search.

            user_input (str): The user's input query.
            k (int): The number of top results to retrieve.
            domain (str, optional): The domain name.
            domainid (int, optional): The domain ID.
            catalog (str, optional): The catalog name.
            catalogid (int, optional): The catalog ID.

        Returns:
            list: The list of similar descriptions retrieved from the VectorDB.

        Raises:
            ServiceUnavailableException: If the VectorDB server is unavailable.
        """
        # Prepare the payload for the VectorDB API request
        vector_search_start_time = datetime.now()
        vector_store = VectorStore()
        payload = {
            "document_type": doc_type,
            "question": user_input,
            "k": k,
            "domain": domain,
            "catalog": catalog,
        }

        vector_response = vector_store.find_similar_documents(payload)
        vector_search_time_taken = (
            datetime.now() - vector_search_start_time
        ).total_seconds()

        # Check the response status and handle accordingly
        if vector_response:

            similar_descp = vector_response["retrieved_info"]
        else:
            raise ServiceUnavailableException(
                detail="VectorDB server is temporarily unavailable. Please try again."
            )

        return similar_descp, vector_search_time_taken

    @staticmethod
    def generate_query(
        session_logger: Any,
        historyid: int,
        user_query: str,
        context: dict[str, Any],
        db_type: str,
        source: str,
        contexts_referred: list[str] | None = None,
        llm_model: str = "gemini_pro",
        feedbacks: list = [],
    ) -> dict[str, Any]:
        """
        Generates an SQL query based on the user query and context.

        Args:
            session_logger (SessionLogger): The session logger instance.
            historyid (str): The history ID associated with the request.
            user_query (str): The user's input query.
            context (dict): The context information.
            db_type (str): The type of database.
            source (str): The source of the context.
            contexts_referred (list, optional): The list of referred contexts. Defaults to None.
            llm_model (str): The selected LLM model. Defaults to "gemini_pro".

        Returns:
            dict: The output containing the generated query and additional information.
        """
        tables_referred_list = []
        samples_referred = {}
        follow_up_questions = []
        # Extract tables and samples referred from the context
        for entry in context["payload"]["context"]:
            if "table" in entry:
                tables_referred_list.append(entry["table"])
            elif "samples" in entry:
                samples_referred.update(entry["samples"])

        # Generate the SQL query using the LLM
        logger_capture.log_info(
            session_logger,
            f"Going to LLM for Query generation and follow up questions ....",
        )

        sql_answer, follow_up_questions,is_confident, token_usage = LLM.generate_query_followups(
            session_logger,
            historyid,
            user_query,
            context,
            db_type,
            llm_model,
            feedbacks,
        )
        logger_capture.log_info(
            session_logger,
            f"Received SQL and Follow Up questions responses ....",
        )

        if not token_usage:
            token_usage = {
                "promptTokenCount": 0,
                "candidatesTokenCount": 0,
                "totalTokenCount": 0,
            }

        if sql_answer:
            logger_capture.log_info(
                session_logger, f"SQL Response from {llm_model}: {sql_answer}"
            )

            status = "success"
            output = {
                "status": status,
                "generated_query": sql_answer,
                "source": source,
                "tables_referred": tables_referred_list,
                "samples_referred": samples_referred,
                "is_confident":is_confident,
                "prompt_tokens": token_usage["promptTokenCount"],
                "completion_tokens": token_usage["candidatesTokenCount"],
                "total_tokens": token_usage["totalTokenCount"],
                "metadata": json.loads(json.dumps(context["payload"]["context"])),
            }

            if contexts_referred:
                output["contexts_referred"] = contexts_referred

            if follow_up_questions:
                output["suggested_followups"] = follow_up_questions
            else:
                output["suggested_followups"] = []

            return output
        else:
            status = "error"
            sql_answer = "Unable to generate"

            output = {"status": status, "generated_query": sql_answer}

            return output
=======================================
thinkforge_agent.py
==============
import json
import re
import requests
from typing import Any, List
import pandas as pd  # type: ignore
from src.config_setup import Config
from src.utils.log_wrapper import logger_capture

cache_api = Config.cache.cache_api


class ThinkForge:

    def insert_cache(
        session_logger: Any,
        user_query: str,
        reasoning_traces: str,
        sql_query: str,
        table_referred: List,
        domain: str,
        catalog: str,
    ):

        api_endpoint = cache_api + "v1/cache"
        cache_request = {
            "nl_query": user_query,
            "reasoning_trace": reasoning_traces,
            "template": sql_query,
            "template_type": "sql",
            "is_template": True,
            "catalog_type": domain,
            "catalog_subtype": catalog,
        }
        json_data = json.dumps(cache_request)
        headers = {"Content-Type": "application/json"}

        response = requests.post(api_endpoint, data=json_data, headers=headers)
        if response.status_code == 200:
            print("Inserted into cache")
        else:
            raise Exception(
                f"ThinkForge Caching Failed with error code {response.status_code}"
            )

    def retrieve_cache(session_logger, user_query, domain, catalog):
        api_endpoint = cache_api + "v1/cache/search"
        cache_request = {
            "nl_query": user_query,
            "catalog_type": domain,
            "catalog_subtype": catalog,
        }

        headers = {"Content-Type": "application/json"}
        response = requests.get(api_endpoint, params=cache_request, headers=headers)
        return response.json()

    def ask_cache(session_logger, user_query, domain, catalog):
        api_endpoint = cache_api + "v1/complete"
        cache_request = {
            "catalog_type": domain,
            "catalog_subtype": catalog,
            "use_llm": True,
        }
        request = {"prompt": user_query}
        headers = {"Content-Type": "application/json"}
        response = requests.post(
            api_endpoint, params=cache_request, json=request, headers=headers
        )

        if response.status_code == 200:
            return response.json()
        else:
            return {}


